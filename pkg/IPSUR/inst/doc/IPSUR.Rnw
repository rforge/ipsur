%% LyX 1.6.5 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english,nogin]{book}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\lstset{basicstyle={\ttfamily},
breaklines=true,
language=R}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\pagestyle{headings}
\setcounter{tocdepth}{1}
\usepackage{color}
\usepackage{babel}

\usepackage{rotating}
\usepackage{float}
\usepackage{url}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{amssymb}
\setstretch{1.2}
\usepackage[unicode=true, 
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=0,
 breaklinks=true,pdfborder={0 0 0},backref=page,colorlinks=true]
 {hyperref}
\hypersetup{pdftitle={Introduction to Probability and Statistics Using R},
 pdfauthor={G. Jay Kerns},
 linkcolor=blue,  citecolor=black, urlcolor=blue}
 
\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
\newcommand{\noun}[1]{\textsc{#1}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{Sweave}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rcommand}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\textit{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\textit{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\ifx\thechapter\undefined
\newtheorem{thm}{Theorem}
\else
\newtheorem{thm}{Theorem}[chapter]
\fi
  \theoremstyle{definition}
  \newtheorem{xca}[thm]{Exercise}
 \theoremstyle{definition}
  \newtheorem{example}[thm]{Example}
\newenvironment{lyxcode}
{\par\begin{list}{}{
\setlength{\rightmargin}{\leftmargin}
\setlength{\listparindent}{0pt}% needed for AMS classes
\raggedright
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\normalfont\ttfamily}%
 \item[]}
{\end{list}}
  \theoremstyle{remark}
  \newtheorem{note}[thm]{Note}
  \theoremstyle{plain}
  \newtheorem{ax}[thm]{Axiom}
  \theoremstyle{plain}
  \newtheorem{prop}[thm]{Proposition}
  \theoremstyle{plain}
  \newtheorem{fact}[thm]{Fact}
  \theoremstyle{definition}
  \newtheorem{defn}[thm]{Definition}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{Remark}
  \theoremstyle{plain}
  \newtheorem{cor}[thm]{Corollary}
  \theoremstyle{plain}
  \newtheorem{assumption}[thm]{Assumption}
  \theoremstyle{remark}
  \newtheorem*{note*}{Note}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% Meta information - fill between {} and do not remove %
% \VignetteIndexEntry{An Introduction to Probability and Statistics Using R}
% \VignetteDepends{}
% \VignetteKeywords{}
% \VignettePackage{IPSUR}
% Package

% user-loaded packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
%\usepackage{theorem}
\usepackage{subfigure}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{makeidx}
\usepackage{showidx}
\usepackage{multicol}
%\usepackage{floatflt}
\usepackage{url}
\usepackage{srcltx}
\usepackage{txfonts}
\usepackage{stmaryrd}
\usepackage{textcomp}
%\usepackage{pslatex}
\usepackage{upgreek}


%  user defined commands
% special operators
\renewcommand{\P}{%
  \mathop{\operator@font I\hspace{-1.5pt}P\hspace{.13pt}}}
\newcommand{\E}{%
  \mathop{\operator@font I\hspace{-1.5pt}E\hspace{.13pt}}}
\newcommand{\VAR}{\operatorname{var}}
\newcommand{\COV}{\operatorname{cov}}
\newcommand{\COR}{\operatorname{cor}}
\renewcommand{\vec}[1]{\mbox{\boldmath$#1$}}

% special symbols
\newcommand{\me}{\mathrm{e}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}

% special logos
\providecommand{\IPSUR}
{\textsc{I\kern 0ex\lower-.3ex\hbox{\small P}\kern -.5ex\lower.4ex\hbox{\footnotesize S}\kern -.25exU}\kern -.1ex\lower .15ex\hbox{\textsf{\large R}}\@}

\providecommand{\IPSURtitle}
{\fontsize{30}{35}\selectfont \textsc{I\kern -.16ex\lower-.5ex\hbox{\Large P}\kern -.5ex\lower.4ex\hbox{\Large S}\kern -.25exU}\kern -.1ex\lower .15ex\hbox{\textsf{\Huge R}}\@}

% add Contents to TOC
\pagenumbering{roman}
\let\myTOC\tableofcontents
\renewcommand{\tableofcontents}{
     \pdfbookmark[1]{Contents}{}
     \myTOC
     \clearpage
}

%%  Sweave specific commands
% make the input blue
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl, formatcom=\color{red}}
% make the output red
\DefineVerbatimEnvironment{Soutput}{Verbatim}{formatcom=\color{blue}}
% get rid of extra Sweave space
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}  

% settings for program listings
\lstset{
language=R,
breaklines=true,
breakatwhitespace=true,
keywordstyle={\ttfamily},
numberstyle = {\ttfamily},
morestring=[b]"
}

\AtBeginDocument{
  \def\labelitemii{\(\circ\)}
}

\makeatother

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                                                  %%%
%%%  IPSUR.Rnw - Introduction to Probability and Statistics Using R  %%%
%%%  Copyright (C) 2009  G. Jay Kerns, <gkerns@ysu.edu>              %%%
%%%  This program is free software: you can redistribute it and/or   %%%
%%%  modify it under the terms of the GNU General Public License as  %%%
%%%  published by the Free Software Foundation, either version 3     %%%
%%%  of the License, or (at your option) any later version.  This    %%%
%%%  program is distributed in the hope that it will be useful,      %%%
%%%  but WITHOUT ANY WARRANTY; without even the implied warranty     %%%
%%%  of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.         %%%
%%%  See the GNU General Public License for more details.  You       %%%
%%%  should have received a copy of the GNU General Public License   %%%
%%%  along with this program.  If not, see                           %%%
%%%  <http://www.gnu.org/licenses/>                                  %%%
%%%                                                                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<echo = FALSE>>=
###  IPSUR.R - Introduction to Probability and Statistics Using R
###  Copyright (C) 2009  G. Jay Kerns, <gkerns@ysu.edu>
###  This program is free software: you can redistribute it and/or modify
###  it under the terms of the GNU General Public License as published by
###  the Free Software Foundation, either version 3 of the License, or
###  (at your option) any later version.
###  This program is distributed in the hope that it will be useful,
###  but WITHOUT ANY WARRANTY; without even the implied warranty of
###  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
###  GNU General Public License for more details.
###  You should have received a copy of the GNU General Public License
###  along with this program.  If not, see <http://www.gnu.org/licenses/>
###################################################
@

<<echo = FALSE>>=
set.seed(42)
#library(random)
#i_seed <- randomNumbers(n = 624, col = 1, min = -1e+09, max = 1e+09)
#.Random.seed[2:626] <- as.integer(c(1, i_seed))
#save.image(file = "seed.RData")
@

<<echo = FALSE>>=
options(useFancyQuotes = FALSE)
#library(prob)
library(RcmdrPlugin.IPSUR)
# Generate RcmdrTestDrive
n <- 168
# generate order 
order <- 1:n
# generate race 
race <- sample(c("White","AfAmer","Asian","Other"), size=n, prob=c(76,13,5,6), replace = TRUE)
race <- factor(race)
# generate gender and smoke 
tmp <- sample(4, size=n, prob=c(12,38,9,41), replace = TRUE) 
gender <- factor(ifelse(tmp < 3,"Male", "Female")) 
smoke <- factor(ifelse(tmp %in% c(1,3), "Yes", "No"))
# generate parking 
parking <- rgeom(n, prob = 0.4) + 1
# generate salary 
m <- 17 + (as.numeric(gender)-1) 
s <- 1 + (2 - as.numeric(gender)) 
salary <- rnorm(n, mean = m, sd = s)
# simulate reduction 
x <- arima.sim(list(order=c(1,0,0), ar=.9), n=n) 
reduction <- as.numeric((20*x + order)/n + 5)
# simulate before and after
before <- rlogis(n, location = 68, scale = 3) 
m <- (as.numeric(smoke)-1)*2.5 
after <- before - rnorm(n, mean = m, sd=0.1)
RcmdrTestDrive <- data.frame(order = order, race = race, smoke = smoke, gender = gender, salary = salary, reduction = reduction, before = before, after = after, parking = parking)
# clean up
remove(list = names(RcmdrTestDrive))
remove(x, n, m, s, tmp)
@

<<echo = FALSE>>=
plot.htest <- function (x, hypoth.or.conf = 'Hypoth',...) { 
require(HH) 
if (x$method == "1-sample proportions test with continuity correction" || x$method == "1-sample proportions test without continuity correction"){
mu <- x$null.value
obs.mean <- x$estimate
n <- NA
std.dev <- abs(obs.mean - mu)/sqrt(x$statistic)
deg.freedom <- NA
if(x$alternative == "two.sided"){
alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
Use.alpha.left <- TRUE
Use.alpha.right <- TRUE
} else if (x$alternative == "less") {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- TRUE
Use.alpha.right <- FALSE
} else {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- FALSE
Use.alpha.right <- TRUE
}
} else if (x$method == "One Sample z-test"){
mu <- x$null.value
obs.mean <- x$estimate
n <- x$parameter[1]
std.dev <- x$parameter[2]
deg.freedom <- NA
if(x$alternative == "two.sided"){
alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
Use.alpha.left <- TRUE
Use.alpha.right <- TRUE
} else if (x$alternative == "less") {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- TRUE
Use.alpha.right <- FALSE
} else {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- FALSE
Use.alpha.right <- TRUE
} 
} else if (x$method == "One Sample t-test" || x$method == "Paired t-test"){
mu <- x$null.value
obs.mean <- x$estimate
n <- x$parameter + 1
std.dev <- x$estimate/x$statistic*sqrt(n)
deg.freedom <- x$parameter
if(x$alternative == "two.sided"){
alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
Use.alpha.left <- TRUE
Use.alpha.right <- TRUE
} else if (x$alternative == "less") {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- TRUE
Use.alpha.right <- FALSE
} else {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- FALSE
Use.alpha.right <- TRUE
}
} else if (x$method == "Welch Two Sample t-test"){
mu <- x$null.value
obs.mean <- -diff(x$estimate)
n <- x$parameter + 2
std.dev <- obs.mean/x$statistic*sqrt(n)
deg.freedom <- x$parameter
if(x$alternative == "two.sided"){
alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
Use.alpha.left <- TRUE
Use.alpha.right <- TRUE
} else if (x$alternative == "less") {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- TRUE
Use.alpha.right <- FALSE
} else {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- FALSE
Use.alpha.right <- TRUE
} 
} else if (x$method == " Two Sample t-test"){
mu <- x$null.value
obs.mean <- -diff(x$estimate)
n <- x$parameter + 2
std.dev <- obs.mean/x$statistic*sqrt(n)
deg.freedom <- x$parameter
if(x$alternative == "two.sided"){
alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
Use.alpha.left <- TRUE
Use.alpha.right <- TRUE
} else if (x$alternative == "less") {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- TRUE
Use.alpha.right <- FALSE
} else {
alpha.right <- 1 - attr(x$conf.int, "conf.level")
Use.alpha.left <- FALSE
Use.alpha.right <- TRUE
}
}
return(normal.and.t.dist(mu.H0 = mu, obs.mean = obs.mean, std.dev = std.dev, n = n, deg.freedom = deg.freedom, alpha.right = alpha.right, Use.obs.mean = TRUE, Use.alpha.left = Use.alpha.left, Use.alpha.right = Use.alpha.right, hypoth.or.conf = hypoth.or.conf))
}
@


\title{\fontsize{36}{42}\selectfont Introduction to Probability\\ and
Statistics Using \textsf{R}}


\date{\noun{First Edition}}


\author{\fontsize{18}{21}\selectfont G.~Jay Kerns}

\maketitle
\noindent \IPSUR: Introduction to Probability and Statistics Using
\textsf{R}

\noindent Copyright \textcopyright~2009 G.~Jay Kerns

\medskip{}


\noindent Permission is granted to copy, distribute and/or modify
this document under the terms of the GNU Free Documentation License,
Version 1.3 or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
Texts. A copy of the license is included in the section entitled ``GNU
Free Documentation License''. 

\tableofcontents{}




\chapter*{Preface}

\addcontentsline{toc}{chapter}{Preface}

Why did I write this book?

The goal for this book is to write a more or less self contained,
essentially complete, correct, textbook. There should be plenty of
exercises for the student, and the problems should have full solutions
for some, and no solutions for others (so that the instructor may
assign them for grading).

For this reason I have constructed this book to have many of the exercises
randomly generated. The concept of the problem remains the same, but
the numbers have changed. This makes it more difficult for students
to copy off of each other and from sharing answers from semester to
semester.

This book was inspired by 
\begin{itemize}
\item Categorical Data Analysis, Agresti ()
\item Forecasting, Time Series, and Regression, 4th Ed., Bowerman, O'Connell,
and Koehler (Duxbury)
\item Mathematical Statistics, Vol. I, 2nd Ed., Bickel and Doksum (Prentice
Hall)
\item Probability and Statistical Inference, 5th Ed., Hogg and Tanis, (Prentice
Hall)
\item Applied Linear Regression Models, 3rd Ed., Neter, Kutner, Nachtsheim,
and Wasserman (Irwin)
\item Statistical Inference, 1st Ed, Casella and Berger (Duxbury)
\item Monte Carlo Statistical Methods, 1st Ed., Robert and Casella (Springer)
\item Introduction to Statistical Thought
\item Using \textsf{R} for Introductory Statistics
\item Introductory Statistics with \textsf{R}
\item Data Analysis and Graphics using \textsf{R}
\end{itemize}
Please bear in mind that the title of this book is {}``Introduction
to Probability and Statistics Using \textsf{R}'', and not {}``Introduction
to \textsf{R} Using Probability and Statistics'', nor even {}``Introduction
to Probability and Statistics and \textsf{R} Using Words''. The goal
is probability and statistics; the tool is \textsf{R}. There are consequently
several important topics about \textsf{R} which some individuals will
feel are underdeveloped, glossed over, or omitted unnecessarily. Some
will feel the same way about the probabilistic and/or statistical
content. Still others will just want to learn \textsf{R} and skip
all of the mathematics.

Despite any misgivings: here it is. I humbly invite said individuals
to take this book -- with the GNU-FDL in hand -- and make it better.
In that spirit there are many ways in which this book could be improved:
\begin{description}
\item [{Better~data:}] the data analyzed in this book are almost entirely
from the \inputencoding{latin9}\lstinline[showstringspaces=false]!datasets!\inputencoding{utf8}
package in base \textsf{R}. There are at least three reasons for this: 

\begin{enumerate}
\item I made a conscious effort to minimize dependence on contributed packages,
\item The data are instantly available, already in the correct format, and
we do not need to waste time managing them, and
\item The data are \emph{real}.
\end{enumerate}
I made no attempt to choose data sets that would be interesting to
the students; rather, data were chosen for their potential to convey
a statistical point. Many of the datasets are decades old, or more
(for instance, the data used to introduce simple linear regression
are the speeds and stopping distances of cars in the 1920's).

In a perfect world with infinite time I would research and contribute
recent, \emph{real} data in a context crafted to engage the students
in \emph{every} example. One day I hope to stumble over that time.
In the meantime, I will add new data sets incrementally as time permits.

\item [{More~proofs:}] for the sake of completeness. Many proofs have
been skipped. There is no rhyme or reason to the current omissions.
I will add more proofs as time permits. 
\item [{More~and~better~graphics:}] I have not used the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,language=R]!ggplot2!\inputencoding{utf8}
package because I do not know how to use it yet. It is on my to-do
list.
\item [{More~and~better~exercises:}] There are not nearly enough exercises
in the first edition. I have not used the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,language=R]!exams!\inputencoding{utf8}
package, but I believe that it is a right way to move forward with
this book. As I learn more about what the package can do I would like
to incorporate it into later editions of this book.
\end{description}

\section*{About This Document}

\IPSUR\ contains many interrelated parts: the \textbf{\emph{Document}},
the \textbf{\emph{Program}}, the \textbf{\emph{Package}}, and the
\textbf{\emph{Ancillaries}}. In short, the \emph{Document} is what
you are reading right now. The \emph{Program} provides an efficient
means to modify the Document. The \emph{Package} is an \textsf{R}
package that houses the Program and the Document. Finally, the \emph{Ancillaries}
are extra materials produced by the Program to supplement use of the
Document. We briefly describe each of them below.


\subsection*{The Document}

The \emph{Document} is that which you are reading right now -- \IPSUR's
\emph{raison d'\^etre}. There are transparent copies (nonproprietary
text files) and opaque copies (everything else). See the GNU Free
Documentation License (GNU-FDL) in Appendix BLANK for more precise
language and details.
\begin{description}
\item [{\texttt{IPSUR.tex}}] is a transparent copy of the Document to be
typeset with a \LaTeX{} distribution such as Mik\TeX{} or \TeX{} Live.
Any reader is free to modify the Document and release the modified
version in accordance with the provisions of the GNU-FDL. Note that
this file cannot be used to generate a randomized copy of the Document.
Indeed, in its released form it is only capable of typesetting the
exact version of \IPSUR\ which you are currently reading. Furthermore,
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.tex!\inputencoding{utf8}
file is unable to generate any of the ancillary materials. 
\item [{\texttt{IPSUR-xxx.eps},~\texttt{IPSUR-xxx.pdf}}] are the image
files for every graph in the Document. These are needed when typesetting
with \LaTeX{}.
\item [{\texttt{IPSUR.pdf}}] is an opaque copy of the Document. This is
the file that instructors will likely want to distribute to students.
\item [{\texttt{IPSUR.dvi}}] is another opaque copy of the Document in
a different file format.
\end{description}

\subsection*{The Program}

The \emph{Program} includes \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.lyx!\inputencoding{utf8}
and its nephew \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.Rnw!\inputencoding{utf8};
the purpose of each is to give instructors a way to quickly customize
the Document for their particular class of students by means of randomly
regenerating the Document with brand new data, exercises, student
and instructor solution manuals, and other ancillaries. 
\begin{description}
\item [{\texttt{IPSUR.lyx}}] is the source \LyX{} file for the Program,
released under the GNU General Public License (GNU GPL) Version 3.
This file is opened, modified, and compiled with \LyX{}, a sophisticated
open-source document processor, and may be used (together with Sweave)
to generate a randomized, modified copy of the Document with brand
new data sets for some of the exercises, and the solution manuals.
Additionally, \LyX{} can easily activate/deactivate entire blocks
of the document, \emph{e.g.~}the \textsf{proofs} of the theorems,
the student \textsf{solutions} to the exercises, or the instructor
\textsf{answers} to the problems, so that the new author may choose
which sections (s)he would like to include in the final Document.
The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.lyx!\inputencoding{utf8}
file is all that a person needs (in addition to a properly configured
system -- see Appendix BLANK) to generate/compile/export to all of
the other formats described above and below, which includes the ancillary
materials \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.Rdata!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.R!\inputencoding{utf8}.
\item [{\texttt{IPSUR.Rnw}}] is another form of the source code for the
Program, also released under the GNU GPL Version 3. It was produced
by exporting \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.lyx!\inputencoding{utf8}
into R/Sweave format (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.Rnw!\inputencoding{utf8}).
This file may be processed with Sweave to generate a randomized copy
of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.tex!\inputencoding{utf8}
-- a transparent copy of the Document -- together with the ancillary
materials \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.Rdata!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.R!\inputencoding{utf8}.
Please note, however, that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.Rnw!\inputencoding{utf8}
is just a simple text file which does not support many of the extra
features that \LyX{} offers such as WYSIWYM editing, instantly (de)activating
branches of the manuscript, and more. 
\end{description}

\subsection*{The Package}

There is a contributed package on \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8},
called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR!\inputencoding{utf8}.
The package has two purposes. The first is to house the Document in
an easy-to-access medium. Indeed, a student can have the Document
at his/her fingertips with only three commands:

<<eval = FALSE>>=
install.packages(IPSUR)
library(IPSUR)
read(IPSUR)
@

The second purpose goes hand in hand with the Document's license;
since \IPSUR\ is free, the source code must be freely available to
anyone that wants it. Hosting the package on \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8}
satisfies this requirement nicely.


\subsection*{Ancillary Materials}

These are extra materials that accompany \IPSUR\.
\begin{description}
\item [{\texttt{IPSUR.RData}}] is a saved image of the \textsf{R} workspace
at the completion of the Sweave processing of \IPSUR. This can be
loaded into memory with \textsf{File} $\triangleright$ \textsf{Load
Workspace} or with the command \inputencoding{latin9}\lstinline[showstringspaces=false]!load("/path/to/IPSUR.Rdata")!\inputencoding{utf8}.
Loading it into \textsf{R} will make every single object in the \textsf{R}
workspace immediately available and in memory. In particular, the
data BLANK from Exercise BLANK in Chapter BLANK on page BLANK will
be loaded. Type BLANK at the command line to see for yourself. 
\item [{\texttt{IPSUR.R}}] is the exported \textsf{R} code from \inputencoding{latin9}\lstinline[showstringspaces=false]!IPSUR.Rnw!\inputencoding{utf8}.
With this script, literally every \textsf{R} command from the entirety
of \IPSUR\ can be resubmitted at the command line. Note that the
\inputencoding{latin9}\lstinline[showstringspaces=false]!set.seed!\inputencoding{utf8}
line at the top should be deleted before distributing to students.
\end{description}

\section*{Notation}

We use the notation \inputencoding{latin9}\lstinline[showstringspaces=false]!x!\inputencoding{utf8}
for simple objects, and use \inputencoding{latin9}\lstinline[showstringspaces=false]!stem.leaf!\inputencoding{utf8}
notation to denote functions. The sequence {}``\textsf{Statistics}
\textsf{$\triangleright$} \textsf{Summaries} \textsf{$\triangleright$}
\textsf{Active Dataset}'' means to click the \textsf{Statistics}
menu item, next click the \textsf{Summaries} submenu item, and finally
click \textsf{Active Dataset}.

\listoffigures
\addcontentsline{toc}{chapter}{List of Figures} 

\listoftables
\addcontentsline{toc}{chapter}{List of Tables} 


\chapter{An Introduction to Probability and Statistics}

\pagenumbering{arabic} 


\section{Probability}

Probability concerns the study of uncertainty. Games of chance have
been played for millenia. 

The common folklore is that probability has been around for years
but did not gain the attention of mathematicians until approximately
1654 when Chevalier de Mere had a problem dividing the payoff to two
players in a game that must end prematurely.


\section{Statistics}

Statistics concerns data; their collection, analysis, and interpretation.
In this book we distinguish between two types of statistics: descriptive
and inferential. 

Loosely speaking, descriptive statistics concerns the summarization
of data. We have a data set and we would like to describe the data
set in multiple ways. Usually this entails calculating numbers from
the data, called descriptive measures. Examples are sum, averages,
percentages, and so forth.

Inferential statistics does more. There is an inference associated
with the data set.

The word statistics seems to have come from the Latin root \emph{status},
or state, and the word \emph{Statistik} was the German word for political
science. It was important to describe the electorate, so information
collection and data keeping became important.

Sir Francis Galton

It was important for Legendre who studied how to collect astronomical
measurements effectively. He was a pioneer in least squares.

Some point later Adolphe Quetelet

Cousin of Charles Darwin

Sir Ronald Aylmer Fisher and Pearson

I would like to mention that there are two schools of thought of statistics:
Frequentist and Bayesian. The difference between the schools is related
to how the two groups interpret the underlying probability (see Section
BLANK). The frequentist school gained a lot of ground among statisticians
due in large part to the work of Fisher, Neyman, and Pearson in the
early twentieth century. That dominance lasted until inexpensive computing
power became widely available; nowadays the Bayesian school is garnering
more attention, at an increasing rate.

This book is devoted mostly to the frequentist viewpoint because that
is how I was trained, with the conspicuous exception of Sections BLANK,
BLANK, and BLANK. I plan to add more Bayesian material in later editions
of this book. 


\chapter{An Introduction to \textsf{R}}

\pagenumbering{arabic} 

What would I like them to know?
\begin{itemize}
\item don't forget to mention rounding issues
\item basic information about how to install, start up, and interact with
R

\begin{itemize}
\item different platforms, the console, the terminal
\item external programs such as Tinn-R, Emacs, or Eclipse
\end{itemize}
\item how to use \textsf{R} like a calculator (essentially arithmetic with
\textsf{R})

\begin{itemize}
\item basic mathematical functions
\item would like to mention complex arithmetic
\end{itemize}
\item what variables are and how to name them
\item about vectors

\begin{itemize}
\item the different types (numeric, character, logical, missing)
\item how to access different parts of a vector
\end{itemize}
\item how to type in data, with c() and scan() enter data, 
\item how to import data frames from packages, and how to import data from
elsewhere ? (this last one)
\item need to know about vectors and (data frames) the different types of
vectors (numeric, character, logical)
\item how to get help

\begin{itemize}
\item mailing lists
\item manuals
\item see the source code
\item ? and ??
\end{itemize}
\item the concept of add on packages, how to download, and how to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!library()!\inputencoding{utf8}
them
\item some frequently asked questions

\begin{itemize}
\item they need to know about finite precision arithmetic
\end{itemize}
\item basic tricks of the trade like command history and clearing the console,
case sensitivity
\end{itemize}
This chapter is designed to help a person to begin to get to know
the \textsf{R} statistical computing environment. It paraphrases and
summarizes information gleaned from materials listed in the \textbf{References}.
Please refer to them for a more complete treatment.


\section{Downloading and Installing \textsf{R}}

The instructions for obtaining \textsf{R} largely depend on the user's
hardware and operating system. The \textsf{R} Project has written
an \textsf{R} Installation and Administration manual with complete,
precise instructions about what to do, together with all sorts of
additional information. The following is just a primer to get a person
started.


\subsection{Installing \textsf{R}}

Visit one of the links below to download the latest version of \textsf{R}
for your operating system:
\begin{description}
\item [{Microsoft~Windows:}] \url{http://cran.r-project.org/bin/windows/base/} 
\item [{MacOS:}] \url{http://cran.r-project/bin/macosx}
\item [{Linux:}] \url{http://cran.r-project/bin/linux}
\end{description}
On MS-Windows, click the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.exe!\inputencoding{utf8}
program file to start installation. When it asks for \textquotedbl{}Customized
startup options\textquotedbl{}, specify \textsf{Yes}. In the next
window, be sure to select the SDI (single-window) option; this is
useful later when we discuss three dimensional plots with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rgl!\inputencoding{utf8}
package.


\subsection{Installing and Loading Add-on Packages}

There are \emph{base} packages (which come with \textsf{R} automatically),
and \emph{contributed} packages (which must be downloaded for installation).
For example, on the version of \textsf{R} being used for this document
the default base packages loaded at startup are 

<<>>=
getOption("defaultPackages")
@

The base packages are maintained by a select group of volunteers,
called {}``\textsf{R} Core''. In addition to the base packages,
there are literally thousands of additional contributed packages written
by individuals all over the world. These are stored worldwide on mirrors
of the Comprehensive \textsf{R} Archive Network, or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8}
for short. Given an active Internet connection, anybody is free to
download and install these packages and even inspect the source code.

To install a package named \inputencoding{latin9}\lstinline[showstringspaces=false]!foo!\inputencoding{utf8},
open up \textsf{R} and type \inputencoding{latin9}\lstinline[showstringspaces=false]!install.packages("foo")!\inputencoding{utf8}.
To install \inputencoding{latin9}\lstinline[showstringspaces=false]!foo!\inputencoding{utf8}
and additionally install all of the other packages on which \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!foo!\inputencoding{utf8}
depends, instead type \inputencoding{latin9}\lstinline[showstringspaces=false]!install.packages("foo", depends = TRUE)!\inputencoding{utf8}.

The general command \inputencoding{latin9}\lstinline[showstringspaces=false]!install.packages()!\inputencoding{utf8}
will (on most operating systems) open a window contaning a huge list
of available packages; simply choose one or more to install.

No matter how many packages are installed onto the system, each one
must first be loaded for use with the \inputencoding{latin9}\lstinline[showstringspaces=false]!library!\inputencoding{utf8}
function. For instance, the \inputencoding{latin9}\lstinline[showstringspaces=false]!foreign!\inputencoding{utf8}
package (in the base distribution) contains all sorts of functions
needed to import data sets into \textsf{R} from other software such
as SPSS, SAS, \emph{etc}. But none of those functions will be available
until the command \inputencoding{latin9}\lstinline[showstringspaces=false]!library(foreign)!\inputencoding{utf8}
is issued. 

Simply type \inputencoding{latin9}\lstinline[showstringspaces=false]!library()!\inputencoding{utf8}
at the command prompt (described below) to see a list of all available
packages in your library.

For complete, precise information regarding installation of \textsf{R}
and add-on packages, see the \textsf{R} Installation and Administration
manual, \url{http://cran.r-project.org/manuals.html}.


\section{Communicating with \textsf{R}}

There are three basic methods for communicating with \textsf{R}.
\begin{enumerate}
\item At the command prompt (\texttt{>}).


This is the most basic way to complete simple, one-line commands.
\textsf{R} will evaluate what is typed there and output the results
in the Console Window.

\item Copy \& Paste from a text file.


Another way is to open a text file with a text editor (say, NotePad
or Microsoft$\circledR$ Word). The user writes code in the text file,
then when satisfied, (s)he copy-pastes it at the command prompt in
\textsf{R}. Then \textsf{R} will run all of the code at once and give
output in the Console Window.

A disadvantage to this method is that all of the code is written in
the same way with the same font. It can become confusing with longer
scripts, and it is more difficult to efficiently identify mistakes
in the code. To address this problem, software developers have designed
powerful \emph{script editors}.

\item Graphical User Interfaces (GUIs): These are actually much more general
than discussed here.

\begin{enumerate}
\item \textsf{R} Gui
\item The \textsf{R} Commander
\item PMG: Poor Man's GUI
\item Rattle
\item JGR (sounds like {}``jaguar'')
\end{enumerate}
\item IDE / Script Editors.


These are programs specially designed to aid the communication and
code writing process. The advantage to using Script Editors is that
they have additional functions and options to help the user write
code more efficiently, including \textsf{R} syntax highlighting, automatic
code completion, delimiter matching, and dynamic help on the \textsf{R}
functions as they are written. In addition, they typically have all
of the text editing features of programs like Microsoft$\circledR$
Word. Lastly, most script editors are fully customizable in the sense
that the user can customize the appearance of the interface to choose
what colors to display, when to display them, and how to display them.

Some of the more popular script editors can be downloaded from the
R-Project website at \url{http://www.sciviews.org/_rgui/}. On the
left side of the screen (under \textbf{Projects}) there are several
choices available.
\begin{itemize}
\item \textsf{\textbf{R}}\textbf{WinEdt}: This option is coordinated with
WinEdt for \LaTeX{} and has features such as code highlighting, remote
sourcing, and many other goodies. However, one first needs to download
and install WinEdt, and even then it is only free for a while. Annoying
windows will eventually pop-up asking for a registration code. This
is nevertheless a fine choice if you are familiar with \LaTeX{} and
own WinEdt already, or are planning to purchase WinEdt in the near
future.
\item \textbf{Tinn-}\textsf{\textbf{R}}: This one has the advantage of being
completely free. It has all of the above mentioned options and lots
more. It is simple enough to use that the user can virtually begin
working with it immediately after installation. But this particular
choice is only available for Microsoft$\circledR$ Windows operating
systems. If you are on MacOS or Linux, a comparable alternative is
Sci-Views - Komodo Edit.
\item \textbf{Bluefish}: This open-source script editor is for Mac OSX users.
Other alternatives for Mac users are SubEthaEdit, AlphaTk, and Eclipse.
I have tried Eclipse only, and only briefly, so I cannot comment on
their strengths and weaknesses. Try them out, and let me know!
\item \textbf{Emacs} / \textbf{ESS}: Click Emacs (ESS) or Emacs (ESS/Windows).
This will take you to download sites with sophisticated programs for
editing, compiling, and coordinating software such as \texttt{S-Plus},
\textsf{R}, and \texttt{SAS} simultaneously. Emacs is short for \emph{E}diting
\emph{MAC}ro\emph{S} and ESS means \emph{E}macs \emph{S}peaks \emph{S}tatistics.
An alternate branch of Emacs is called XEmacs. This editor is -- \emph{by
far} -- the most powerful of the text editors, but all of the flexibility
comes at a price. Emacs requires a level of computer-savvy that the
others do not, and the learning curve is much more steep.

\begin{itemize}
\item Emacs is an all purpose text editor. It can do absolutely anything
with respect to modifying, searching, editing, and manipulating, text.
And if Emacs can't do it, then you extend Emacs by writing a program
in the Lisp language.
\item In particular, a team of volunteers have written an Emacs extension
called ESS, which stands for Emacs Speaks Statistics. Using ESS, one
can speak to \textsf{R} and do all of the tricks that the other script
editors offer, and much, much, more.
\item If you want to learn Emacs, and if you grew up with Microsoft$\circledR$
Windows or Macintosh, then you are going to need to relearn everything
you thought you knew about computers your whole life. (Or, since Emacs
is completely customizable, you can reconfigure Emacs to behave like
you want.)
\end{itemize}
\end{itemize}
\end{enumerate}
Communicating with R


\paragraph*{One line at a time}
\begin{enumerate}
\item Rgui (Windows)
\item RalphaTkGUI
\item Terminal
\item Emacs/ESS, XEmacs
\end{enumerate}

\paragraph*{Multiple lines at a time}

For longer programs (called \emph{scripts}) there is too much code
to write all at once at the command prompt. Furthermore, for longer
scripts it is convenient to be able to only modify a certain piece
of the script and run it again in \textsf{R}. 
\begin{enumerate}
\item \textsf{R} Editor (Windows): In Microsoft$\circledR$ Windows, \textsf{R}
provides its own built-in script editor, called \textsf{R} Editor.
From the console window, select \textsf{File}\emph{ }\textsf{$\triangleright$}\emph{
}\textsf{New}\textsf{\emph{ }}\textsf{Script}\emph{.} A script window
opens, and the lines of code can be written in the window. When satisfied
with the code, the user highlights all of the commands and presses
\textsf{Ctrl+R}. The commands are automatically run at once in \textsf{R}
and the output is shown. To save the script for later, click \textsf{File}\emph{
}\textsf{$\triangleright$}\emph{ }\textsf{Save as...} in \textsf{R}
Editor. The script can be reopened later with \textsf{File}\emph{
}\textsf{$\triangleright$}\emph{ }\textsf{Open Script...} in RGui.
\item Tinn-\textsf{R}/Sciviews-K
\item Emacs/ESS:
\item JGR (read {}``Jaguar''): based on Java, so it is cross-platform.
\item Kate, etc.
\end{enumerate}

\paragraph*{Graphical User Interfaces (GUIs)}

The usual way to interact with \textsf{R} in Microsoft$\circledR$
Windows is with \textsf{Rgui}, mentioned above.
\begin{enumerate}
\item The \textsf{R} Commander (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}):
this one provides a point-and-click interface to many basic statistical
tasks. It is called the {}``Commander'' because every time one makes
a selection from the menus, the code corresponding to the task is
listed in the output window. One can take this code, copy-and-paste
it to a text file, then re-run it again at a later time without the
\textsf{R} Commander's assistance. It is well suited for the introductory
level.\\
In addition, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
allows for user-contributed {}``Plugins'' which are separate packages
on \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8}
that add extra functionality to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
package. The plugins are typically named with the prefix \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrPlugin!\inputencoding{utf8}
to make them easy to identify in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8}
package list.
\item Poor Man's GUI (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!pmg!\inputencoding{utf8}):
this is an alternative to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
which is based on GTk instead of Tcl/Tk. Benefits include drag-and-drop
datasets for plots.
\item Rattle (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rattle!\inputencoding{utf8}):
this GUI was specifically designed for data mining applications but
it provides enough other general functionality to merit mention here.
\item Others: there are many more GUIs which exist but which the author
has tried only in passing: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RKward!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RPad!\inputencoding{utf8}.
\end{enumerate}

\section{Basic \textsf{R} Operations and Concepts}

The \textsf{R} developers have written an introductory document entitled,
{}``Introduction to \textsf{R}''. There is a sample session included
which shows what basic interaction with \textsf{R} looks like. I recommend
that all new users of \textsf{R} read this document, but there are
concepts mentioned there which will be unfamiliar to the beginner.

Below are some of the most basic operations that can be done with
\textsf{R}. Almost every book about \textsf{R} begins with a section
like the one below; look around to see all sorts of things that can
be done at this most basic level.


\subsection{Arithmetic}

<<keep.source = TRUE>>=
2 + 3       # add
4 * 5 / 6   # multiply and divide
7^8         # 7 to the 8th power
@

Notice the comment character \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!#!\inputencoding{utf8}.
Anything typed after a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!#!\inputencoding{utf8}
symbol is ignored by \textsf{R}. We know that $20/6$ is a repeating
decimal, but the above example shows only 7 digits. We can change
the number of digits displayed with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!options!\inputencoding{utf8}:

<<keep.source = TRUE>>=
options(digits = 16)
10/3                 # see more digits
sqrt(2)              # square root
exp(1)               # Euler's constant, e
pi       
options(digits = 7)  # back to default
@

Note that it is possible to set \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!digits!\inputencoding{utf8}
up to 22, but setting them over 16 is not recommended (the extra significant
digits are not necessarily reliable). Above notice the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sqrt!\inputencoding{utf8}
function for square roots and the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!exp!\inputencoding{utf8}
function for powers of $\me$, Euler's constant.


\subsection{Assignment, Object names, and Data types}

It is often convenient to assign numbers and values to variables (objects)
to be used later. The proper way to assign values to a variable is
with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!<-!\inputencoding{utf8}
operator (with a space on either side). The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!=!\inputencoding{utf8}
symbol works too, but it is recommended by the \textsf{R} masters
to reserve \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!=!\inputencoding{utf8}
for specifying arguments to functions (discussed later). In this book
we will follow their advice and use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!<-!\inputencoding{utf8}
for assignment. Once a variable is assigned, its value can be printed
by simply entering the variable name by itself.

<<keep.source = TRUE>>=
x <- 7*41/pi   # don't see the calculated value
x              # take a look
@

When choosing a variable name you can use letters, numbers, dots {}``\texttt{.}'',
or underscore {}``\texttt{\_}'' characters. You cannot use mathematical
operators, and a leading dot may not be followed by a number. Examples
of valid names are: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x1!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y.value!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},language=Caml]!y_hat!\inputencoding{utf8}.
(More precisely, the set of allowable characters in object names depends
on one's particular system and locale; see An Introduction to \textsf{R}
for more discussion on this.)

Objects can be of many \emph{types}, \emph{modes}, and \emph{classes}.
At this level, it is not necessary to investigate all of the intricacies
of the respective types, but there are some with which you need to
become familiar:
\begin{description}
\item [{integer:}] the values $0$, $\pm1$, $\pm2$, \ldots{}; these
are represented exactly by \textsf{R}.
\item [{double:}] real numbers (rational and irrational); these numbers
are not represented exactly (save integers or fractions with a denominator
that is a multiple of 2, see An Introduction to \textsf{R}).
\item [{character:}] elements that are wrapped with pairs of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!'!\inputencoding{utf8};
\item [{logical:}] includes \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!TRUE!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!FALSE!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}
(which are reserved words); the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}
stands for {}``not available'', \emph{i.e.}, a missing value.
\end{description}
You can determine an object's type with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!typeof!\inputencoding{utf8}
function. In addition to the above, there is the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!complex!\inputencoding{utf8}
data type:

<<five, keep.source = TRUE>>=
sqrt(-1)            # isn't defined
sqrt(-1+0i)         # is defined
(0 + 1i)^2          # should be -1
typeof((0 + 1i)^2)
@

Note that you can just type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!(1i)^2!\inputencoding{utf8}
to get the same answer. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NaN!\inputencoding{utf8}
stands for {}``not a number''; it is represented internally as \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!double!\inputencoding{utf8}. 


\subsection{Vectors}

All of this time we have been manipulating vectors of length 1. Now
let us move to vectors with multiple entries.


\paragraph*{Entering data vectors}
\begin{enumerate}
\item \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!c!\inputencoding{utf8}:
If you would like to enter the data \texttt{74,31,95,61,76,34,23,54,96}
into \textsf{R}, you may create a data vector with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!c!\inputencoding{utf8}
function (which is short for \emph{concatenate}).


<<>>=
x <- c(74, 31, 95, 61, 76, 34, 23, 54, 96)
x
@

\item \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scan!\inputencoding{utf8}:
This method is useful when the data are stored somewhere else. For
instance, you may type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x <- scan()!\inputencoding{utf8}
at the command prompt and \textsf{R} will display \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1:!\inputencoding{utf8}
to indicate that it is waiting for the first data value. Type a value
and press \textsf{Enter}, at which point \textsf{R} will display \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!2:!\inputencoding{utf8},
and so forth. Note that entering an empty line stops the scan. This
method is especially handy when you have a column of values, say,
stored in a text file or spreadsheet. You may copy and paste them
all at the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1:!\inputencoding{utf8}
prompt, and \textsf{R} will store all of the values instantly in the
vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
\item repeated data; regular patterns: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!LETTERS!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!letters!\inputencoding{utf8}
\end{enumerate}
The type of a vector is usually taken by \textsf{R} to be the most
general type of any of its elements.


\paragraph*{Indexing data vectors}
\begin{enumerate}
\item workspace: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!objects()!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ls()!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rm()!\inputencoding{utf8}
\end{enumerate}

\subsection{Functions and Expressions}

A function takes arguments as input and returns an object as output.

<<keep.source = TRUE>>=
x <- 1:5
sum(x)
length(x)
min(x)
mean(x)      # sample mean
sd(x)        # sample standard deviation
@

The great thing about \textsf{R} is that it is open-source, which
means that anybody is free to look under the hood of a function and
see how things are calculated. For accessing the sources see this
article BLANK. In short:
\begin{enumerate}
\item Type the name of the function without any parentheses or arguments.
If you are lucky then the code for the entire function will be printed,
right there looking at you. For instance, suppose that we would like
to see how the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!intersect!\inputencoding{utf8}
function works:


<<>>=
intersect
@

\item If instead it shows \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!UseMethod("!\inputencoding{utf8}\emph{something}\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!")!\inputencoding{utf8}then
you will need to identify the \emph{class} of the object to be inputted
and next look at the \emph{method} that will be \emph{dispatched}
to the object. For instance, typing \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rev!\inputencoding{utf8}
says 


<<>>=
rev
@

The output is telling us that there are multiple methods associated
with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rev!\inputencoding{utf8}
function. To see what these are, type

<<>>=
methods(rev)
@

Now we learn that there are two different \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rev(x)!\inputencoding{utf8}
functions, which are chosen depending on what \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
is. There is one for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dendrogram!\inputencoding{utf8}
objects and a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!default!\inputencoding{utf8}
method for everything else. Simply type the name to see what each
method does. For example, the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!default!\inputencoding{utf8}
method can be viewed with

<<>>=
rev.default
@

\item Some functions are hidden by a \emph{namespace} (see the \textsf{R}
Manual BLANK), and are not visible on the first try. For example,
if we try to look at the code for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!wilcox.test!\inputencoding{utf8}
(see Chapter BLANK) we get the following:


<<>>=
wilcox.test
methods(wilcox.test)
@

If we were to try \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!wilcox.test.default!\inputencoding{utf8}
we would get a {}``not found'' error, because it is hidden behind
the namespace for the package \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stats!\inputencoding{utf8}
(shown in the last line when we tried \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!wilcox.test!\inputencoding{utf8}).
In cases like these we prefix the package name to the front of the
function name with three colons; the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stats:::wilcox.test.default!\inputencoding{utf8}
will show the source code, omitted here for brevity.

\item If it shows \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.Internal(!\inputencoding{utf8}\emph{something}\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!)!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.Primitive("!\inputencoding{utf8}\emph{something}\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!")!\inputencoding{utf8},
then it will be necessary to download the source code of \textsf{R}
(which is \emph{not} a binary version with an \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.exe!\inputencoding{utf8}
extension) and search inside the code there. See Ligges for more discussion
on this. An example is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!exp!\inputencoding{utf8}:


<<>>=
exp
@

Be warned that most of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.Internal!\inputencoding{utf8}
functions are written in other computer languages which the beginner
may not understand, at least initially.

\end{enumerate}

\section{Getting Help}

When you are using \textsf{R}, it will not take long before you find
yourself needing help. Fortunately, \textsf{R} has extensive help
resources and you should immediately become familiar with them. Begin
by clicking \textsf{Help} on \textsf{R} GUI. The following options
are available.
\begin{itemize}
\item \textbf{Console}: gives useful shortcuts, for instance, \textsf{Ctrl+L},
to clear the \textsf{R} console screen.
\item \textbf{FAQ on }\textsf{\textbf{R}}: frequently asked questions concerning
general \textsf{R} operation.
\item \textbf{FAQ on }\textsf{\textbf{R}}\textbf{ for Windows}: frequently
asked questions about \textsf{R}, tailored to the Microsoft Windows
operating system.
\item \textbf{Manuals}: technical manuals about all features of the \textsf{R}
system including installation, the complete language definition, and
add-on packages.
\item \textsf{\textbf{R}}\textbf{ functions (text)\ldots{}}: use this if
you know the \emph{exact} name of the function you want to know more
about, for example, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plot!\inputencoding{utf8}.
Typing \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8}
in the window is equivalent to typing \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!help("mean")!\inputencoding{utf8}
at the command line, or more simply, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!?mean!\inputencoding{utf8}.
Note that this method only works if the function of interest is contained
in a package that is already loaded into the search path with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!library!\inputencoding{utf8}.
\item \textbf{HTML Help}: use this to browse the manuals with point-and-click
links. It also has a Search Engine \& Keywords for searching the help
page titles, with point-and-click links for the search results. This
is possibly the best help method for beginners. It can be started
from the command line with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!help.start()!\inputencoding{utf8}.
\item \textbf{Search help\ldots{}}: use this if you do not know the exact
name of the function of interest, or if the function is in a package
that has not been loaded yet. For example, you may enter \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plo!\inputencoding{utf8}
and a text window will return listing all the help files with an alias,
concept, or title matching `\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plo!\inputencoding{utf8}'
using regular expression matching; it is equivalent to typing \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!help.search("plo")!\inputencoding{utf8}
at the command line. The advantage is that you do not need to know
the exact name of the function; the disadvantage is that you cannot
point-and-click the results. Therefore, one may wish to use the HTML
Help search engine instead. An equivalent way is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!??plo!\inputencoding{utf8}
at the command line.
\item \textbf{search.r-project.org\ldots{}}: this will search for words
in help lists and email archives of the \textsf{R} Project. It can
be very useful for finding other questions that other users have asked.
\item \textbf{Apropos\ldots{}}: use this for more sophisticated partial
name matching of functions. See \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!?apropos!\inputencoding{utf8}
for details.
\end{itemize}
On the help pages for a function there are sometimes {}``Examples''
listed at the bottom of the page, which will work if copy-pasted at
the command line ( unless marked otherwise). The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!example!\inputencoding{utf8}
function will run the code automatically, skipping the intermediate
step. For instance, we may try \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!example(mean)!\inputencoding{utf8}
to see a few examples of how the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8}
function works.


\subsection{\textsf{R} Help Mailing Lists}

There are several mailing lists associated with \textsf{R}, and there
is a huge community of people that read and answer questions related
to \textsf{R}. See here \url{http://www.r-project.org/mail.html}
for an idea of what is available. Particularly pay attention to the
bottom of the page which lists several special interest groups (SIGs)
related to \textsf{R}.

Bear in mind that \textsf{R} is free software, which means that it
was written by volunteers, and the people that frequent the mailing
lists are also volunteers who are not paid by customer support fees.
Consequently, if you want to use the mailing lists for free advice
then you must adhere to some basic etiquette, or else you may not
get a reply, or even worse, you may receive a reply which is a bit
less cordial than you are used to. Below are a few considerations
which the author would like to highlight.
\begin{enumerate}
\item Read the FAQ (\url{http://cran.r-project.org/faqs.html}). Note that
there are different FAQs for different operating systems. You should
read these now, even without a question at the moment, to learn a
lot about the idiosyncracies of \textsf{R}.
\item Search the archives. Even if your question is not a FAQ, there is
a very high likelihood that your question has been asked before on
the mailing list. If you want to know about topic \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!foo!\inputencoding{utf8},
then you can do \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RSiteSearch("foo")!\inputencoding{utf8}
to search the mailing list archives (and the online help) for it. 
\item Do a Google search. 
\end{enumerate}
If your question is not a FAQ, has not been asked on \textsf{R}-help
before, and does not yield to a Google (or alternative) search, then,
and only then, should you even consider asking \textsf{R}-help. Below
are a few considerations that the author would like to highlight. 
\begin{enumerate}
\item \textbf{Read the posting guide (\url{http://www.r-project.org/posting-guide.html})
before posting.} This will save you a lot of trouble and pain.
\item Get rid of the command prompts (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!>!\inputencoding{utf8})
from output. Readers of your message will take the text from your
mail and copy-paste into an \textsf{R} session. If you make the readers'
job easier then it will increase the likelihood of a response.
\item Questions are often related to a specific data set, and the best way
to communicate the data is with a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dump!\inputencoding{utf8}
command. For instance, if your question involves data stored in a
vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
you can type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dump("x","")!\inputencoding{utf8}
at the command prompt and copy-paste the output into the body of your
email message. Then the reader may easily copy-paste the message from
your email into \textsf{R} and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
will be available to him/her.
\item Sometimes the answer the question is related to the operating system
used, the attached packages, or the exact version of \textsf{R} being
used. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sessionInfo()!\inputencoding{utf8}
command collects all of this information to be copy-pasted into an
email.
\end{enumerate}

\section*{Some References}
\begin{itemize}
\item communicating with R

\begin{itemize}
\item at the command line
\item text editor
\item the \textsf{R} Commander
\end{itemize}
\item Data

\begin{itemize}
\item types of data
\item entering data
\item reading in data
\item built in data
\end{itemize}
\item working with data

\begin{itemize}
\item assignment
\item continuation prompt
\item regular sequences
\item Normally all alphanumeric symbols are allowed1 (and in some countries
this includes accented letters) plus . and \_, with the restriction
that a name must start with . or a letter, and if it starts with
. the second character must not be a digit. 
\item vectorizing functions
\end{itemize}
\item quitting R

\begin{itemize}
\item workspaces
\end{itemize}
\end{itemize}

\section{External resources}
\begin{itemize}
\item R project
\item CRAN
\item R Wiki
\item R Graph Gallery
\item Other
\end{itemize}

\section{Other tips}

It is unnecessary to retype commands repeatedly, since \textsf{R}
remembers what you have recently entered on the command line. On the
Microsoft$\circledR$ Windows \textsf{R}Gui, to cycle through the
previous commands just push the $\uparrow$ (up arrow) key. On Emacs/ESS
the command is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Alt-p!\inputencoding{utf8}
(which means hold down the \textsf{Alt} button and press {}``\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!p!\inputencoding{utf8}'').
More generally, the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!history()!\inputencoding{utf8}
will show a whole list of recently entered commands.

Missing values in \textsf{R} are denoted by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}.
Operations on data vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}
values treat them as if the values can't be found. This means adding
(as well as subtracting and all of the other mathematical operations)
a number to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}
results in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}.

To find out what all variables are in the current work environment,
use the commands \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!objects()!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ls()!\inputencoding{utf8}.
These list all available objects in the workspace. If you wish to
remove one or more variables, use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!remove(var1, var2, var3)!\inputencoding{utf8},
or more simply use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rm(var1, var2, var3)!\inputencoding{utf8},
and to remove all objects use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rm(list = ls())!\inputencoding{utf8}.
\begin{enumerate}
\item Another use of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scan!\inputencoding{utf8}
is when you have a long list of numbers (separated by spaces or on
different lines) already typed somewhere else, say in a text file.
To enter all the data in one fell swoop, first highlight and copy
the list of numbers to the Clipboard with \textsf{Edit}\emph{ }\textsf{$\triangleright$}\emph{
}\textsf{Copy} (or by right-clicking and selecting \textsf{Copy}).
Next type the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x <- scan()!\inputencoding{utf8}
command in the \textsf{R} console, and paste the numbers at the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1:!\inputencoding{utf8}
prompt with \textsf{Edit}\emph{ }\textsf{$\triangleright$}\emph{
}\textsf{Paste}. All of the numbers will automatically be entered
into the vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.\end{enumerate}
\begin{itemize}
\item The command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Ctrl+l!\inputencoding{utf8}
clears the screen in the Microsoft$\circledR$ Windows \textsf{R}Gui.
The comparable command for Emacs/ESS is 
\item Once you use \textsf{R} for awhile there may be some commands that
you wish to run automatically whenever \textsf{R} starts. These commands
may be saved in a file called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rprofile.site!\inputencoding{utf8}
which is usually in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!etc!\inputencoding{utf8}
folder, which lives in the \textsf{R} home directory (which on Microsoft$\circledR$
Windows usually is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!C:\Program Files\R!\inputencoding{utf8}).
Alternatively, you can make a file \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.Rprofile!\inputencoding{utf8}
to be stored in the user's home directory, or anywhere \textsf{R}
is invoked. This allows for multiple configurations for different
projects or users. See {}``Customizing the Environment'' of \emph{An
Introduction to }\textsf{\emph{R}} for more details.
\item When exiting \textsf{R} the user is given the option to {}``save
the workspace''. I recommend that beginners DO NOT save the workspace
when quitting. If \textsf{Yes} is selected, then all of the objects
and data currently in \textsf{R}'s memory is saved in a file located
in the working directory called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.RData!\inputencoding{utf8}.
This file is then automatically loaded the next time \textsf{R} starts
(in which case \textsf{R} will say \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]![previously saved workspace restored]!\inputencoding{utf8}).
This is a valuable feature for experienced users of \textsf{R}, but
I find that it causes more trouble than it saves with beginners.
\end{itemize}

\section{Chapter Exercises}

\setcounter{thm}{0}

\textbf{Directions:} Complete the following exercises and submit your
answers. \emph{Please Note}: only answers are required; it is not
necessary to submit the \textsf{R} output on the screen. 
\begin{xca}
Write out line \Sexpr{sample(3:12, size = 1)} of the source code
for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plot!\inputencoding{utf8}
function.
\end{xca}

\paragraph*{Solution:}

Type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plot!\inputencoding{utf8}
at the command line (with no parentheses).

<<>>=
plot
@



<<echo = FALSE, results = hide>>=
x <- rnbinom(6, size = 4, prob = 0.25)
k <- sample(1:9, size = 3, replace = FALSE)
@
\begin{xca}
Let our small data set of size \Sexpr{length(x)} be 

<<fifteen, echo = FALSE>>=
x
@

\noindent Enter this data into a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
\begin{enumerate}
\item Raise all of the numbers in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
to the power \Sexpr{k[1]}. 
\item Subtract \Sexpr{k[2]} from each number in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
\item Add \Sexpr{k[3]} to all of the numbers in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
then take the (natural) logarithm of the answers.
\end{enumerate}
Use vectorization of functions to do all of the above, using a single
line of code for each.

\end{xca}

\paragraph*{Answers:}

<<echo = FALSE>>=
x^k[1]
x - k[2]
log(x + k[3])
@



<<echo = FALSE, results = hide>>=
x <- round(rnorm(13, mean = 20, sd = 2), 1)
@
\begin{xca}
The asking price of used MINI Coopers varies from seller to seller.
An online listing has these values in thousands: 

<<echo = FALSE>>=
x
@
\begin{enumerate}
\item What is the smallest amount? The largest?
\item Find the average amount with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8}.
\item Calculate the difference of the mean value from the largest and smallest
amounts (the first number will be positive, the second will be negative).
\end{enumerate}
\end{xca}

\paragraph*{Answers:}

<<echo = FALSE>>=
c(min(x), max(x))
mean(x)
c(max(x), min(x)) - mean(x)
@



<<twenty, echo = FALSE, results = hide>>=
x <- round(rnorm(12, mean = 3, sd = 0.3), 3) * 1000
names(x) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
@
\begin{xca}
The twelve monthly sales of Ramen noodles in the United States during
2009 were 

<<echo = FALSE>>=
x
@

Note that the first entry above was the sales from January, the second
entry was from February, and so forth.
\begin{enumerate}
\item Enter these data into a variable \texttt{H2}. Use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cumsum!\inputencoding{utf8}
to find the cumulative total sales for 2009. What was the total number
sold? 
\item Using \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!diff!\inputencoding{utf8},
find the month with the greatest increase from the previous month,
and the month with the greatest decrease from the previous month.
\emph{Hint:} Dont know how to use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!diff!\inputencoding{utf8}?
No problem! Check it out using the \textsf{Help} system.
\end{enumerate}
\end{xca}
\small


\paragraph*{Solution:}

First enter the data into a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
You can make it fancy with the months of the year with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!names!\inputencoding{utf8}
function. 

<<>>=
names(x) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
x
@

Now let's check out \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cumsum!\inputencoding{utf8}
:

<<>>=
cumsum(x)
@

This shows that the total amount sold was \Sexpr{max(cumsum(x))}.
We next check out what \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!diff!\inputencoding{utf8}
does:

<<>>=
diff(x)
@

We see that the first entry of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!diff(x)!\inputencoding{utf8}
is the difference in sales, February minus January. The second entry
is March minus February, and so forth. The greatest increase from
the previous month was \Sexpr{max(diff(x))}, which happened in {}``\Sexpr{names(x)[which(diff(x)==max(diff(x)))+1]}''.
The greatest decrease from the previous month was \Sexpr{min(diff(x))},
which happened in {}``\Sexpr{names(x)[which(diff(x)==min(diff(x)))+1]}''.
(These can be found by inspection of the output or even quicker with
a command like \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!max(diff(x))!\inputencoding{utf8}).

\normalsize



<<twentyfive, echo = FALSE, results = hide>>=
commute = sample(150:250, size = 10, replace = TRUE)/10
k = sample(1:10, size = 1)
new = sample(150:250, size = 1, replace = TRUE)/10

@
\begin{xca}
You track your commute times for 10 days, recording the following
times (in minutes):

<<echo = FALSE>>=
commute
@
\begin{enumerate}
\item \noindent Enter these data into \textsf{R}. Use the function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!max!\inputencoding{utf8}
to find the longest travel time, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!min!\inputencoding{utf8}
to find the smallest, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8}
to find the average time, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sd!\inputencoding{utf8}
to find the sample standard deviation of the times.
\item Oops! The \Sexpr{commute[k]} was a mistake. It should have been \Sexpr{new},
instead. How can you fix this (without retyping the whole vector)?
Correct the mistake and report the new \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!max!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!min!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8},
and sample standard deviation.
\end{enumerate}
\end{xca}

\paragraph*{Answers:}

<<echo = FALSE>>=
c(max(commute), min(commute), mean(commute), sd(commute))
commute[k] <- new
c(max(commute), min(commute), mean(commute), sd(commute))
@


\chapter{Describing Data Distributions \label{cha:Describing-Data-Distributions}}

What would I like them to know?
\begin{itemize}
\item what is data

\begin{itemize}
\item the different types, especially quantitative versus qualitative, and
discrete versus continuous
\end{itemize}
\item how to describe data both visually and numerically, and how the methods
differ depending on the data type
\item CUSS
\item how to do all of the above but in the context of describing data broken
down by groups
\item the concept of factor and what it means for subdividing data
\end{itemize}
In this chapter we introduce the different types of data that a statistician
is likely to encounter. In each subsection we describe how to display
the data of that particular type.
\begin{itemize}
\item First we classify data into one of many types that the statistician
is likely to encounter.
\item Next, we discuss how to go display the data of the respective types
in graphical or tabular format.
\item Once data are displayed, we talk about properties of data sets that
can be observed from the displays. This is done in an entirely qualitative
fashion.
\item Next, we talk about ways of quantifying the properties discussed previously.
We introduce common measures used to quantify the considerations.
\item This is followed by EDA in which we examine in more detail some visual
and tabular devices; outliers are discussed here.
\item Next we move to introducing dependence with multivariate data, and
the technical \textsf{R} concept of data frames.
\item We end with graphical/numerical ways to compare data sets or subpopulations
using the devices studied previously.
\end{itemize}
Once we see how to display data distributions, we next introduce the
basic properties of data distributions. We qualitatively explore several
data sets. Once that we have intuitive properties of data sets, we
next discuss how we may numerically measure and describe those properties
with descriptive statistics.


\section{Types of Data\label{sec:Types-of-Data}}

Loosely speaking, a datum is any piece of collected information, and
a data set is a collection of data related to each other in some way.


\subsection{Quantitative Data\label{sub:Quantitative-Data}}

Quantitative data are any data that take numerical values. Quantitative
data can be further subdivided into two categories. 
\begin{itemize}
\item Discrete data take values in a finite or countably infinite set of
numbers. Examples include: counts, number of arrivals, number of successes,
attendance.
\item Continuous data take values in an interval of numbers. These are also
known as scale data, interval data, or measurement data. Examples
include: height, weight, length, time, \emph{etc}.
\end{itemize}

\subsection*{Displaying Quantitative Data\label{sub:Displaying-Quantitative-Data}}

There are many graphs available for displaying quantitative data. 


\paragraph*{Strip charts (also known as Dot plots)}

These are done with a call to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stripchart!\inputencoding{utf8}
function, and are often used when the data set is not too large. Along
the horizontal axis is a numerical scale above which the data values
are plotted. There are three methods for strip charts.
\begin{itemize}
\item Overplot: plots ties covering each other. This method is good to display
only the distinct values assumed by the dataset.
\item Jitter: adds some noise to the data in the $y$ direction so that
data values are not covered up by ties.
\item Stack: plots repeated values stacked on top of one another. This method
is best used for discrete data with a lot of ties; if there are no
repeats then this method is identical to overplot.
\end{itemize}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
par(mfrow = c(1,3)) # 3 plots: 1 row, 3 columns
stripchart(uspop, xlab="length", ylim=c(0, 2))
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number of discoveries")
par(mfrow = c(1,1)) # back to normal
@
\par\end{centering}

\caption{Strip charts of the parking variable\label{fig:Stripcharts-of-parking}}

\end{figure}



\paragraph*{Histogram}

Used for continuous data. There are many ways to plot histograms,
one of the easiest is done with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!hist!\inputencoding{utf8}
function. These plots are some of the most common summary displays,
and they are often misidentified as {}``Bar Graphs'' (see below.)
The scale on the $y$ axis can be frequency, percentage, or density
(relative frequency).

A histogram is constructed by first deciding on a set of classes,
or bins. The bins partition the real line into a set of classes into
which the data values fall. 
\begin{quotation}
HISTOGRAM. The term histogram was coined by Karl Pearson. In his Contributions
to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous
Material, Philosophical Transactions of the Royal Society A, 186,
(1895) Pearson explained in a footnote (p. 399) that the term was
introduced by the writer in his lectures on statistics as a term
for a common form of graphical representation, i.e., by columns marking
as areas the frequency corresponding to the range of their base.

The term histogram appears in a lecture of November 1891 in the series
of lectures on the Geometry of Statistics that Pearson gave at Gresham
College in the academic year 1891-2. The lectures are described by
S. M. Stigler History of Statistics pp. 326-7 and T. M. Porter Karl
Pearson: The Scientific Life in a Statistical Age, p. 236. 

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
par(mfrow = c(1,2)) # 2 plots: 1 row, 2 columns
hist(volcano, freq = TRUE)
hist(volcano, freq = FALSE)
par(mfrow = c(1,1)) # back to normal
@
\par\end{centering}

\caption{Histograms of the volcano data.\label{fig:hist-volcano}}

\end{figure}

\end{quotation}

\paragraph*{Stemplots (more to be said in Section \ref{sec:Exploratory-Data-Analysis})}

Stemplots have two basic parts: \emph{stems} and \emph{leaves}. The
final digit of the data values is taken to be a \emph{leaf}, and the
leading digit(s) is (are) taken to be \emph{stems}. A vertical line
is drawn, and to the left of the line are listed the stems. To the
right of the line, the leaves are listed beside their corresponding
stem. There will typically be several leaves for each stem, in which
case the leaves accumulate to the right. It is sometimes necessary
to round the data values, especially for larger data sets.

Consider the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!UKDriverDeaths!\inputencoding{utf8}
data. We construct a stem and leaf diagram in \textsf{R} with the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf!\inputencoding{utf8}
function from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!aplpack!\inputencoding{utf8}
package.

<<>>=
library(aplpack)
stem.leaf(UKDriverDeaths, depth = FALSE)
@

Notice that in the arguments we are not showing {}``depths''. To
learn more about this option and many others, see Section \ref{sec:Exploratory-Data-Analysis}.
An advantage of using the stemplot is that the original data values
are not lost in the display, in contrast to a histogram.


\paragraph*{Index Plot}

Done with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plot!\inputencoding{utf8}
function. These are good for plotting data which are ordered in the
dataset, for example, when the data are measured over time. That is,
the first observation was measured at time 1, the second at time 2,
\emph{etc}. It is a two dimensional plot, in which the index is the
$x$ variable and the observation is the $y$ variable. There are
two plotting methods for index plots:
\begin{itemize}
\item Spikes: draws a vertical line from the $x$-axis to the observation
height.
\item Points: plots a simple point at the observation height.
\end{itemize}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 8, width = 6>>=
par(mfrow = c(2,1)) # 2 plots: 1 row, 2 columns
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
par(mfrow = c(1,1)) # back to normal
@
\par\end{centering}

\caption{Index plots of salary\label{fig:indpl-lakehuron}}

\end{figure}



\subsection{Qualitative Data, Categorical Data, and Factors\label{sub:Qualitative-Data}}

Qualitative data are simply any type of data that are not numerical,
or do not represent numerical quantities. Examples of qualitative
variables include a subject's name, gender, race/ethnicity, political
party, socio-economic status, driver's license number, and social
security number (SSN).

Some qualitative data serve merely to \emph{identify} the observation
(such a subject's name, driver's license number, or SSN), but other
qualitiative data serve to \emph{subdivide} a data set into categories.
The identification types do not play much of a role in statistics,
but the latter subdivision types show up all the time, and we call
them \emph{factors}. In the above examples, gender, race, political
party, and socio-economic status would be considered factors. 

The possible values of a factor are called its \emph{levels}. For
instance, the factor \emph{gender} would have two levels, namely,
male and female.

Factors have special status in \textsf{R}. They are represented internally
by numbers, but even when they are written numerically their values
do not convey any numeric meaning or obey any mathematical rules (that
is, Stage III cancer is not Stage I cancer + Stage II cancer).


\subsection*{Displaying Qualitative Data}


\paragraph*{Tables}

Here we choose a variable and count frequencies and list proportions.
We can do this at the console with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!table!\inputencoding{utf8}
function. In \textsf{R} Commander you can do it with \textsf{Statistics}
\textsf{$\triangleright$} \textsf{Frequency Distribution\ldots{}.}
Alternatively, to look at tables for all factors in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Active data set!\inputencoding{utf8}
you can do \textsf{Statistics} \textsf{$\triangleright$} \textsf{Summaries}
\textsf{$\triangleright$} \textsf{Active Dataset}.

<<keep.source = TRUE>>=
Tbl <- table(state.division)
Tbl            # frequencies
Tbl/sum(Tbl)   # relative frequencies
@


\paragraph*{Bar Graphs}

A bar graph is the categorical analogue of a histogram which is used
for categorical data. A bar is displayed for each level of a factor,
with the height of the bars proportional to the frequencies of observations
falling in the respective categories. A disadvantage of bar graphs
is that the levels are ordered alphabetically (by default), which
may sometimes obscure patterns in the display. For an example, see
Figure \ref{fig:bar-gr-stateregion}.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 7>>=
par(mfrow = c(1,2)) # 2 plots: 1 row, 2 columns
barplot(table(state.region), cex.names=0.60)
barplot(prop.table(table(state.region)), cex.names=0.60)
par(mfrow = c(1,1)) # back to normal
@
\par\end{centering}

\caption{Bar Graphs of the factor state.region \label{fig:bar-gr-stateregion}}

\end{figure}



\paragraph*{Pareto Diagrams }

These can be done with \textsf{R} Commander, or with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!pareto.chart!\inputencoding{utf8}
function at the console (from the package \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!qcc!\inputencoding{utf8}).

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 5, width = 6>>=
library(qcc)
pareto.chart(table(state.division), ylab="Frequency")
@
\par\end{centering}

\caption{Pareto chart of the state.division\label{fig:Pareto-chart}}

\end{figure}



\paragraph*{Pie Graphs }

These can be done with \textsf{R} Commander, but these have lost popularity
in recent years. The reason is that the human eye cannot judge angles
very well. Use it to display 2 to 6 fractions of one unit. Can only
show marked differences in values. Pie charts are a very bad way of
displaying information. The eye is good at judging linear measures
and bad at judging relative areas. A bar chart or dot chart is a preferable
way of displaying this type of data. 

Cleveland (1985), page 264: {}``Data that can be shown by pie charts
always can be shown by a dot chart. This means that judgements of
position along a common scale can be made instead of the less accurate
angle judgements.''This statement is based on the empirical investigations
of Cleveland and McGill as well as investigations by perceptual psychologists. 

Prior to \textsf{R} 1.5.0 this was known as piechart, which is the
name of a Trellis function, so the name was changed to be compatible
with \textsf{S}. 


\paragraph*{Mosaic Plots}


\subsection{Logical Data\label{sub:Logical-Data}}

There is another type of information recognized by \textsf{R} which
does not fall into the above categories. The value is either \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!TRUE!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!FALSE!\inputencoding{utf8}
(note that equivalently you can use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1 = TRUE!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!0 = FALSE!\inputencoding{utf8}).
Here is an example of a logical vector:

<<>>=
x <- c(5,7)
v <- (x<6)
v
@

Many functions in \textsf{R} have options that the user may or may
not want to activate in the function call. For example, the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf!\inputencoding{utf8}
function has the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!depths!\inputencoding{utf8}
argument which is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!TRUE!\inputencoding{utf8}
by default. We saw in Section BLANK how to turn the option off, simply
enter \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf(x, depths = FALSE)!\inputencoding{utf8}
and depths will not be shown on the display. 


\subsection{Missing Data\label{sub:Missing-Data}}

Missing data are a persistent and prevalent problem in many statistical
analyses, especially those associated with the social sciences. \textsf{R}
reserves the special symbol \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}
to representing missing data. You can test which entries in a data
vector are missing with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!is.na!\inputencoding{utf8}
function. Certain functions are equipped with a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!na.rm!\inputencoding{utf8}
argument, which when \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!TRUE!\inputencoding{utf8}
will ignore missing data in the function arguments and will return
the function value. Other functions are not equipped and will return
an error if \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!NA!\inputencoding{utf8}s
are present.


\section{Features of Data Distributions\label{sec:Features-of-Data}}

Given that the data have been appropriately displayed, the next step
is to try to identify salient features represented in the graph. The
acronym to remember is \emph{C}enter, \emph{U}nusual features, \emph{S}pread,
and \emph{S}hape. (CUSS).


\subsection{Center\label{sub:Center}}

One of the most basic features of a dataset is its center. Loosely
speaking, the center of a dataset is associated with a number that
represents a middle or general tendency to the data. Of course, there
are usually several values that would serve as a center, and our later
tasks will be focused on choosing an appropriate one for the data
at hand. Judging from the histogram that we saw before, a measure
of center would be about BLANK. 


\subsection{Spread\label{sub:Spread}}

The spread of a dataset is associated with its variability; datasets
with a large spread tend to cover a large interval of values, while
datasets with small spread tend to cluster tightly around a central
value. 


\subsection{Shape\label{sub:Shape}}

The shape of


\paragraph*{Symmetry and Skewness}

When we discuss the shape of a dataset, we are usually referring to
the shape exhibited by an associated graphical display, such as a
histogram.

skewness, symmetry A distribution is said to be right-skewed (or positively
skewed) if the right tail seems to be stretched from the center. A
left skewed (or negatively skewed) distribution is stretched to the
left side. A symmetric distribution has a graph that may be reflected
about a central line of symmetry.

Examples of skewed distributions:

There are other shapes including uniform, J-shaped, etc.


\paragraph*{Kurtosis}

Introduced by Pearson in 1905 \url{http://jeff560.tripod.com/k.html}Another
component to the shape of a distribution is how {}``peaked'' it
is. Some distributions tend to have a flat shape with thin tails.
These are called \emph{platykurtic}, and an example of a platykurtic
distribution is the uniform distribution; see Section BLANK. On the
other end of the spectrum are distributions with a steep peak, or
spike, which is accompanied by heavy tails; these are called \emph{leptokurtic}.
Examples of leptokurtic distributions are the Laplace distribution
and the logistic distribution. See Section BLANK. In between are distributions
(called \emph{mesokurtic}) with a rounded peak and moderately sized
tails. The standard example of a mesokurtic distribution is the famous
bell-shaped curve, also known as the Gaussian, or normal, distribution,
and binomial distribution can be mesokurtic for specific choices of
$p$. See Sections BLANK, BLANK, and BLANK.


\subsection{Clusters and Gaps\label{sub:Clusters-and-Gaps}}

Clusters or gaps are sometimes observed in quantitative data distributions.
They indicate clumping of the data about distinct values, and gaps
may exist between clusters. Clusters often suggest an underlying grouping
to the data. For example, perhaps we are studying how response time
on a driving test is affected by alcohol consumption. Suppose there
are two groups: one that received an alcoholic beverage before taking
a computerized driving test, and another group that received a non-alcoholic
beverage before taking the test. If response times are measured, we
would conceivably observe two clumps, or groups of similar response
times, with the alcoholic group showing a longer response time.


\subsection{Extreme Observations and other Unusual Features\label{sub:Extreme-Observations-and}}

Extreme observations fall far from the rest of the data. Such observations
are troublesome to many statistical procedures, causing exaggerated
estimates and instability of the methods. It is important to identify
extreme observations and examine the source of the data more closely.
There are many possible reasons underlying an extreme observation:
\begin{itemize}
\item \textbf{Maybe the value is a typographical error.} Especially with
large data sets becoming more prevalent, many of which being recorded
by hand, mistakes are a common problem. After closer scrutiny, these
can often be fixed.
\item \textbf{Maybe the observation was not meant for the study}, because
it does not belong to the population of interest. For example, in
medical reasearch some subjects may have relevant complications in
their genealogical history that would rule out their participation
in the experiment. Or when a manufacturing company investigates the
properties of one of its devices, perhaps a particular product is
malfunctioning and is not representative of the majority of the items.
\item \textbf{Maybe it indicates a deeper trend or phenomenon}. Many of
the most influential scientific discoveries were made when the investigator
noticed an unexpected result, a value that was not predicted by the
classical theory. Albert Einstein, Louis Pasteur, and others built
their careers on exactly this circumstance.
\end{itemize}

\section{Descriptive Statistics\label{sec:Descriptive-Statistics}}


\subsection{Frequencies and Relative Frequencies\label{sub:Frequencies-and-Relative}}

These are used for categorical data. The idea is that there are a
number of different categories, and we would like to get some idea
about how the categories are represented in the population. For example,
we may want to see how the 


\subsection{Measures of Center\label{sub:Measures-of-Center}}

There

The \textbf{\emph{sample mean}} is denoted $\xbar$ (read {}``$x$-bar'')
and is simply the arithmetic average of the observations:\begin{equation}
\xbar=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.\end{equation}

\begin{itemize}
\item Good: natural, easy to compute, has nice mathematical properties
\item Bad: sensitive to extreme values
\end{itemize}
It is appropriate for use with datasets that are not highly skewed
without extreme observations.

The \textbf{\emph{sample median}} is another popular measure of center
and is denoted $\tilde{x}$. To calculate its value, first sort the
data into an increasing sequence of numbers. If the data set has an
odd number of observations then $\tilde{x}$ is the value of the middle
observation, which lies in position $(n+1)/2$; otherwise, there are
two middle observations and $\tilde{x}$ is the average of those middle
values.
\begin{itemize}
\item Good: resistant to extreme values, easy to describe
\item Bad: not as mathematically tractable, need to sort the data to calculate
\end{itemize}
One desirable property of the sample median is that it is resistant
to extreme observations, in the sense that the value of $\tilde{x}$
depends only the values of the middle observations, and is quite unaffected
by the actual values of the outer observations in the ordered list.
The same cannot be said for the sample mean. Any significant changes
in the magnitude of an observation $x_{k}$ results in a corresponding
change in the value of the mean. Hence, the sample mean is said to
be sensitive to extreme observations.

The \textbf{\emph{trimmed mean}} is a measure designed to address
the sensitivity of the sample mean to extreme observations. The idea
is to {}``trim'' a fraction (less than 1/2) of the observations
off each end of the ordered list, and then calculate the sample mean
of what remains. We will denote it by $\xbar_{t=0.05}$.
\begin{itemize}
\item Good: resistant to extreme values, shares nice statistical properties
\item Bad: need to sort the data
\end{itemize}

\subsection{How to do it with \textsf{R}}
\begin{itemize}
\item You can calculate the frequencies or relative frequencies with the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!table!\inputencoding{utf8}
function.
\item You can calculate the sample mean of a data vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean(x)!\inputencoding{utf8}. 
\item You can calculate the sample median of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!median(x)!\inputencoding{utf8}.
\item You can calculate the trimmed mean with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trim!\inputencoding{utf8}
argument; \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean(x, trim = 0.05)!\inputencoding{utf8}.
\end{itemize}

\subsection{Order Statistics and the Sample Quantiles\label{sub:Order-Statistics-and}}

A common first step in an analysis of a data set is to sort the values.
Given a data set $x_{1}$, $x_{2}$, \ldots{},$x_{n}$, we may sort
the values to obtain an increasing sequence\begin{equation}
x_{(1)}\leq x_{(2)}\leq x_{(3)}\leq\cdots\leq x_{(n)}\end{equation}
and the resulting values are called the \emph{order statistics}. The
$k$$^{\text{th}}$ entry in the list, $x_{(k)}$, is the $k$$^{\text{th}}$
order statistic, and approximately $100(k/n)$\% of the observations
fall below $x_{(k)}$. The order statistics give an indication of
the shape of the data distribution, in the sense that a person can
look at the order statistics and have an idea about where the data
are concentrated, and where they are sparse.

The \emph{sample quantiles} are related to the order statistics. Unfortunately,
there is not a universally accepted definition of them. Indeed, \textsf{R}
is equipped to calculate quantiles using nine distinct definitions!
We will describe the default method (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!type = 7!\inputencoding{utf8}),
but the interested reader can see the details for the other methods
with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!?quantile!\inputencoding{utf8}.

Suppose the dataset has $n$ observations. Find the sample quantile
of order $p$ ($0<p<1$), denoted $\tilde{q}_{p}$ , in the following
way:
\begin{enumerate}
\item Sort the data to obtain the order statistics $x_{(1)}$, $x_{(2)}$,
\ldots{},$x_{(n)}$. 
\item Calculate $(n-1)p+1$ and write it in the form $k.d$, where $k$
is an integer and $d$ is a decimal.
\item The sample quantile $\tilde{q}_{p}$ is then\begin{equation}
\tilde{q}_{p}=x_{(k)}+d(x_{(k+1)}-x_{(k)}).\end{equation}

\end{enumerate}
The interpretation of $\tilde{q}_{p}$ is that approximately $100p$\%
of the data fall below the value $\tilde{q}_{p}$ . 

Keep in mind that there is not a unique definition of percentiles,
quartiles, \emph{etc}. Open a different book, and you'll find a different
procedure. The difference is small and seldom plays a role except
in small datasets with repeated values. In fact, most people do not
even notice in common use.

Clearly, the most popular sample quantile is $\tilde{q}_{0.50}$,
also known as the sample median, $\tilde{x}$. The closest runners-up
are the \emph{first quartile} $\tilde{q}_{0.25}$ and the \emph{third
quartile} $\tilde{q}_{0.75}$ (the \emph{second quartile} is the median). 


\subsection{How to do it with \textsf{R}}


\paragraph*{At the command prompt}

We can find the order statistics of a data set stored in a vector
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sort(x)!\inputencoding{utf8}. 

You can calculate the sample quantiles of any order $p$ where $0<p<1$
for a dataset stored in a data vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!quantile!\inputencoding{utf8}
function, for instance, the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!quantile(x, probs = c(0, 0.25, 0.37))!\inputencoding{utf8}
will return the smallest observation, the first quartile, $\tilde{q}_{0.25}$,
and the 37th sample quantile, $\tilde{q}_{0.37}$. For $\tilde{q}_{p}$
simply change the values in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
argument to the value $p$.


\paragraph*{With the \textsf{R} Commander}

In \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
we can find the order statistics of a variable in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Active data set!\inputencoding{utf8}
by doing \textsf{Data $\triangleright$ Manage variables in Active
data set\ldots{} $\triangleright$ Compute new variable}\textsf{\emph{\ldots{}}}.
In the \textsf{Expression to compute} dialog simply type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sort(varname)!\inputencoding{utf8},
where \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!varname!\inputencoding{utf8}
is the variable that it is desired to sort.

In \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8},
we can calculate the sample quantiles for a particular variable with
the sequence \textsf{Statistics $\triangleright$ Summaries $\triangleright$
Numerical Summaries\ldots{}}. We can automatically calculate the
quartiles for all variables in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Active data set!\inputencoding{utf8}
with the sequence \textsf{Statistics $\triangleright$ Summaries $\triangleright$
Active Dataset}.


\subsection{Measures of Spread\label{sub:Measures-of-Spread}}


\paragraph*{Sample Variance and Standard Deviation}

The \emph{sample variance} is denoted $s^{2}$ and is calculated with
the formula\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\xbar)^{2}.\end{equation}
The \emph{sample standard deviation} is $s=\sqrt{s^{2}}$. Intuitively,
the sample variance is approximately the average squared distance
of the observations from the sample mean. The sample standard deviation
is used to scale the estimate back to the measurement units of the
original data. 
\begin{itemize}
\item Good: tractable, has nice mathematical/statistical properties
\item Bad: sensitive to extreme values
\end{itemize}

\paragraph*{Interquartile Range}

Just as the sample mean is sensitive to extreme values, so the associated
measure of spread is similarly sensitive to extremes. Further, the
problem is exascerbated by the fact that the extreme distances are
squared. We know that the sample quartiles are resistant to extremes,
and a measure of spread associated with them is the \emph{interquartile
range} ($IQR$) defined by $IQR=q_{0.75}-q_{0.25}$. 
\begin{itemize}
\item Good: stable, resistant to outliers, robust to nonnormality, easy
to explain
\item Bad: not as tractable, need to sort the data, only involves the middle
50\% of the data.
\end{itemize}

\paragraph*{Median Absolute Deviation }

The $IQR$ is useful as a first attempt at robustness to extreme observations,
however, a much more robust choice is given by the \emph{median absolute
deviation} ($MAD$). The absolute deviations are the nonnegative numbers
$|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|$,
and the $MAD$ is proportional to the median of these values:\begin{equation}
MAD\propto\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)\end{equation}
That is, the $MAD=c\cdot\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)$,
where $c$ is a constant chosen so that the $MAD$ has nice properties.
The value of $c$ in \textsf{R} is by default $c=1.4286$. This value
is chosen to ensure that the estimator of $\sigma$ is correct, on
the average, under suitable sampling assumptions.
\begin{itemize}
\item Good: stable, excellently robust, even more so than the $IQR$
\item Bad: not tractable, not well known or easy to explain, need to sort
the data twice.
\end{itemize}

\subsubsection*{Comparing Apples to Apples}

We have seen three different measures of spread which, for a given
data set, will give three different answers. Which one should we use?
It depends on the data set. If the data are well behaved, with an
approximate bell-shaped distribution, then the sample mean and sample
standard deviation are natural choices with nice mathematical properties.
However, if the data have an unusual or skewed shape with several
extreme values, perhaps the more resistant choices among the $IQR$
or $MAD$ would be more appropriate.

However, once we are looking at the three numbers it is important
to understand that the estimators are not all measuring the same quantity,
on the average. In particular, it can be shown that when the data
follow an approximately bell-shaped distribution, then on the average,
the sample standard deviation $s$ and the $MAD$ will be the approximately
the same value, namely, $\sigma.$ However, it can also be shown that
in repeating a certain experiment many times, the $IQR$ will be on
the average 1.349 times larger than $s$ and the $MAD$. See Chapter
BLANK for more details.


\subsection{How to do it with \textsf{R}}


\paragraph*{At the Command Prompt}

From the console we may compute the sample range with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!range(x)!\inputencoding{utf8}
and the sample variance with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!var(x)!\inputencoding{utf8}.
The sample standard deviation may be found with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sqrt(var(x))!\inputencoding{utf8}
or simply \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sd(x)!\inputencoding{utf8}.
The console syntax for the $IQR$ is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IQR(x)!\inputencoding{utf8},
where \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
is a numeric vector. The console command for the median absolute deviation
is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mad(x)!\inputencoding{utf8}.


\paragraph*{In \textsf{R} Commander}

In \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
we can calculate the sample standard deviation with the \textsf{Statistics
$\triangleright$ Summaries $\triangleright$ Numerical Summaries}\ldots{}
combination. \textsf{R} Commander does not calculate the $IQR$ or
$MAD$ in any of the menu selections, by default.

Chebychev's Rule: The proportion of observations within $k$ standard
deviations of the mean, where , is at least , i.e., at least 75\%,
89\%, and 94\% of the data are within 2, 3, and 4 standard deviations
of the mean, respectively.

Empirical Rule: If data follow a bell-shaped curve, then approximately
68\%, 95\%, and 99.7\% of the data are within 1, 2, and 3 standard
deviations of the mean, respectively. 


\subsection{Measures of Shape\label{sub:Measures-of-Shape}}


\paragraph*{Sample Skewness}

The sample skewness, denoted by $g_{1}$, is defined by the formula\begin{equation}
g_{1}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\xbar)^{3}}{s^{3}}.\end{equation}


The sample skewness can be any value $-\infty<g_{1}<\infty$. The
sign of $g_{1}$ indicates the direction of skewness of the distribution.
Samples that have $g_{1}>0$ indicate right-skewed distributions (or
positively skewed), and samples with $g_{1}<0$ indicate left-skewed
distributions (or negatively skewed). Values of $g_{1}$ near zero
indicate a symmetric distribution. These are not hard and fast rules,
however. The value of $g_{1}$ is subject to sampling variability
and thus only provides a suggestion to the skewness of the underlying
distribution. 

Need to talk about {}``how big is big?'' Rule of thumb is to should
be no bigger than $2\cdot\sqrt{6/n}$. Reference to Tabachnick \&
Fidell.


\paragraph*{Sample Excess Kurtosis}

The sample excess kurtosis, denoted by $g_{2}$, is given by the formula\begin{equation}
g_{2}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\xbar)^{4}}{s^{4}}-3.\end{equation}


The first term in the formula is always nonnegative, implying that
the sample excess kurtosis takes values $-3\leq g_{2}<\infty$. The
subtraction of 3 may seem mysterious to the reader, but it is done
so that mound shaped samples have values of $g_{2}$ near $0$. Samples
with $g_{2}>0$ are called leptokurtic, and samples with $g_{2}<0$
are called platykurtic. Samples with $g_{2}\approx0$ are called mesokurtic.
$ $

Notice that both the sample skewness and the sample kurtosis are independent
of location and scale, in other words, the values of $g_{1}$ and
$g_{2}$ do not depend on the measurement units.

Need to talk about {}``how big is big?'' Rule of thumb is to divide
by $2\cdot\sqrt{24/n}$. Reference to Tabachnick \& Fidell.


\subsection{How to do it with \textsf{R}}


\paragraph*{At the Command Prompt}

First, we must load the \inputencoding{latin9}\lstinline[showstringspaces=false]!e1071!\inputencoding{utf8}
package (after installing it) with the command \inputencoding{latin9}\lstinline[showstringspaces=false]!library(e1071)!\inputencoding{utf8}.
Next, we may compute the sample skewness with \inputencoding{latin9}\lstinline[showstringspaces=false]!skewness(x)!\inputencoding{utf8}
and the sample excess kurtosis with \inputencoding{latin9}\lstinline[showstringspaces=false]!kurtosis(x)!\inputencoding{utf8}.
Both functions have a \inputencoding{latin9}\lstinline[showstringspaces=false]!na.rm!\inputencoding{utf8}
argument which is \inputencoding{latin9}\lstinline[showstringspaces=false]!TRUE!\inputencoding{utf8}
by default.


\section{Exploratory Data Analysis\label{sec:Exploratory-Data-Analysis}}

This field was founded (mostly) by John Tukey (1915-2000). Its tools
are useful when not much is known regarding the underlying causes
associated with the dataset, and are often used for checking assumptions.
For example, suppose we perform an experiment and collect some data\ldots{}
now what? We look at the data using exploratory visual tools.


\subsection{More About Stemplots}

There are many bells and whistles associated with stemplots, and the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf!\inputencoding{utf8}
function can do many of them.
\begin{enumerate}
\item Trim Outliers.
\item Splitting Stems. 
\item Depths: these are used to give insight into the balance of the observations
as they accumulate toward the median. In a column beside the standard
stemplot, the frequency of the stem containing the sample median is
shown in parentheses. Next, frequencies are accumulated from the outside
inward and including the outliers.
\end{enumerate}
<<>>=
x <- c(109, 84, 73, 42, 61, 51,54, 71, 47, 70, 65, 57,69, 82, 76, 60, 38, 81,76, 85, 58, 73, 65, 42)
stem.leaf(x)
@


\paragraph*{Variations:}

More than one part per stem and trim outliers.


\subsection{How to do it with \textsf{R}}


\paragraph*{At the Command Prompt}

The basic command is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem(x)!\inputencoding{utf8}
or a more sophisticated version written by Peter Wolf called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf(x)!\inputencoding{utf8}
in the \textsf{R} Commander. We will describe \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf!\inputencoding{utf8}
since that is the one used by \textsf{R} Commander.


\paragraph*{With the \textsf{R} Commander}

WARNING: Sometimes when making a stem plot the result will not be
what you expected. There are several reasons for this:
\begin{itemize}
\item Stemplots by default will trim extreme observations (defined in Section
\ref{sub:Outliers}) from the display. This in some cases will result
in stemplots that are not as wide as expected.
\item The leafs digit is chosen automatically by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf!\inputencoding{utf8}
according to an algorithm that the computer believes will represent
the data well. Depending on the choice of the digit, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stem.leaf!\inputencoding{utf8}
may drop digits from the data or round the values in unexpected ways.
\end{itemize}
<<>>=
stem.leaf(rivers)
@

<<>>=
stem.leaf(precip)
@


\subsection{Hinges and the Five Number Summary\label{sub:Hinges-and-the} }

Given a dataset $x_{1}$, $x_{2}$, \ldots{}, $x_{n}$, the hinges
are found by the following method: 
\begin{itemize}
\item Find the order statistics $x_{(1)}$, $x_{(2)}$, \ldots{}, $x_{(n)}$. 
\item The \emph{lower hinge} $h_{L}$ is in position $L=\left\lfloor (n+3)/2\right\rfloor /2$,
where the symbol $\left\lfloor x\right\rfloor $ denotes the largest
integer less than or equal to $x$. If the position $L$ is not an
integer, then the hinge $h_{L}$ is the average of the adjacent order
statistics.
\item The \emph{upper hinge} $h_{U}$ is in position $n+1-L$.
\end{itemize}
Given the hinges, the \emph{five number summary} ($5NS$) is defined
to be\begin{equation}
5NS=(x_{(1)},\ h_{L},\ \tilde{x},\ h_{U},\ x_{(n)})\end{equation}
An advantage of the $5NS$ is that it reduces a potentially large
dataset to a shorter list of only five numbers, and further, these
numbers give insight regarding the shape of a data distribution similar
to the sample quantiles in Section BLANK.


\subsection{How to do it with \textsf{R}}


\paragraph*{At the Command Prompt}

If the data are stored in a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!x!\inputencoding{utf8},
then you can compute the $5NS$ with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!fivenum!\inputencoding{utf8}
function.


\subsection{Boxplots\label{sub:Boxplots} }

A boxplot is essentially a graphical representation of the $5NS$.
It can be a handy alternative to a stripchart when the sample size
is large.

A boxplot is constructed by drawing a box alongside the data axis
with sides located at the upper and lower hinges. A line is drawn
parallel to the sides to denote the sample median. Lastly, whiskers
are extended from the sides of the box to the maximum and minimum
data values (more precisely, to the most extreme values that are not
potential outliers, defined below).

Boxplots are good for quick visual summaries of data sets, and the
relative positions of the values in the $5NS$ are good at indicating
the underlying shape of the data distribution, although perhaps not
as effectively as a histogram. Perhaps the greatest advantage of a
boxplot is that it can help to objectively identify extreme observations
in the data set as described in the next section.

Boxplots are also good because one can visually assess multiple features
of the data set simultaneously:
\begin{description}
\item [{Center}] can be estimated by the sample median, $\tilde{x}$.
\item [{Spread}] can be judged by the width of the box, $h_{U}-h_{L}$.
We know that this will be close to the $IQR$, which can be compared
to $s$ and the $MAD$, perhaps after rescaling if appropriate.
\item [{Shape}] is indicated by the relative lengths of the whiskers, and
the position of the median inside the box. Boxes with unbalanced whiskers
indicate skewness in the direction of the long whisker. Skewed distributions
often have the median tending in the opposite direction of skewness.
Kurtosis can be assessed using the box and whiskers. A wide box with
short whiskers will tend to be platykurtic, while a skinny box with
wide whiskers indicates leptokurtic distributions.
\item [{Extreme~observations}] are identified with open circles (see below).
\end{description}

\subsection{Outliers\label{sub:Outliers} }

A \emph{potential outlier} is any observation that falls beyond 1.5
times the width of the box on either side, that is, any observation
less than $h_{L}-1.5(h_{U}-h_{L})$ or greater than $h_{U}+1.5(h_{U}-h_{L})$.
A \emph{suspected outlier} is any observation that falls beyond 3
times the width of the box on either side. In \textsf{R}, both potential
and suspected outliers (if present) are denoted by open circles; there
is no distinction between the two. 

When potential outliers are present, the whiskers of the boxplot are
then shortened to extend to the most extreme observation that is not
a potential outlier. If an outlier is displayed in a boxplot, the
index of the observation may be identified in a subsequent plot in
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
by clicking the \textsf{Identify outliers with mouse} option in the
\textsf{Boxplot} dialog.

What do we do about outliers? They merit further investigation. The
primary goal is to determine why the observation is outlying, if possible.
If the observation is a typographical error, then it should be corrected
before continuing. If the observation is from a subject that does
not belong to the population of interest, then perhaps the datum should
be removed. Otherwise, perhaps the value is hinting at some hidden
structure to the data.


\subsubsection*{Standardizing variables}

It is sometimes useful to compare datasets with each other, on a scale
that does not depend on the measurement units.


\section{Multivariate Data and Data Frames\label{sec:Multivariate-Data}}

We have had experience with vectors of data, which are long lists
of numbers. Typically, each entry in the vector is a single measurement
on a subject or experimental unit in the study. We saw in Section
BLANK how to form vectors with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!c!\inputencoding{utf8}
function or the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scan!\inputencoding{utf8}
function. 

However, statistical studies often involve experiments where there
are two (or more) measurements associated with each subject. We display
the measured information in a rectangular array, or table. In the
table, each row corresponds to a subject, and the columns contain
the measurements for each respective variable. For instance, if one
were to measure the height and weight of each of 11 persons in a research
study, the information could be represented with a rectangular array.
There would be 11 rows. Each row would have the person's height in
the first column and weight in the second column.

The corresponding objects in \textsf{R} are called \emph{data frames},
and they can be constructed with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data.frame!\inputencoding{utf8}
function. Each row is an observation, and each column is a variable.
\begin{example}
Suppose we have two vectors \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
and we want to make a data frame out of them.
\end{example}
<<>>=
x <- 5:8
y <- letters[3:6]
data.frame(x,y)
@

Notice that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
are the same length. This is \emph{necessary}. Also notice that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
is a numeric vector and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
is a character vector. We may choose numeric and character vectors
(or even factors) for the columns of the dataframe, but each column
must be of exactly one type. That is, we can have a column for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!height!\inputencoding{utf8}
and a column for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!gender!\inputencoding{utf8},
but we will get an error if we try to mix function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!height!\inputencoding{utf8}
(numeric) and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!gender!\inputencoding{utf8}
(character or factor) information in the same column.


\subsection{Bivariate Data\label{sub:Bivariate-Data}}

What about the correlation coefficient?


\subsubsection*{Displaying Bivariate Data}
\begin{itemize}
\item Two-Way Tables. You can do this with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!table!\inputencoding{utf8},
or in the \textsf{R} Commander by following \textsf{Statistics $\triangleright$
Contingency Tables $\triangleright$} \textsf{Two-way Tables}. You
can also enter and analyze a two-way table. Example: BLANK
\item Scatterplot: look for linear association and correlation. Need a data
set that has linear association. Example BLANK.
\item Line Plot: good for displaying time series data. Example: BLANK
\item barplot(table(state.region, state.division))

\begin{itemize}
\item barplot(prop.table(table(state.region, state.division)))
\end{itemize}
\item spineplot(state.region, state.division) or spineplot(state.division
\textasciitilde{} state.region) 

\begin{itemize}
\item legend(\textquotedbl{}topright\textquotedbl{},legend=levels(state.division),fill=gray.colors(9))
\end{itemize}
\end{itemize}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
matplot(rnorm(100), rnorm(100), type="b", lty=1, pch=1)
@
\par\end{centering}

\caption{Line Graph of the salary variable\label{fig:Line-Graph-salary}}

\end{figure}



\subsection{Multivariate Data\label{sub:Multivariate-Data}}

Displaying Multivariate Data
\begin{itemize}
\item Multi-Way Tables. You can do this with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!table!\inputencoding{utf8},
or in \textsf{R} Commander by following \textsf{Statistics} \textsf{$\triangleright$}
\textsf{Contingency Tables} \textsf{$\triangleright$} \textsf{Multi-way
Tables}. Example: BLANK
\item Scatterplot Matrix. used for displaying pairwise scatterplots simultaneously.
Again, look for linear association and correlation. Need data here
that display multicollinearity. Example: BLANK
\item 3D Scatterplot. Need data here that follow a plane.
\item plot(state.region,state.division) 
\item barplot(table(state.division,state.region),legend.text=TRUE)
\end{itemize}

\section{Comparing Populations\label{sec:Comparing-Data-Sets}}

Sometimes we have data from two or more groups (or populations) and
we would like to compare them and draw conclusions. What we should
imagine is

Some issues that we would like to address:
\begin{itemize}
\item Comparing Centers and Spreads: Variation Within versus Between Groups
\item Comparing Clusters and gaps
\item Comparing Outliers and Unusual features
\item Comparing Shapes.
\end{itemize}

\subsection{Numerically}

I am thinking here about the \textsf{Statistics} \textsf{$\triangleright$}
\textsf{Numerical Summaries} \textsf{$\triangleright$ Summarize by
groups} option or the \textsf{Statistics} \textsf{$\triangleright$}
\textsf{Summaries} \textsf{$\triangleright$Table of Statistics} option. 


\subsection{Graphically}

The graphs that can be plotted by groups:
\begin{itemize}
\item Boxplot (Rcmdr, lattice)

\begin{itemize}
\item Variable Width: if this option is checked, then the width of the drawn
boxplots are proportional to $\sqrt{n_{i}}$, where $n_{i}$ is the
size of the $i^{\text{th}}$ group. Why? Because many statistics have
variability proportional to the reciprocal of the square root of the
sample size.
\item Notches: (if requested) extend to $1.58\cdot(h_{U}-h_{L})/\sqrt{n}$.
The idea is to give roughly a 95\% confidence interval for the difference
in two medians. See Chapter BLANK.
\end{itemize}
\item Stripchart(Rcmdr, console)
\item Histogram (lattice)
\item Scatterplot (Rcmdr, lattice) If the by groups option is selected then
the observations are color and symbol coded, depending on the group
to which they belong.
\item Scatterplot Matrices. (Rcmdr)
\item Cleveland Dotplot (console)
\item Plot of Means (Rcmdr): this one is useful for plotting the means of
a variable according to the levels of up to two factors. By default,
error bars are plotted. If \textquotedbl{}Standard Errors\textquotedbl{},
the default, error bars around means give plus or minus one standard
error of the mean; if \textquotedbl{}Standard Deviations\textquotedbl{},
error bars give plus or minus one standard deviation; if \textquotedbl{}Confidence
Intervals\textquotedbl{}, error bars give a confidence interval around
each mean; if \textquotedbl{}none\textquotedbl{}, error bars are suppressed.
\item Quantile-Quantile Plots: There are two ways to do this. One way is
to compare two independent samples (of the same size). qqplot(x,y).
Another way is to compare the sample quantiles of one variable to
the theoretical uantiles of another distribution. (Let's talk about
this in the probability chapter).
\end{itemize}
Given two samples $\left\{ x_{1},\, x_{2},\,\ldots,\, x_{n}\right\} $
and $\left\{ y_{1},\, y_{2},\,\ldots,\, y_{n}\right\} $, we may find
the order statistics $x_{(1)}\leq x_{(2)}\leq\cdots\leq x_{(n)}$
and $y_{(1)}\leq y_{(2)}\leq\cdots\leq y_{(n)}$. Next, plot the $n$
points $(x_{(1)},y_{(1)})$, $(x_{(2)},y_{(2)})$ ,\ldots{},$(x_{(n)},y_{(n)})$.

It is clear that if $x_{(k)}=y_{(k)}$ for all $k=1,2,\ldots,n$,
then we will have a straight line. It is also clear that in the real
world, a straight line is NEVER observed, and instead we have a scatterplot
that hopefully had a general linear trend. What do the rules tell
us?
\begin{itemize}
\item If the $y$-intercept of the line is greater (less) than zero, then
the center of the $Y$ data is greater (less) than the center of the
$X$ data.
\item If the slope of the line is greater (less) than one, then the spread
of the $Y$ data is greater (less) than the spread of the $X$ data..
\end{itemize}

\subsection{Lattice Graphics\label{sub:Lattice-Graphics}}

The following types of plots are useful when there is one variable
of interest and there is a factor in the dataset by which the variable
is categorized. need to attach(Dataset). 

Also need 

lattice.options(default.theme = \textquotedbl{}col.whitebg\textquotedbl{})


\paragraph*{Side by side boxplots}

bwplot( \textasciitilde{}before | gender)

%
\begin{figure}[H]
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 4>>=
library(lattice)
print(bwplot(~ weight | feed, data = chickwts))
@
\par\end{centering}

\caption{boxplots of weight by feed type}

\end{figure}





\paragraph*{Histograms}

histogram(\textasciitilde{} after | race)

%
\begin{figure}[H]
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 4>>=
library(lattice)
print(histogram(~age | education, data = infert))
@
\par\end{centering}

\caption{histograms of age by education level}

\end{figure}



\paragraph*{Scatterplots}

xyplot( salary \textasciitilde{} time | race)

%
\begin{figure}[H]
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 4>>=
library(lattice)
print(xyplot(Petal.Length ~ Petal.Width | Species, data = iris))
@
\par\end{centering}

\caption{xyplot of petal length versus petal width by species}

\end{figure}



\paragraph*{Coplots}

do ?coplot and look at the examples

%
\begin{figure}[H]
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 4>>=
library(lattice)
print(coplot(conc ~ uptake | Type * Treatment, data = CO2))
@
\par\end{centering}

\caption{coplot of reduction versus order by gender and smoke}

\end{figure}



\paragraph*{Shingle Plots}


\section{Chapter Exercises}

\setcounter{thm}{0}


\paragraph*{Directions: }

Open \textsf{R} and issue the following commands at the command line
to get started. Note that you need to have the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrPlugin.IPSUR!\inputencoding{utf8}
package installed, and for some exercises you need the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!e1071!\inputencoding{utf8}
package.
\begin{lyxcode}
library(RcmdrPlugin.IPSUR)~\\
data(RcmdrTestDrive)~~\\
attach(RcmdrTestDrive)~~\\
names(RcmdrTestDrive)~~\#~shows~names~of~variables
\end{lyxcode}
<<echo = FALSE, results = hide>>=
attach(RcmdrTestDrive)
names(RcmdrTestDrive)
@

To load the data in the \textsf{R} Commander (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}),
click the \textsf{Data Set} button, and select \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrTestDrive!\inputencoding{utf8}
as the active data set. To learn more about the data set and where
it comes from, type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!?RcmdrTestDrive!\inputencoding{utf8}
at the command line.
\begin{xca}
Perform a summary of all variables in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrTestDrive!\inputencoding{utf8}.
You can do this with the command \inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily}]
summary(RcmdrTestDrive)
\end{lstlisting}
\inputencoding{utf8}

Alternatively, you can do this in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
with the sequence \textsf{Statistics} \textsf{$\triangleright$ Summaries}
\textsf{$\triangleright$ Active Data Set}. Report the values of the
summary statistics for each variable.


\paragraph*{Answers:}

<<"Find summary statistics">>=
summary(RcmdrTestDrive)
@
\end{xca}

\begin{xca}
Make a table of the \emph{race} variable. Do this with \textsf{Statistics}
\textsf{$\triangleright$ Summaries} \textsf{$\triangleright$ IPSUR
- Frequency Distributions}\emph{...}
\begin{enumerate}
\item Which ethnicity has the highest frequency?
\item Which ethnicity has the lowest frequency?
\item Include a bar graph of \emph{race}. Do this with \textsf{Graphs} \textsf{$\triangleright$}
\textsf{IPSUR - Bar Graph}...
\end{enumerate}
\end{xca}

\paragraph*{Solution:}

First we will make a table of the \emph{race} variable with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!table!\inputencoding{utf8}
function.

<<>>=
table(race)
@
\begin{enumerate}
\item For these data, \Sexpr{names(table(race))[which(table(race)==max(table(race)))]}
has the highest frequency.
\item For these data, \Sexpr{names(table(race))[which(table(race)==min(table(race)))]}
has the lowest frequency.
\item The graph is shown below.
\end{enumerate}
\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
barplot(table(RcmdrTestDrive$race), main="", xlab="race", ylab="Frequency", legend.text=FALSE, col=NULL) 
@
\par\end{center}


\begin{xca}
Calculate the average \emph{salary} by the factor \emph{gender}. Do
this with \textsf{Statistics} \textsf{$\triangleright$ Summaries}
\textsf{$\triangleright$ Table of Statistics}\emph{...}
\begin{enumerate}
\item Which \emph{gender} has the highest mean \emph{salary}? 
\item Report the highest mean \emph{salary}.
\item Compare the spreads for the genders by calculating the standard deviation
of \emph{salary} by \emph{gender}. Which \emph{gender} has the biggest
standard deviation?
\item Make boxplots of \emph{salary} by \emph{gender} with the following
method:

\begin{quote}
On the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8},
click \textsf{Graphs} \textsf{$\triangleright$} \textsf{IPSUR - Boxplot}...\\
In the \textsf{Variable} box, select \emph{salar}y.\\
Click the \textsf{Plot by groups}... box and select \emph{gender}.
Click \textsf{OK}.\\
Click \textsf{OK} to graph the boxplot.
\end{quote}
How does the boxplot compare to your answers to (1) and (3)?

\end{enumerate}
\end{xca}

\paragraph*{Solution:}

We can generate a table listing the average salaries by gender with
two methods. The first uses \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!tapply!\inputencoding{utf8}:

<<>>=
x = tapply(RcmdrTestDrive$salary, list(gender=RcmdrTestDrive$gender), mean, na.rm=TRUE)
x
@

The second method uses the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!by!\inputencoding{utf8}
function:

<<keep.source = TRUE>>=
by(salary, gender, mean, na.rm=TRUE) # another way to do it
@

Now to answer the questions:
\begin{enumerate}
\item Which gender has the highest mean salary? 


We can answer this by looking above. For these data, the gender with
the highest mean salary is \Sexpr{names(x)[which(x==max(x))]}.

\item Report the highest mean salary.


Depending on our answer above, we would do something like \inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily}]
mean(salary[gender == Male])
\end{lstlisting}
\inputencoding{utf8} for example. For these data, the highest mean salary is 

<<>>=
x[which(x==max(x))]
@

\item Compare the spreads for the genders by calculating the standard deviation
of \emph{salary} by \emph{gender}. Which gender has the biggest standard
deviation?


<<>>=
y = tapply(RcmdrTestDrive$salary, list(gender=RcmdrTestDrive$gender), sd, na.rm=TRUE)
y
@

For these data, the the largest standard deviation is approximately
\Sexpr{round(y[which(y==max(y))],2)} which was attained by the \Sexpr{names(y)[which(y==max(y))]}
gender.

\item Make boxplots of \emph{salary} by \emph{gender}. How does the boxplot
compare to your answers to (1) and (3)?


The graph is shown below.

\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
boxplot(salary~gender, xlab="salary", ylab="gender", main="", notch=FALSE, varwidth=TRUE, horizontal=TRUE, data=RcmdrTestDrive) 
@
\par\end{center}

Answers will vary. There should be some remarks that the center of
the box is farther to the right for the \Sexpr{names(x)[which(x==max(x))]}
gender, and some recognition that the box is wider for the \Sexpr{names(y)[which(y==max(y))]}
gender.\end{enumerate}



\begin{xca}
For this problem we will study the variable \emph{reduction}.
\begin{enumerate}
\item Find the order statistics and store them in a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
\emph{Hint: }\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x <- sort(reduction)!\inputencoding{utf8}
\item Find $x_{(137)}$, the 137$^{\text{th}}$ order statistic.
\item Find the IQR.
\item Find the Five Number Summary (5NS).
\item Use the 5NS to calculate what the width of a boxplot of \emph{reduction}
would be.
\item Compare your answers (3) and (5). Are they the same? If not, are they
close?
\item Make a boxplot of \emph{reduction}, and include the boxplot in your
report. You can do this with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boxplot!\inputencoding{utf8}
function, or in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
with \textsf{Graphs} \textsf{$\triangleright$} \textsf{IPSUR - Boxplot}...
\item Are there any potential/suspected outliers? If so, list their values.
\emph{Hint:} use your answer to (a).
\item Using the rules discussed in the text, classify answers to (8), if
any, as \emph{potential} or \emph{suspected} outliers.
\end{enumerate}
\end{xca}

\paragraph*{Answers:}

<<echo = FALSE, results = hide>>=
x = sort(reduction)
@

<<>>=
x[137]
IQR(x)
fivenum(x)
fivenum(x)[4] - fivenum(x)[2]
@

\noindent Compare your answers (3) and (5). Are they the same? If
not, are they close?

Yes, they are close, within \Sexpr{abs(IQR(x)-(fivenum(x)[4] - fivenum(x)[2]))}
of each other.

\noindent The boxplot of \emph{reduction} is below.

\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
boxplot(reduction, xlab="reduction", main="", notch=FALSE, varwidth=TRUE, horizontal=TRUE, data=RcmdrTestDrive) 
@
\par\end{center}

<<>>=
in.fence = 1.5 * (fivenum(x)[4] - fivenum(x)[2]) + fivenum(x)[4]
out.fence = 3 * (fivenum(x)[4] - fivenum(x)[2]) + fivenum(x)[4]
which(x > in.fence)
which(x > out.fence)
@

Observations \Sexpr{which(x > in.fence)} would be considered potential
outliers, while observation(s) \Sexpr{which(x > out.fence)} would
be considered a suspected outlier.


\begin{xca}
In this problem we will compare the variables \emph{before} and \emph{after}.
Don't forget \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!library(e1071)!\inputencoding{utf8}.
\begin{enumerate}
\item Examine the two measures of center for both variables that you found
in Exercise BLANK. Judging from these measures, which variable has
a higher center?
\item Which measure of center is more appropriate for \emph{before}? (You
may want to look at a boxplot.) Which measure of center is more appropriate
for \emph{after}?
\item Based on your answer to (2), choose an appropriate measure of spread
for each variable, calculate it, and report its value. Which variable
has the biggest spread? (Note that you need to make sure that your
measures are on the same scale.) 
\item Calculate and report the skewness and kurtosis for \emph{before}.
Based on these values, how would you describe the shape of \emph{before}?
\item Calculate and report the skewness and kurtosis for \emph{after}. Based
on these values, how would you describe the shape of \emph{after}?
\item Plot histograms of \emph{before} and \emph{after} and compare them
to your answers to (4) and (5).
\end{enumerate}
\end{xca}

\paragraph*{Solution:}
\begin{enumerate}
\item Examine the two measures of center for both variables that you found
in problem 1. Judging from these measures, which variable has a higher
center?


We may take a look at the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(RcmdrTestDrive)!\inputencoding{utf8}
output from Exercise BLANK. Here we will repeat the relevant summary
statistics.

<<>>=
c(mean(before), median(before))
c(mean(after), median(after))
@

The idea is to look at the two measures and compare them to make a
decision. In a nice world, both the mean and median of one variable
will be larger than the other which sends a nice message. If We get
a mixed message, then we should look for other information, such as
extreme values in one of the variables, which is one of the reasons
for the next part of the problem.

\item Which measure of center is more appropriate for \emph{before}? (You
may want to look at a boxplot.) Which measure of center is more appropriate
for \emph{after}?


The boxplot of \emph{before} is shown below.

\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
boxplot(before, xlab="before", main="", notch=FALSE, varwidth=TRUE, horizontal=TRUE, data=RcmdrTestDrive) 
@
\par\end{center}

We want to watch out for extreme values (shown as circles separated
from the box) or large departures from symmetry. If the distribution
is fairly symmetric then the mean and median should be approximately
the same. But if the distribution is highly skewed with extreme values
then we should be skeptical of the sample mean, and fall back to the
median which is resistant to extremes. By design, the before variable
is set up to have a fairly symmetric distribution.

A boxplot of \emph{after} is shown next.

\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
boxplot(after, xlab="after", notch=FALSE, varwidth=TRUE, horizontal=TRUE, data=RcmdrTestDrive) 
@
\par\end{center}

The same remarks apply to the \emph{after} variable. The \emph{after}
variable has been designed to be left-skewed\ldots{} thus, the median
would likely be a good choice for this variable.

\item Based on your answer to (2), choose an appropriate measure of spread
for each variable, calculate it, and report its value. Which variable
has the biggest spread? (Note that you need to make sure that your
measures are on the same scale.) 


Since \emph{before} has a symmetric, mound shaped distribution, an
ecellent measure of center would be the sample standard deviation.
And since \emph{after} is left-skewed, we should use the median absolute
deviation. It is also acceptable to use the IQR, but we should rescale
it appropriately, namely, by dividing by 1.349. The exact values are
shown below.

<<>>=
sd(before)
mad(after)
IQR(after)/1.349
@

Judging from the values above, we would decide which variable has
the higher spread. Look at how close the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mad!\inputencoding{utf8}
and the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IQR!\inputencoding{utf8}
(after suitable rescaling) are; it goes to show why the rescaling
is important.

\item Calculate and report the skewness and kurtosis for \emph{before}.
Based on these values, how would you describe the shape of \emph{before}?


The values of these descriptive measures are shown below.

<<>>=
library(e1071)
skewness(before)
kurtosis(before)
@

We should take the sample skewness value and compare it to $2\sqrt{6/n}\approx$\Sexpr{round(2*sqrt(6/length(before)),3)}
in absolute value to see if it is substantially different from zero.
The direction of skewness is decided by the sign (positive or negative)
of the skewness value. 

We should take the sample kurtosis value and compare it to $2\cdot\sqrt{24/168}\approx$\Sexpr{round(4*sqrt(6/length(before)),3)}),
in absolute value to see if the excess kurtosis is substantially different
from zero. And take a look at the sign to see whether the distribution
is platykurtic or leptokurtic.

\item Calculate and report the skewness and kurtosis for \emph{after}. Based
on these values, how would you describe the shape of \emph{after}?


The values of these descriptive measures are shown below.

<<>>=
skewness(after)
kurtosis(after)
@

We should do for this one just like we did previously. We would again
compare the sample skewness and kurtosis values (in absolute value)
to \Sexpr{round(2*sqrt(6/length(after)),3)} and \Sexpr{round(4*sqrt(6/length(after)),3)},
respectively.

\item Plot histograms of \emph{before} and \emph{after} and compare them
to your answers to (4) and (5).


The graphs are shown below.

\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
hist(before, xlab="before", data=RcmdrTestDrive) 
@
\par\end{center}

\begin{center}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
hist(after, xlab="after", data=RcmdrTestDrive) 
@
\par\end{center}

Answers will vary. We are looking for visual consistency in the histograms
to our statements above.\end{enumerate}



\chapter{Probability}

In this chapter, we define the basic terminology associated with probability
and derive some of its properties. We discuss three interpretations
of probability. We discuss conditional probability and independent
events, along with Bayes' Theorem. We finish the chapter with an introduction
to random variables.

First we introduce the building blocks of probability, and we discuss
the interpretations of probability which define one of many paths
that we could take forward.

Next we discuss the basic mathematical properties of probability and
probability functions, with proofs, and develop some skill with the
machinery. The Equally Likely Model (ELM) is introduced.

Counting techniques are developed in the next section, which is particularly
pertinent given the ELM.

Conditional probability and examples are next, followed by independent
events, then Bayes Theorem.

We end with random variables and their connection to probability measures,
which paves the way for the next two chapters.

In this book we distinguish between two types of experiments: \emph{deterministic}
and \emph{random}. A \emph{deterministic} experiment is one whose
outcome may be predicted with certainty beforehand, such as combining
Hydrogen and Oxygen, or adding two numbers such as $2+3$. A \emph{random}
experiment is one whose outcome is determined by chance. We posit
that the outcome of a random experiment may not be predicted with
certainty beforehand, even in principle. Examples of random experiments
include tossing a coin, rolling a die, and throwing a dart on a board,
how many red lights you encounter on the drive home, how many ants
traverse a certain patch of sidewalk over a short period, \emph{etc}.


\section{Sample Spaces}

For a random experiment $E$, the set of all possible outcomes of
$E$ is called the \emph{sample space} and is denoted by the letter
$S$. For the coin-toss experiment, $S$ would be the results {}``Head''
and {}``Tail'', which we may represent by $S=\left\{ H,T\right\} $.
Formally, the performance of a random experiment is the unpredictable
selection of an outcome in $S$.


\subsection{How to do it with \textsf{R}}

Most of the probability work in this book is done with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package. A sample space is (usually) represented by a \emph{data frame},
that is, a rectangular collection of variables (see Section BLANK).
Each row of the data frame corresponds to an outcome of the experiment.
The data frame choice is convenient both for its simplicity and its
compatibility with the \textsf{R} Commander. Data frames alone are,
however, not sufficient to describe some of the more interesting probabilistic
applications we will study later; to handle those we will need to
consider a more general \emph{list} data structure. See Section BLANK
for details.
\begin{example}
Consider the random experiment of dropping a styrofoam cup onto the
floor from a height of four feet. The cup hits the ground and eventually
comes to rest. It could land upside down, right side up, or it could
land on its side. We represent these possible outcomes of the random
experiment by the following.

<<>>=
S <- data.frame(lands = c("down","up","side"))
S
@

The sample space \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8}
contains the column \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lands!\inputencoding{utf8}
which stores the outcomes \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"down"!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"up"!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"side"!\inputencoding{utf8}. 

\end{example}
Some sample spaces are so common that convenience wrappers were written
to set them up with minimal effort. The underlying machinery that
does the work includes the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!expand.grid!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!base!\inputencoding{utf8}
package, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!combn!\inputencoding{utf8}
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!combinat!\inputencoding{utf8}
package, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!permsn!\inputencoding{utf8}
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package%
\footnote{The seasoned \textsf{R} user can get the job done without the convenience
wrappers. I encourage their use for the beginner to get started, but
I also recommend that introductory students wean themselves as soon
as possible. The wrappers were designed for ease and intuitive use,
not for speed or efficiency.%
}.

Consider the random experiment of tossing a coin. The outcomes are
$H$ and $T$. We can set up the sample space quickly with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!tosscoin!\inputencoding{utf8}
function:

<<>>= 
library(prob)
tosscoin(1) 
@

The number \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1!\inputencoding{utf8}
tells \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!tosscoin!\inputencoding{utf8}
that we only want to toss the coin once. We could toss it three times: 

<<echo=TRUE,print=TRUE>>= 
tosscoin(3) 
@ 

Alternatively we could roll a fair die: 

<<echo=TRUE,print=TRUE>>= 
rolldie(1) 
@

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie!\inputencoding{utf8}
function defaults to a 6-sided die, but we can specify others with
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!nsides!\inputencoding{utf8}
argument. The command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie(3, nsides = 4)!\inputencoding{utf8}
would be used to roll a 4-sided die three times.

Perhaps we would like to draw one card from a standard set of playing
cards (it is a long data frame):

<<echo=TRUE,print=TRUE>>= 
head(cards()) 
@

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cards!\inputencoding{utf8}
function that we just used has optional arguments \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!jokers!\inputencoding{utf8}
(if you would like Jokers to be in the deck) and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!makespace!\inputencoding{utf8}
which we will discuss later. There is also a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!roulette!\inputencoding{utf8}
function which returns the sample space associated with one spin on
a roulette wheel. There are EU and USA versions available. Interested
readers may contribute any other game or sample spaces that may be
of general interest.


\subsection{Sampling from Urns}

This is perhaps the most fundamental type of random experiment. We
have an urn that contains a bunch of distinguishable objects (balls)
inside. We shake up the urn, reach inside, grab a ball, and take a
look. That's all.

But there are all sorts of variations on this theme. Maybe we would
like to grab more than one ball -- say, two balls. What are all of
the possible outcomes of the experiment now? It depends on how we
sample. We could select a ball, take a look, put it back, and sample
again. Another way would be to select a ball, take a look -- but do
not put it back -- and sample again (equivalently, just reach in and
grab two balls). There are certainly more possible outcomes of the
experiment in the former case than in the latter. In the first (second)
case we say that sampling is done \emph{with} (\emph{without}) \emph{replacement}.

There is more. Suppose we do not actually keep track of which ball
came first. All we observe are the two balls, and we have no idea
about the order in which they were selected. We call this \emph{unordered
sampling} (in contrast to \emph{ordered}) because the order of the
selections does not matter with respect to what we observe. We might
as well have selected the balls and put them in a bag before looking.

Note that this one general class of random experiments contains as
a special case all of the common elementary random experiments. Tossing
a coin twice is equivalent to selecting two balls labeled $H$ and
$T$ from an urn, with replacement. The die-roll experiment is equivalent
to selecting a ball from an urn with six elements, labeled 1 through
6.


\subsection{How to do it with \textsf{R} }

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package accomplishes sampling from urns with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
function, which has arguments \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!size!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!replace!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8}.
The argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
represents the urn from which sampling is to be done. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!size!\inputencoding{utf8}
argument tells how large the sample will be. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!replace!\inputencoding{utf8}
arguments are logical and specify how sampling will be performed.
We will discuss each in turn.
\begin{example}
Let our urn simply contain three balls, labeled 1, 2, and 3, respectively.
We are going to take a sample of size 2 from the urn. 

\subsubsection*{Ordered, With Replacement}

If sampling is with replacement, then we can get any outcome 1, 2,
or 3 on any draw. Further, by {}``ordered'' we mean that we shall
keep track of the order of the draws that we observe. We can accomplish
this in \textsf{R} with

<<echo=TRUE,print=TRUE>>=
urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)
@ 

Notice that rows 2 and 4 are identical, save for the order in which
the numbers are shown. Further, note that every possible pair of the
numbers 1 through 3 are listed. This experiment is equivalent to rolling
a 3-sided die twice, which we could have accomplished with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie(2, nsides = 3)!\inputencoding{utf8}.


\subsubsection*{Ordered, Without Replacement}

Here sampling is without replacement, so we may not observe the same
number twice in any row. Order is still important, however, so we
expect to see the outcomes \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1,2!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!2,1!\inputencoding{utf8}
somewhere in our data frame. 

<<echo=TRUE,print=TRUE>>= 
urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)
@ 

This is just as we expected. Notice that there are less rows in this
answer due to the more restrictive sampling procedure. If the numbers
1, 2, and 3 represented {}``Fred'', {}``Mary'', and {}``Sue'',
respectively, then this experiment would be equivalent to selecting
two people of the three to serve as president and vice-president of
a company, respectively, and the sample space shown above lists all
possible ways that this could be done.


\subsubsection*{Unordered, Without Replacement}

Again, we may not observe the same outcome twice, but in this case,
we will only retain those outcomes which (when jumbled) would not
duplicate earlier ones. 

<<echo=TRUE,print=TRUE>>= 
urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) 
@ 

This experiment is equivalent to reaching in the urn, picking a pair,
and looking to see what they are. This is the default setting of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8},
so we would have received the same output by simply typing \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples(1:3, 2)!\inputencoding{utf8}.


\subsubsection*{Unordered, With Replacement}

The last possibility is perhaps the most interesting. We replace the
balls after every draw, but we do not remember the order in which
the draws came. 

<<echo=TRUE,print=TRUE>>= 
urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 
@ 

We may interpret this experiment in a number of alternative ways.
One way is to consider this as simply putting two 3-sided dice in
a cup, shaking the cup, and looking inside -- as in a game of \emph{Liar's
Dice}, for instance. Each row of the sample space is a potential pair
we could observe. Another way is to view each outcome as a separate
methodway to distribute two identical golf balls into three boxes
labeled 1, 2, and 3. Regardless of the interpretation, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
lists every possible way that the experiment can conclude.

\end{example}
Note that the urn does not need to contain numbers; we could have
just as easily taken our urn to be \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x = c("Red","Blue","Green")!\inputencoding{utf8}.
But, there is an \textbf{important} point to mention before proceeding.
Astute readers will notice that in our example, the balls in the urn
were \textit{distinguishable} in the sense that each had a unique
label to distinguish it from the others in the urn. A natural question
would be, {}``What happens if your urn has indistinguishable elements,
for example, what if \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x = c("Red","Red","Blue")!\inputencoding{utf8}?''
The answer is that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
behaves as if each ball in the urn is distinguishable, regardless
of its actual contents. We may thus imagine that while there are two
red balls in the urn, the balls are such that we can tell them apart
(in principle) by looking closely enough at the imperfections on their
surface.

In this way, when the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
argument of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
has repeated elements, the resulting sample space may appear to be
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered = TRUE!\inputencoding{utf8}
even when, in fact, the call to the function was \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples(..., ordered = FALSE)!\inputencoding{utf8}.
Similar remarks apply for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!replace!\inputencoding{utf8}
argument. We investigate this issue further in Section BLANK.


\section{Events}

An \emph{event} $A$ is merely a collection of outcomes, or in other
words, a subset of the sample space%
\footnote{This naive definition works for finite or countably infinite sample
spaces, but is inadequate for sample spaces in general. In this book,
we will not address the subtleties that arise, but will refer the
interested reader to any text on advanced probability or measure theory.%
}. After the performance of a random experiment $E$ we say that the
event $A$ \emph{occurred} if the experiment's outcome belongs to
$A$. We say that a bunch of events $A_{1}$, $A_{2}$, $A_{3}$,
\ldots{} are \emph{mutually exclusive} or \emph{disjoint} if $A_{i}\cap A_{j}=\emptyset$
for any distinct pair $A_{i}\neq A_{j}$. For instance, in the coin-toss
experiment the events $A=\left\{ \mbox{Heads}\right\} $ and $B=\left\{ \mbox{Tails}\right\} $
would be mutually exclusive. Now would be a good time to review the
algebra of sets in Appendix BLANK.


\subsection{How to do it with \textsf{R}}

Given a data frame sample/probability space \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8},
we may extract rows using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]![]!\inputencoding{utf8}
operator: 

<<echo=TRUE,print=TRUE>>= 
S <- tosscoin(2, makespace = TRUE) 
S[1:3, ] 
S[c(2,4), ] 
@ 

and so forth. We may also extract rows that satisfy a logical expression
using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!subset!\inputencoding{utf8}
function, for instance 

<<echo=TRUE,print=FALSE>>= 
S <- cards() 
@ 

<<echo=TRUE,print=TRUE>>= 
subset(S, suit == "Heart") 
subset(S, rank %in% 7:9)
@ 

We could continue indefinitely. Also note that mathematical expressions
are allowed: 

<<echo=TRUE,print=TRUE>>= 
subset(rolldie(3), X1+X2+X3 > 16) 
@


\subsection{Functions for Finding Subsets}

It does not take long before the subsets of interest become complicated
to specify. Yet the main idea remains: we have a particular logical
condition to apply to each row. If the row satisfies the condition,
then it should be in the subset. It should not be in the subset otherwise.
The ease with which the condition may be coded depends of course on
the question being asked. Here are a few functions to get started.


\subsubsection*{The \texttt{\%in\%} function}

The function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!%in%!\inputencoding{utf8}
helps to learn whether each value of one vector lies somewhere inside
another vector. 

<<echo=TRUE,print=FALSE>>= 
x <- 1:10 
y <- 8:12 
y %in% x
@ 

Notice that the returned value is a vector of length 5 which tests
whether each element of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
is in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
in turn.


\subsubsection*{The \texttt{isin} function}

It is more common to want to know whether the \emph{whole} vector
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
is in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
We can do this with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!isin!\inputencoding{utf8}
function. 

<<echo=TRUE,print=TRUE>>= 
isin(x,y) 
@

Of course, one may ask why we did not try something like \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!all(y %in% x)!\inputencoding{utf8},
which would give a single result, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!TRUE!\inputencoding{utf8}.
The reason is that the answers are different in the case that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
has repeated values. Compare: 

<<echo=TRUE,print=FALSE>>= 
x <- 1:10 
y <- c(3,3,7) 
@ 

<<echo=TRUE,print=TRUE>>= 
all(y %in% x)
isin(x,y) 
@ 

The reason for the above is of course that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
contains the value 3, but \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
does not have \emph{two} 3's. The difference is important when rolling
multiple dice, playing cards, \emph{etc}. Note that there is an optional
argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8}
which tests whether the elements of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}
appear in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
in the order in which they are appear in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!y!\inputencoding{utf8}.
The consequences are 

<<echo=TRUE,print=TRUE>>= 
isin(x, c(3,4,5), ordered = TRUE) 
isin(x, c(3,5,4), ordered = TRUE) 
@ 

The connection to probability is that have a data frame sample space
and we would like to find a subset of that space. A \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data.frame!\inputencoding{utf8}
method was written for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!isin!\inputencoding{utf8}
that simply applies the function to each row of the data frame. We
can see the method in action with the following: 

<<echo=TRUE,print=FALSE>>= 
S <- rolldie(4) 
subset(S, isin(S, c(2,2,6), ordered = TRUE)) 
@

There are a few other functions written to find useful subsets, namely,
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!countrep!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!isrep!\inputencoding{utf8}.
Essentially these were written to test for (or count) a specific number
of designated values in outcomes. See the documentation for details.


\subsection{Set Union, Intersection, and Difference}

Given subsets $A$ and $B$, it is often useful to manipulate them
in an algebraic fashion. To this end, we have three set operations
at our disposal: union, intersection, and difference. Below is a table
that summarizes the pertinent information about these operations.

\begin{center}
\begin{tabular}{|l|c|l|l|}
\hline 
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
Name  & Denoted  & Defined by elements  & Code \tabularnewline
\hline 
Union  & $A\cup B$  & in $A$ or $B$ or both  & \texttt{union(A,B)} \tabularnewline
Intersection  & $A\cap B$  & in both $A$ and $B$  & \texttt{intersect(A,B)} \tabularnewline
Difference  & $A\textbackslash B$  & in $A$ but not in $B$  & \texttt{setdiff(A,B)} \tabularnewline
\hline
\end{tabular}
\par\end{center}

Some examples follow. 

<<echo=TRUE,print=FALSE>>= 
S = cards() 
A = subset(S, suit == "Heart") 
B = subset(S, rank %in% 7:9)
@ 

We can now do some set algebra: 

<<echo=TRUE,print=TRUE>>= 
union(A,B) 
intersect(A,B) 
setdiff(A,B) 
setdiff(B,A) 
@ 

Notice that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!setdiff!\inputencoding{utf8}
is not symmetric. Further, note that we can calculate the \emph{complement}
of a set $A$, denoted $A^{c}$ and defined to be the elements of
$S$ that are not in $A$ simply with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!setdiff(S,A)!\inputencoding{utf8}.

There have been methods written for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!intersect!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!setdiff!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!subset!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!union!\inputencoding{utf8}
in the case that the input objects are of class \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ps!\inputencoding{utf8}.
See Section BLANK.
\begin{note}
When you load the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package you will most certainly notice the message: {}``\texttt{The
following object(s) are masked from package:base: intersect setdiff,
subset, union}''. The reason for this message is that there already
exist methods for the functions \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!intersect!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!setdiff!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!subset!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!union!\inputencoding{utf8}
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!base!\inputencoding{utf8}
package which ships with \textsf{R}. However, these methods were designed
for when the arguments are vectors of the same mode. Since we are
manipulating sample spaces which are data frames and lists, it was
necessary to write methods to handle those cases as well. When the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package is loaded, \textsf{R} recognizes that there are multiple versions
of the same function in the search path and acts to shield the new
definitions from the existing ones. But there is no cause for alarm,
thankfully, because the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
functions have been carefully defined to match the usual \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!base!\inputencoding{utf8}
package definition in the case that the arguments are vectors.
\end{note}

\section{Model Assignment\label{sec:Interpreting-Probabilities}}

Let us take a look at the coin-toss experiment more closely. What
do we mean when we say {}``the probability of Heads'' or write $\P(\mbox{Heads})$?
Given a coin and an itchy thumb, how do we go about finding what $\P(\mbox{Heads})$
should be? There are three main approaches.


\subsection{The Measure Theory Approach}

This approach states that the way to handle $\P(\mbox{Heads})$ is
to define a mathematical function, called a \emph{probability measure},
on the sample space. Probability measures satisfy certain axioms (to
be introduced later) and have special mathematical properties, so
not just any mathematical function will do. But in any given physical
circumstance there are typically all sorts of probability measures
from which to choose, and it is left to the experimenter to make a
reasonable choice -- usually based on considerations of objectivity.
For the tossing coin example, a valid probability measure assigns
probability $p$ to the event $\left\{ \mbox{Heads}\right\} $, where
$p$ is some number $0\leq p\leq1$. An experimenter that wishes to
incorporate the symmetry of the coin would choose $p=1/2$ to balance
the likelihood of $\left\{ \mbox{Heads}\right\} $ and $\left\{ \mbox{Tails}\right\} $
.

Once the probability measure is chosen (or determined), there is not
much left to do. All assignments of probability are made by the probability
function, and the experimenter needs only to plug the event $\left\{ \mbox{Heads}\right\} $
into to the probability function to find $\P(\mbox{Heads})$. In this
way, the probability of an event is simply a calculated value, nothing
more, nothing less. Of course this is not the whole story; there are
many theorems and consequences associated with this approach that
will keep us occupied for the remainder of this book. The approach
is called \emph{measure theory} because the measure (probability)
of a set (event) is associated with how big it is (how likely it is
to occur).

The measure theory approach is well suited for situations where there
is symmetry to the experiment, such as flipping a balanced coin or
spinning an arrow around a circle with well-defined pie slices. It
is also handy because of its mathematical simplicity, elegance, and
flexibility. There are literally volumes of information that one can
prove about probability measures, and the cold rules of mathematics
allow us to analyze intricate probabilistic problems with vigor. 

The large degree of flexibility is also a disadvantage, however. When
symmetry fails it is not always obvious what an {}``objective''
choice of probability measure should be; for instance, what probability
should we assign to $\left\{ \mbox{Heads}\right\} $ if we spin the
coin rather than flip it? (It is not $1/2$.) Furthermore, the mathematical
rules are restrictive when we wish to incorporate subjective knowledge
into the model, knowledge which changes over time and depends on the
experimenter, such as personal knowledge about the properties of the
specific coin being flipped, or of the person doing the flipping.

The mathematician who revolutionized this way to do probability theory
was Andrey Kolmogorov, who published a landmark monograph in 1933.
See \url{http://www-history.mcs.st-andrews.ac.uk/Mathematicians/Kolmogorov.html}
for more information.


\subsection{Relative Frequency Approach}

This approach states that the way to determine $\P(\mbox{Heads})$
is to flip the coin repeatedly, in exactly the same way each time.
Keep a tally of the number of flips and the number of Heads observed.
Then a good approximation to $\P(\mbox{Heads})$ will be\begin{equation}
\P(\mbox{Heads})\approx\frac{\mbox{number of observed Heads}}{\mbox{total number of flips}}.\end{equation}


The mathematical underpinning of this approach is the celebrated \textbf{Law
of Large Numbers}, which may be loosely described as follows. Let
$E$ be a random experiment in which the event $A$ either does or
does not occur. Perform the experiment repeatedly, in an identical
manner, in such a way that the successive experiments do not influence
each other. After each experiment, keep a running tally of whether
or not the event $A$ occurred. Let $S_{n}$ count the number of times
that $A$ occurred in the $n$ experiments. Then the law of large
numbers says that \begin{equation}
\frac{S_{n}}{n}\to\P(A)\mbox{ \,\ as }n\to\infty.\end{equation}


As the reasoning goes, to learn about the probability of an event
$A$ we need only repeat the random experiment to get a reasonable
estimate of its numerical value, and if we are not satisfied with
our estimate then we may simply repeat the experiment more times,
all the while confident that with more and more experiments our estimate
will stabilize to the true value. 

The frequentist approach is good because it is relatively light on
assumptions and does not worry about symmetry or claims of objectivity
like the measure-theoretic approach does. It is perfect for the spinning
coin experiment. One drawback to the method is that one can never
know the exact value of a probability, only a long-run approximation.
It also does not work well with experiments that can not be repeated
indefinitely, say, the probability that it will rain today, the chances
that you get will get an A in your Statistics class, or the probability
that the world is destroyed by nuclear war.

This approach was espoused by Richard von Mises in the early twentieth
century, and some of his main ideas were incorporated into the measure
theory approach. See \url{http://www-history.mcs.st-andrews.ac.uk/Biographies/Mises.html}
for more.


\subsection{The Subjective Approach}

The subjective approach interprets probability as the experimenter's
\emph{degree of belief} that the event will occur. The estimate of
the probability of an event is based on the totality of the individual's
knowledge at the time. As new information becomes available, the estimate
is modified accordingly to best reflect his/her current knowledge.
The method by which the probabilities are updated is commonly done
with Bayes' Rule, discussed in Section BLANK. 

So for the coin toss example, a person may have $\P(\mbox{Heads})=1/2$
in the absence of additional information. But perhaps the observer
knows additional information about the coin or the thrower that would
shift the probability in a certain direction. For instance, parlor
magicians may be trained to be quite skilled at tossing coins, and
some are so skilled that they may toss a fair coin and get nothing
but Heads, indefinitely. I have \emph{seen} this. It was similarly
claimed in \emph{Bringing Down the House} BLANK that MIT students
were accomplished enough with cards to be able to cut a deck to the
same location, every single time. In such cases, one clearly should
use the additional information to assign $\P(\mbox{Heads})$ away
from the symmetry value of $1/2$.

This approach works well in situations that cannot be repeated indefinitely,
for example, to assign your probability that you will get an A in
this class, the chances of a devastating nuclear war, or the likelihood
that a cure for the common cold will be discovered.

The roots of subjective probability reach back a long time. See \url{http://en.wikipedia.org/wiki/Subjective_probability}
for a short discussion and links to references about the subjective
approach.


\subsection{Equally Likely Model (ELM)}

We have seen several approaches to the assignment of a probability
model to a given random experiment and they are very different in
thier underlying interpretation. But they all cross paths when it
comes to the equally likely model which assigns equal probability
to all elementary outcomes of the experiment.

The ELM appears in the measure theory approach when the experiment
boasts symmetry of some kind. If symmetry guarantees that all outcomes
have equal {}``size'', and if outcomes with equal {}``size'' should
get the same probability, then the ELM is a logical objective choice
for the experimenter. Consider the balanced 6-sided die, the fair
coin, or the dart board with equal-sized wedges.

The ELM appears in the subjective approach when the experimenter resorts
to indifference or ignorance with respect to his/her knowledge of
the outcome of the experiment. If the experimenter has no prior knowledge
to suggest that (s)he prefer Heads over Tails, then it is reasonable
for the him/her to assign equal subjective probability to both possible
outcomes.

The ELM appears in the relative frequency approach as a fascinating
fact of Nature: when we flip balanced coins over and over again, we
observe that the proportion of times that the coin comes up Heads
tends to $1/2$. Of course if we assume that the measure theory applies
then we can prove that the sample proportion must tend to 1/2 as expected,
but that is putting the cart before the horse, in a manner of speaking.

The ELM is only available when there are finitely many elements in
the sample space.


\subsection{How to do it with \textsf{R}}

In the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package, a probability space is an object of outcomes \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8}
and a vector of probabilities (called {}``\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}'')
with entries that correspond to each outcome in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8}.
When \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8}
is a data frame, we may simply add a column called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8}
and we will be finished; the probability space will simply be a data
frame which we may call \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8}.
In the case that S is a list, we may combine the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!outcomes!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
into a larger list, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!space!\inputencoding{utf8};
it will have two components: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!outcomes!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}.
The only requirements we need are for the entries of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
to be nonnegative and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sum(probs)!\inputencoding{utf8}
to be one.

To accomplish this in \textsf{R}, we may use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace!\inputencoding{utf8}
function. The general syntax is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace(x, probs)!\inputencoding{utf8},
where \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
is a sample space of outcomes and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
is a vector (of the same length as the number of outcomes in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}).
The specific choice of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
depends on the context of the problem, and some examples follow to
demonstrate some of the more common choices.
\begin{example}
The Equally Likely Model asserts that every outcome of the sample
space has the same probability, thus, if a sample space has $n$ outcomes,
then \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
would be a vector of length $n$ with identical entries $1/n$. The
quickest way to generate \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
is with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rep!\inputencoding{utf8}
function. We will start with the experiment of rolling a die, so that
$n=6$. We will construct the sample space, generate the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
vector, and put them together with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace!\inputencoding{utf8}. 

<<echo=TRUE,print=TRUE>>= 
outcomes <- rolldie(1) 
p <- rep(1/6, times = 6) 
probspace(outcomes, probs = p) 
@ 

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace!\inputencoding{utf8}
function is designed to save us some time in many of the most common
situations. For example, due to the especial simplicity of the sample
space in this case, we could have achieved the same result with only
(note the name change for the first column) 

<<echo=TRUE,print=TRUE>>= 
probspace(1:6, probs = p) 
@ 

Further, since the equally likely model plays such a fundamental role
in the study of probability the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace!\inputencoding{utf8}
function will assume that the equally model is desired if no \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
are specified. Thus, we get the same answer with only 

<<echo=TRUE,print=TRUE>>= 
probspace(1:6) 
@ 

And finally, since rolling dice is such a common experiment in probability
classes, the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie!\inputencoding{utf8}
function has an additional logical argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!makespace!\inputencoding{utf8}
that will add a column of equally likely \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
to the generated sample space: 

<<echo=TRUE,print=TRUE>>= 
rolldie(1, makespace = TRUE)
@ 

or just \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie(1, TRUE)!\inputencoding{utf8}.
Many of the other sample space functions (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!tosscoin!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cards!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!roulette!\inputencoding{utf8},
\textit{etc}.) have similar \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!makespace!\inputencoding{utf8}
arguments. Check the documentation for details.

\end{example}
One sample space function that does NOT have a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!makespace!\inputencoding{utf8}
option is the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
function. This was intentional. The reason is that under the varied
sampling assumptions the outcomes in the respective sample spaces
are NOT, in general, equally likely. It is important for the user
to carefully consider the experiment to decide whether or not the
outcomes are equally likely and then use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace!\inputencoding{utf8}
to assign the model.
\begin{example}
An unbalanced coin. While the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!makespace!\inputencoding{utf8}
argument to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!tosscoin!\inputencoding{utf8}
is useful to represent the tossing of a \emph{fair} coin, it is not
always appropriate. For example, suppose our coin is not perfectly
balanced, for instance, maybe the {}``$H$'' side is somewhat heavier
such that the chances of a $H$ appearing in a single toss is 0.70
instead of 0.5. We may set up the probability space with 

<<echo=TRUE,print=TRUE>>= 
probspace(tosscoin(1), probs = c(0.70, 0.30)) 
@ 

The same procedure can be used to represent an unbalanced die, roulette
wheel, \textit{etc}.

\end{example}

\subsection{Words of Warning}

It should be mentioned that while the splendour of \textsf{R }is uncontested,
it, like everything else, has limits both with respect to the sample/probability
spaces it can manage and with respect to the finite accuracy of the
representation of most numbers (see the \textsf{R} FAQ 7.31). When
playing around with probability, one may be tempted to set up a probability
space for tossing 100 coins or rolling 50 dice in an attempt to answer
some scintillating question. (Bear in mind: rolling a die just 9 times
has a sample space with over \emph{10 million} outcomes.)

Alas! Even if there were enough RAM to barely hold the sample space
(and there were enough time to wait for it to be generated), the infinitesimal
probabilities that are associated with SO MANY outcomes make it difficult
for the underlying machinery to handle reliably. In some cases, special
algorithms need to be called just to give something that holds asymptotically.
User beware.


\section{Properties of Probability\label{sec:Properties-of-Probability}}


\subsection{Probability Functions\label{sub:Probability-Functions}}

A \emph{probability function} is a rule that associates with each
event $A$ of the sample space a unique number $\P(A)=p$, called
the probability of $A$. Any probability function $\P$ satisfies
the following three Kolmogorov Axioms:
\begin{ax}
$\P(A)\geq0$ for any event $A\subset S$.
\end{ax}

\begin{ax}
$\P(S)=1$.
\end{ax}

\begin{ax}
If the events $A_{1}$, $A_{2}$, $A_{3}$\ldots{} are disjoint then\begin{equation}
\P\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\P(A_{i})\mbox{ for every }n,\end{equation}
and furthermore,\begin{equation}
\P\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\P(A_{i}).\end{equation}

\end{ax}
The intuition behind the axioms: first, the probability of an event
should never be negative. And since the sample space contains all
possible outcomes, its probability should be one, or 100\%. The final
axiom may look intimidating, but it simply means that for a sequence
of disjoint events (in other words, sets that do not overlap), their
total probability (measure) should equal the sum of its parts. For
example, the chance of rolling a 1 or a 2 on a die is the chance of
rolling a 1 plus the chance of rolling a 2. The connection to measure
theory could not be more clear.


\subsection{Properties}

For any events $A$ and $B$,
\begin{enumerate}
\item $\P(A^{c})=1-\P(A)$. \begin{proof}
Since $A\cup A^{c}=S$ and $A\cap A^{c}=\emptyset$, we have\[
1=\P(S)=\P(A\cup A^{c})=\P(A)+\P(A^{c}).\]

\end{proof}

\item $\P(\emptyset)=0$.

\begin{proof}
Note that $\emptyset=S^{c}$, and use Property 1.
\end{proof}
\item If $A\subset B$ , then $\P(A)\leq\P(B)$.

\begin{proof}
Write $B=A\cup\left(B\cap A^{c}\right)$, and notice that $A\cap\left(B\cap A^{c}\right)=\emptyset$;
thus\[
\P(B)=\P(A\cup\left(B\cap A^{c}\right))=\P(A)+\P\left(B\cap A^{c}\right)\geq\P(A),\]
since $\P\left(B\cap A^{c}\right)\ge0$. 
\end{proof}
\item $0\leq\P(A)\leq1$.

\begin{proof}
The left inequality is immediate from Axiom 1, and the second inequality
follows from Property 5 since $A\subset S$.
\end{proof}
\item \textbf{\emph{The General Addition Rule.}}\begin{equation}
\P(A\cup B)=\P(A)+\P(B)-\P(A\cap B).\end{equation}



More generally, for events $A_{1}$, $A_{2}$, $A_{3}$,\ldots{},
$A_{n}$,\begin{equation}
\P\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\P(A_{i})-\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\P(A_{i}\cap A_{j})+\cdots+(-1)^{n-1}\P\left(\bigcap_{i=1}^{n}A_{i}\right)\end{equation}


\item \textbf{\emph{The Theorem of Total Probability.}} Let $B_{1}$, $B_{2}$,
\ldots{}, $B_{n}$ be mutually exclusive and exhaustive. Then\begin{equation}
\P(A)=\P(A\cap B_{1})+\P(A\cap B_{2})+\cdots+\P(A\cap B_{n}).\end{equation}

\end{enumerate}

\subsection{Assigning Probabilities}

A model of particular interest is the \emph{equally likely model}.
The idea is to divide the sample space $S$ into a finite collection
of elementary events $\left\{ a_{1},\ a_{2},\ldots,a_{N}\right\} $
that are equally likely in the sense that each $a_{i}$ has equal
chances of occurring. The probability function associated with this
model must satisfy $\P(S)=1$, by Axiom 2. On the other hand, it must
also satisfy\[
\P(S)=\P(\left\{ a_{1},\ a_{2},\ldots,a_{N}\right\} )=\P(a_{1}\cup a_{2}\cup\cdots\cup a_{N})=\sum_{i=1}^{N}\P(a_{i}),\]
by Axiom 3. Since $\P(a_{i})$ is the same for all $i$, each one
necessarily equals $1/N$. 

For an event $A\subset S$, we write it as a collection of elementary
outcomes: if $A=\left\{ a_{i_{1}},a_{i_{2}},\ldots,a_{i_{k}}\right\} $
then $A$ has $k$ elements and \begin{align*}
\P(A) & =\P(a_{i_{1}})+\P(a_{i_{2}})+\cdots+\P(a_{i_{k}}),\\
 & =\frac{1}{N}+\frac{1}{N}+\cdots+\frac{1}{N},\\
 & =\frac{k}{N}=\frac{\#(A)}{\#(S)}.\end{align*}
In other words, under the equally likely model, the probability of
an event $A$ is determined by the number of elementary events that
$A$ contains.
\begin{example}
Consider the random experiment $E$ of tossing a coin. Then the sample
space is $S=\{H,T\}$, and under the equally likely model, these two
outcomes have $\P(H)=\P(T)=1/2$. This model is taken when it is reasonable
to assume that the coin is fair.
\end{example}

\begin{example}
Suppose the experiment $E$ consists of tossing a fair coin twice.
The sample space may be represented by $S=\{HH,\, HT,\, TH,\, TT\}$.
Given that the coin is fair and that the coin is tossed in an independent
and identical manner, it is reasonable to apply the equally likely
model. 

What is $\P(\mbox{at least 1 Head})$? Looking at the sample space
we see the elements $HH$, $HT$, and $TH$ have at least one Head;
thus, $\P(\mbox{at least 1 Head})=3/4$. 

What is $\P(\mbox{no Heads})$? Notice that the event $\left\{ \mbox{no Heads}\right\} =\left\{ \mbox{at least one Head}\right\} ^{c}$,
which by Property BLANK means $\P(\mbox{no Heads})=1-\P(\mbox{at least one Head})=1-3/4=1/4$.
It is obvious in this simple example that the only outcome with no
Heads is $TT$, however, this complementation trick is useful in more
complicated circumstances.
\end{example}

\begin{example}
Imagine a three child family, each child being either Boy ($B$) or
Girl ($G$). An example sequence of siblings would be $BGB$. The
sample space may be written\[
S=\left\{ \begin{array}{cccc}
BBB, & BGB, & GBB, & GGB,\\
BBG, & BGG, & GBG, & GGG\end{array}\right\} .\]
Note that for many reasons (for instance, it turns out that girls
are slightly more likely to be born than boys), this sample space
is \emph{not} equally likely. For the sake of argument, however, we
will assume that the elementary outcomes each have probability $1/8$.

What is $\P(\mbox{exactly 2 Boys})$? Inspecting the sample space
reveals three outcomes with exactly two boys: $\left\{ BBG,\, BGB,\, GBB\right\} $.
Therefore $\P(\mbox{exactly 2 Boys})=3/8$. 

What is $\P(\mbox{at most 2 Boys})$? One way to solve the problem
would be to count the outcomes that have 2 or less Boys, but a quicker
way would be to recognize that the only way that the event $\left\{ \mbox{at most 2 Boys}\right\} $
does \emph{not} occur is the event $\left\{ \mbox{all Girls}\right\} $.
Thus\[
\P(\mbox{at most 2 Boys})=1-\P(GGG)=1-1/8=7/8.\]

\end{example}

\begin{example}
Consider the experiment of rolling a six-sided die, and let the outcome
be the face showing up when the die comes to rest. Then $S=\left\{ 1,2,3,4,5,6\right\} $.
It is usually reasonable to suppose that the die is fair, so that
the six outcomes are equally likely.
\end{example}

\begin{example}
Consider a standard deck of 52 cards. These are usually labeled with
the four \emph{suits}: Clubs, Diamonds, Hearts, and Spades, and the
13 \emph{ranks}: 2, 3, 4, \ldots{}, 10, Jack (J), Queen (Q), King
(K), and Ace (A). Depending on the game played, the Ace may be ranked
below 2 or above King. 

Let the random experiment $E$ consist of drawing exactly one card
from a well-shuffled deck, and let the outcome be the face of the
card. Define the events $A=\left\{ \mbox{draw an Ace}\right\} $ and
$B=\left\{ \mbox{draw a Club}\right\} $. Bear in mind: we are only
drawing one card.

Immediately we have $\P(A)=4/52$ since there are four Aces in the
deck; similarly, there are $13$ Clubs which implies $\P(B)=13/52$.

What is $\P(A\cap B)$? We realize that there is only one card of
the 52 which is an Ace and a Club at the same time, namely, the Ace
of Clubs. Therefore $\P(A\cap B)=1/52$.

To find $\P(A\cup B)$ we may use the above with the General Addition
Rule to get\begin{eqnarray*}
\P(A\cup B) & = & \P(A)+\P(B)-\P(A\cap B),\\
 & = & 4/52+13/52-1/52,\\
 & = & 16/52.\end{eqnarray*}

\end{example}

\begin{example}
Staying with the deck of cards, let another random experiment be the
selection of a five card stud poker hand, where {}``five card stud''
means that we draw exactly five cards from the deck without replacement,
no more, and no less. It turns out that the sample space $S$ is so
large and complicated that we will be obliged to settle for the trivial
description $S=\left\{ \mbox{all possible 5 card hands}\right\} $
for the time being. We will have a more precise description later.

What is $\P(\mbox{Royal Flush})$, or in other words, $\P(\mbox{A, K, Q, J, 10 all in the same suit})$? 

It should be clear that there are only four possible royal flushes.
Thus, if we could only count the number of outcomes in $S$ then we
could simply divide four by that number and we would have our answer
under the equally likely model. This is the subject of Section BLANK.
\end{example}

\subsection{How to do it with \textsf{R}}

Probabilities are calculated in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
function.

Consider the experiment of drawing a card from a standard deck of
playing cards. Let's denote the probability space associated with
the experiment as \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!S!\inputencoding{utf8},
and let the subsets \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!A!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!B!\inputencoding{utf8}
be defined by the following: 

<<echo=TRUE,print=FALSE>>= 
S <- cards(makespace = TRUE) 
A <- subset(S, suit == "Heart") 
B <- subset(S, rank %in% 7:9)
@ 

Now it is easy to calculate 

<<echo=TRUE,print=TRUE>>= 
prob(A) 
@ 

Note that we can get the same answer with 

<<echo=TRUE,print=TRUE>>= 
prob(S, suit == "Heart") 
@ 

We also find \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob(B) = 0.23!\inputencoding{utf8}
(listed here approximately, but 12/52 actually) and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob(S) = 1!\inputencoding{utf8}.
Internally, the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
function operates by summing the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
column of its argument. It will find subsets on-the-fly if desired.

We have as yet glossed over the details. More specifically, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
has three arguments: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
which is a probability space (or a subset of one), \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!event!\inputencoding{utf8},
which is a logical expression used to define a subset, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!given!\inputencoding{utf8},
which is described in Section BLANK.

\emph{WARNING}. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!event!\inputencoding{utf8}
argument is used to define a subset of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
that is, the only outcomes used in the probability calculation will
be those that are elements of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
and satisfy \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!event!\inputencoding{utf8}
simultaneously. In other words, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob(x, event)!\inputencoding{utf8}
calculates \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob(intersect(x, subset(x, event)))!\inputencoding{utf8}.
Consequently, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
should be the entire probability space in the case that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!event!\inputencoding{utf8}
is non-null.


\section{Counting Methods\label{sec:Methods-of-Counting}}

The equally-likely model is a convenient and popular way to analyze
random experiments. And when the equally likely model applies, finding
the probability of an event $A$ amounts to nothing more than counting
the number of outcomes that $A$ contains (together with the number
of events in $S$). Hence, to be a master of probability one must
be skilled at counting outcomes in events of all kinds.
\begin{prop}
The Multiplication Principle. Suppose that an experiment is composed
of two successive steps. Further suppose that the first step may be
performed in $n_{1}$ distinct ways while the second step may be performed
in $n_{2}$ distinct ways. Then the experiment may be performed in
$n_{1}n_{2}$ distinct ways.

More generally, if the experiment is composed of $k$ successive steps
which may be performed in $n_{1}$, $n_{2}$, \ldots{}, $n_{k}$
distinct ways, respectively, then the experiment may be performed
in $n_{1}n_{2}\cdots n_{k}$ distinct ways.\end{prop}
\begin{example}
We would like to order a pizza. It will be sure to have cheese (and
marinara sauce), but we may elect to add one or more of the following
five (5) available toppings:\[
\mbox{pepperoni, sausage, anchovies, olives, and green peppers.}\]
How many distinct pizzas are possible?

There are many ways to approach the problem, but the quickest avenue
employs the Multiplication Principle directly. We will separate the
action of ordering the pizza into a series of stages. At the first
stage, we will decide whether or not to include pepperoni on the pizza
(two possibilities). At the next stage, we will decide whether or
not to include sausage on the pizza (again, two possibilities). We
will continue in this fashion until at last we will decide whether
or not to include green peppers on the pizza.

At each stage we will have had two options, or ways, to select a pizza
to be made. The Multiplication Principle says that we should multiply
the 2's to find the total number of possible pizzas: $2\cdot2\cdot2\cdot2\cdot2=2^{5}=32$.
\end{example}

\begin{example}
We would like to buy a desktop computer to study statistics. We go
to a website to build our computer our way. Given a line of products
we have many options to customize our computer. In particular, there
are 2 choices for a processor, 3 different operating systems, 4 levels
of memory, 4 hard drives of differing sizes, and 10 choices for a
monitor. How many possible types of computer must Gell be prepared
to build? \textbf{Answer:} $2\cdot3\cdot4\cdot4\cdot10=960$.
\end{example}

\subsection{Ordered Samples}

Imagine a bag with $n$ distinguishable balls inside. Now shake up
the bag and select $k$ balls at random. How many possible sequences
might we observe?
\begin{prop}
The number of ways in which one may select an ordered sample of $k$
subjects from a population that has $n$ distinguishable members is
\begin{itemize}
\item $n^{k}$ if sampling is done with replacement,
\item $n(n-1)(n-2)\cdots(n-k+1)$ if sampling is done without replacement.
\end{itemize}
\end{prop}
Recall from calculus the notation for \emph{factorials}: \begin{eqnarray*}
1! & = & 1,\\
2! & = & 2\cdot1=2,\\
3! & = & 3\cdot2\cdot1=6,\\
 & \vdots\\
n! & = & n(n-1)(n-2)\cdots3\cdot2\cdot1.\end{eqnarray*}

\begin{fact}
The number of permutations of $n$ elements is $n!$.\end{fact}
\begin{example}
Take a coin and flip it 7 times. How many sequences of Heads and Tails
are possible? \textbf{Answer:} $2^{7}=128$.
\end{example}

\begin{example}
In a class of 20 students, we randomly select a class president, a
class vice-president, and a treasurer. How many ways can this be done?
\textbf{Answer:} $20\cdot19\cdot18=6840$.
\end{example}

\begin{example}
We rent five movies to watch over the span of two nights. We wish
to watch 3 movies on the first night. How many distinct sequences
of 3 movies could we possibly watch? \textbf{Answer:} $5\cdot4\cdot3=60$.
\end{example}

\subsection{Unordered Samples}

The number of ways in which one may select an unordered sample of
$k$ subjects from a population that has $n$ distinguishable members
is
\begin{itemize}
\item $(n-1+k)!/[(n-1)!k!]$ if sampling is done with replacement,
\item $n!/[k!(n-k)!]$ if sampling is done without replacement.
\end{itemize}
The quantity $n!/[k!(n-k)!]$ is called a \emph{binomial coefficient}
and plays a special role in mathematics; it is denoted\begin{equation}
{n \choose k}=\frac{n!}{k!(n-k)!}\end{equation}
and is read {}``$n$ choose $k$''.
\begin{example}
You rent five movies to watch over the span of two nights, but only
wish to watch 3 movies the first night. Your friend, Fred, wishes
to borrow some movies to watch at his house on the first night. You
owe Fred a favor, and allow him to select 2 movies from the set of
5. How many choices does Fred have? \textbf{Answer:} ${5 \choose 2}=10$.
\end{example}

\begin{example}
Place 3 six-sided dice into a cup. Next, shake the cup well and pour
out the dice. How many distinct rolls are possible? \textbf{Answer:}
$(6-1+3)!/[(6-1)!3!]={8 \choose 5}=56$. 
\end{example}

\subsection{How to do it with \textsf{R}}

The factorial $n!$ is computed with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!factorial(n)!\inputencoding{utf8}
and the binomial coefficient ${n \choose k}$ with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!choose(n,k)!\inputencoding{utf8}.

The sample spaces we have computed so far have been relatively small,
and we can visually study them without much trouble. However, it is
\emph{very} easy to generate sample spaces that are prohibitively
large. And while \textsf{R} is wonderful and powerful and does almost
everything except wash windows, even \textsf{R} has limits of which
we should be mindful.

But we often do not need to actually generate the sample space; it
suffices to count the number of outcomes. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!nsamp!\inputencoding{utf8}
function will calculate the number of rows in a sample space made
by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
without actually devoting the memory resources necessary to generate
the space. The arguments are \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!n!\inputencoding{utf8},
the number of (distinguishable) objects in the urn, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!k!\inputencoding{utf8},
the sample size, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!replace!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8},
as above.

\begin{center}
%
\begin{table}
\begin{centering}
\begin{tabular}{l|cc}
 & \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!ordered = TRUE!\inputencoding{utf8} & \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!ordered = FALSE!\inputencoding{utf8}\tabularnewline
\hline
 &  & \tabularnewline
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!replace = TRUE!\inputencoding{utf8} & $n^{k}$ & $\frac{(n-1+k)!}{(n-1)!k!}$\tabularnewline
 &  & \tabularnewline
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!replace = FALSE!\inputencoding{utf8} & $\frac{n!}{(n-k)!}$ & ${n \choose k}$\tabularnewline
\multicolumn{1}{l}{} &  & \tabularnewline
\end{tabular}
\par\end{centering}

\caption{Sampling $k$ from $n$ objects with \texttt{urnsamples}\label{tab:Sampling-k-from-n}}

\end{table}

\par\end{center}
\begin{example}
We will compute the number of outcomes for each of the four \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
examples that we saw in Example BLANK. Recall that we took a sample
of size two from an urn with three distinguishable elements.
\end{example}
<<echo=TRUE,print=TRUE>>= 
nsamp(n=3, k=2, replace = TRUE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = FALSE) 
nsamp(n=3, k=2, replace = TRUE, ordered = FALSE) 
@ 

Compare these answers with the length of the data frames generated
above.


\subsubsection*{The Multiplication Principle}

A benefit of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!nsamp!\inputencoding{utf8}
is that it is \emph{vectorized} so that entering vectors instead of
numbers for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!n!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!k!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!replace!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8}
results in a vector of corresponding answers. This becomes particularly
convenient for combinatorics problems.
\begin{example}
There are 11 artists who each submit a portfolio containing 7 paintings
for competition in an art exhibition. Unfortunately, the gallery director
only has space in the winners' section to accomodate 12 paintings
in a row equally spread over three consecutive walls. The director
decides to give the first, second, and third place winners each a
wall to display the work of their choice. The walls boast 31 separate
lighting options apiece. How many displays are possible?

Answer: The judges will pick 3 (ranked) winners out of 11 (with \texttt{rep=FALSE},
\texttt{ord=TRUE}). Each artist will select 4 of his/her paintings
from 7 for display in a row (\texttt{rep=FALSE}, \texttt{ord=TRUE}),
and lastly, each of the 3 walls has 31 lighting possibilities (\texttt{rep=TRUE},
\texttt{ord=TRUE}). These three numbers can be calculated quickly
with 

<<echo=TRUE,print=FALSE>>= 
n <- c(11,7,31) 
k <- c(3,4,3) 
r <- c(FALSE,FALSE,TRUE) 
@ 

<<echo=TRUE,print=TRUE>>= 
x <- nsamp(n, k, rep = r, ord = TRUE) 
@ 

(Notice that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8}
is always \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!TRUE!\inputencoding{utf8};
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!nsamp!\inputencoding{utf8}
will recycle \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ordered!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!replace!\inputencoding{utf8}
to the appropriate length.) By the Multiplication Principle, the number
of ways to complete the experiment is the product of the entries of
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}: 

<<echo=TRUE,print=TRUE>>= 
prod(x) 
@ 

Compare this with the some other ways to compute the same thing: 

<<echo=TRUE,print=TRUE>>= 
(11*10*9)*(7*6*5*4)*313 
@

or alternatively 

<<echo=TRUE,print=TRUE>>= 
prod(9:11)*prod(4:7)*313 
@ 

or even 

<<echo=TRUE,print=TRUE>>= 
prod(factorial(c(11,7))/factorial(c(8,3)))*313 
@ 

\end{example}
As one can guess, in many of the standard counting problems there
aren't substantial savings in the amount of typing; it is about the
same using \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!nsamp!\inputencoding{utf8}
versus \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!factorial!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!choose!\inputencoding{utf8}.
But the virtue of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!nsamp!\inputencoding{utf8}
\texttt{l}ies in its collecting the relevant counting formulas in
a one-stop shop. Ultimately, it is up to the user to choose the method
that works best for him/herself. 
\begin{example}
\textbf{The Birthday Problem.} Suppose that there are $n$ people
together in a room. Each person announces the date of his/her birthday
in turn. The question is: what is the probability of at least one
match? If we let the event $A$ represent $\left\{ \mbox{there is at least one match}\right\} $,
then would like to know $\P(A)$, but as we will see, it is more convenient
to calculate $\P(A^{c})$.

For starters we will ignore leap years and assume that there are only
365 days in a year. Second, we will assume that births are equally
distributed over the course of a year (which is not true due to all
sorts of complications such as hospital delivery schedules). See \url{http://en.wikipedia.org/wiki/Birthday_problem}
for more.

Let us next think about the sample space. There are 365 possibilities
for the first person's birthday, 365 possibilities for the second,
and so forth. The total number of possible birthday sequences is therefore
$\#(S)=365^{n}$.

Now we will use the complementation trick we saw in Example BLANK.
We realize that the only situation in which $A$ does \emph{not} occur
is if there are \emph{no} matches among all people in the room, that
is, only when everybody's birthday is different, so\[
\P(A)=1-\P(A^{c})=1-\frac{\#(A^{c})}{\#(S)},\]
since the outcomes are equally likely. Let us then suppose that there
are no matches. The first person has one of 365 possible birthdays.
The second person must not match the first, thus, the second person
has only 364 available birthdays from which to choose. Similarly,
the third person has only 363 possible birthdays, and so forth, until
we reach the $n$$^{\text{th}}$ person, who has only $365-n+1$ remaining
possible days for a birthday. By the Multiplication Principle, we
have $\#(A^{c})=365\cdot364\cdots(365-n+1)$, and\begin{equation}
\P(A)=1-\frac{365\cdot364\cdots(365-n+1)}{365^{n}}=1-\frac{364}{365}\cdot\frac{363}{365}\cdots\frac{(365-n+1)}{365}.\end{equation}


As a surprising consequence, consider this: how many people does it
take to be in the room so that the probability of at least one match
is at least 0.50? Clearly, if there is only $n=1$ person in the room
then the probability of a match is zero, and when there are $n=366$
people in the room there is a 100\% chance of a match (recall that
we are ignoring leap years). So how many people does it take so that
there is an equal chance of a match and no match?

When I have asked this question to students, the usual response is
{}``somewhere around $n=180$ people'' in the room. The reasoning
seems to be that in order to get a 50\% chance of a match, there should
be 50\% of the available days to be occupied. The number of students
in a typical classroom is 25, so as a companion question I ask students
to estimate the probability of a match when there are $n=25$ students
in the room. Common estimates are a 1\%, or 0.5\%, or even 0.1\% chance
of a match. After they have given their estimates, we go around the
room and each student announces their birthday. More often than not,
we observe a match in the class, to the students' disbelief.

Students are usually surprised to hear that, using the formula above,
one needs only $n=23$ students to have a greater than 50\% chance
of at least one match. Figure BLANK shows a graph of the birthday
probabilities:
\end{example}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 4>>=
g <- Vectorize(pbirthday.ipsur)
plot( 1:50, g(1:50), xlab = "Number of people in room", ylab = "Prob(at least one match)")
abline(h = 0.5)
abline(v = 23, lty = 2)
remove(g)
@
\par\end{centering}

\caption{The Birthday Problem: the horizontal line is at $p=0.50$ and the
vertical line is at $n=23$.\label{fig:The-Birthday-Problem}}

\end{figure}





\subsection{How to do it with \textsf{R}}

\inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily},breaklines=true,frame=leftline,numbers=left,showstringspaces=false,tabsize=2]
	g <- Vectorize(pbirthday)	# vectorize pbirthday function
	plot( 1:50, g(1:50),
			xlab = "Number of people in room",
			ylab = "Prob(at least one match)",
			main = "The Birthday Problem")
	abline(h = 0.5)
	abline(v = 23, lty = 2)	# dashed line
\end{lstlisting}
\inputencoding{utf8}

There is a \textsf{Birthday problem} item in the \textsf{Probability}
menu of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrPlugin.IPSUR!\inputencoding{utf8}.

In the base \textsf{R} version, one can compute approximate probabilities
for the more general case of probabilities other than 1/2, for differing
total number of days in the year, and even for more than two matches.


\section{Conditional Probability\label{sec:Conditional-Probability}}

Consider a full deck of 52 standard playing cards. Now select two
cards from the deck, in succession. Let $A=\left\{ \mbox{first card drawn is an Ace}\right\} $
and $B=\left\{ \mbox{second card drawn is an Ace}\right\} $. Since
there are four Aces in the deck, it is natural to assign $\P(A)=4/52$.
Suppose we look at the first card. What now is the probability of
$B$? Of course, the answer depends on the value of the first card.
If the first card is an Ace, then the probability that the second
also is an Ace should be $3/51$, but if the first card is not an
Ace, then the probability that the second is an Ace should be $4/51$.
As notation for these two situations we write\[
\P(B|A)=3/51,\quad\P(B|A^{c})=4/51.\]

\begin{defn}
The conditional probability of $B$ given $A$, denoted $\P(B|A)$,
is defined by\begin{equation}
\P(B|A)=\frac{\P(A\cap B)}{\P(A)},\quad\mbox{if }\P(A)>0.\end{equation}
We will not be discussing a conditional probability of $B$ given
$A$ when $\P(A)=0$, even though this theory exists, is well developed,
and forms the foundation for the study of stochastic processes%
\footnote{Conditional probability in this case is defined by means of \emph{conditional
expectation}, a topic that is well beyond the scope of this text.
The interested reader should consult an advanced text on probability
theory, such as Billingsley, Resnick, or Ash Dooleans-Dade.%
}. \end{defn}
\begin{example}
Toss a coin twice. The sample space is given by $S=\left\{ HH,\ HT,\ TH,\ TT\right\} $.
Let $A=\left\{ \mbox{a head occurs}\right\} $ and $B=\left\{ \mbox{a head and tail occur}\right\} $.
It should be clear that $\P(A)=3/4$, $\P(B)=2/4$, and $\P(A\cap B)=2/4$.
What now are the probabilities $\P(A|B)$ and $\P(B|A)$?\[
\P(A|B)=\frac{\P(A\cap B)}{\P(B)}=\frac{2/4}{2/4}=1,\]
in other words, once we know that a Head and Tail occur, we may be
certain that a Head occurs. Next\[
\P(B|A)=\frac{\P(A\cap B)}{\P(A)}=\frac{2/4}{3/4}=\frac{2}{3},\]
which means that given the information that a Head has occurred, we
no longer need to account for the outcome $TT$, and the remaining
three outcomes are equally likely with exactly two outcomes lying
in the set $B$. 
\end{example}

\begin{example}
Toss a six-sided die twice. The sample space consists of all ordered
pairs $(i,j)$ of the numbers $1,2,\ldots,6$, that is, $S=\left\{ (1,1),\ (1,2),\ldots,(6,6)\right\} $.
We know from Section \ref{sec:Methods-of-Counting} that $\#(S)=6^{2}=36$.
Let $A=\left\{ \mbox{outcomes match}\right\} $ and $B=\left\{ \mbox{sum of outcomes at least 8}\right\} $.
The sample space may be represented by a matrix:

%
\begin{table}
\begin{centering}
\begin{tabular}{c}
\begin{sideways}
First Roll%
\end{sideways}\tabularnewline
\end{tabular}\begin{tabular}{c|cccccc|}
\multicolumn{1}{c}{} & \multicolumn{6}{c}{Second Roll}\tabularnewline
\multicolumn{1}{c}{} & 1 & 2 & 3 & 4 & 5 & \multicolumn{1}{c}{6}\tabularnewline
\cline{2-7} 
1 & $\varprod$ &  &  &  &  & \tabularnewline
2 &  & $\varprod$ &  &  &  & {\Large $\bigcirc$}\tabularnewline
3 &  &  & $\varprod$ &  & {\Large $\bigcirc$} & {\Large $\bigcirc$}\tabularnewline
4 &  &  &  & {\huge $\otimes$} & {\Large $\bigcirc$} & {\Large $\bigcirc$}\tabularnewline
5 &  &  & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\huge $\otimes$} & {\Large $\bigcirc$}\tabularnewline
6 &  & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\huge $\otimes$}\tabularnewline
\cline{2-7} 
\end{tabular}
\par\end{centering}

\caption{Rolling two dice\label{tab:Rolling-two-dice}}

\end{table}


The outcomes lying in the event $A$ are marked with the symbol {}``$\varprod$'',
the outcomes falling in $B$ are marked with {}``$\bigcirc$'',
and those in both $A$ and $B$ are marked {}``$\otimes$''. Now
it is clear that $\P(A)=6/36$, $\P(B)=15/36$, and $\P(A\cap B)=3/36$.
Finally, \[
\P(A|B)=\frac{3/36}{15/36}=\frac{1}{5},\quad\P(B|A)=\frac{3/36}{6/36}=\frac{1}{2}.\]
Again, we see that given the knowledge that $B$ occurred (the 15
outcomes in the lower right triangle), there are 3 of the 15 that
fall into the set $A$, thus the probability is $3/15$. Similarly,
given that $A$ occurred (we are on the diagonal), there are 3 out
of 6 outcomes that also fall in $B$, thus, the probability of $B$
given $A$ is 1/2. 
\end{example}

\subsection{How to do it with \textsf{R}}

Continuing with Example BLANK, the first thing to do is set up the
probability space with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie!\inputencoding{utf8}
function.

<<keep.source = TRUE>>=
library(prob)
S <- rolldie(2, makespace = TRUE)  # assumes equally likely model
head(S)                           #  first few rows
@

Next we define the events

<<keep.source = TRUE>>=
A <- subset(S, X1 == X2)
B <- subset(S, X1 + X2 >= 8)
@

And now we are ready to calculate probabilities. To do conditional
probability, we use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!given!\inputencoding{utf8}
argument of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
function:

<<keep.source = TRUE>>=
prob(A, given = B)
prob(B, given = A)
@

Note that we do not actually need to define the events $A$ and $B$
separately as long as we reference the original probability space
$S$ as the first argument of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
calculation:

<<keep.source = TRUE>>=
prob(S, X1==X2, given = (X1 + X2 >= 8) )
prob(S, X1+X2 >= 8, given = (X1==X2) )
@


\subsection{Properties and Rules}

The following theorem establishes that conditional probabilities behave
just like regular probabilities when the conditioned event is fixed. 
\begin{thm}
For any fixed event $A$ with $\P(A)>0$,
\begin{enumerate}
\item $\P(B|A)\geq0,$ for all events $B\subset S$,
\item $\P(S|A)=1$, and
\item If $B_{1}$, $B_{2}$, $B_{3}$,\ldots{} are disjoint events, then\begin{equation}
\P\left(\left.\bigcup_{k=1}^{\infty}B_{k}\:\right|A\right)=\sum_{k=1}^{\infty}\P(B_{k}|A).\end{equation}

\end{enumerate}
\end{thm}
In other words, $\P(\cdot|A)$ is a legitimate probability function.
With this fact in mind, the following properties are immediate:
\begin{prop}
For any events $A$, $B$, and $C$ with $\P(A)>0$,
\begin{enumerate}
\item $\P(B^{c}|A)=1-\P(B|A).$
\item If $B\subset C$ then $\P(B|A)\leq\P(C|A)$.
\item $\P[(B\cup C)|A]=\P(B|A)+\P(C|A)-\P[(B\cap C|A)].$$ $
\item \textbf{The Multiplication Rule.} For any two events $A$ and $B$,\begin{equation}
\P(A\cap B)=\P(A)\P(B|A).\end{equation}
And more generally, for events $A_{1}$, $A_{2}$, $A_{3}$,\ldots{},
$A_{n}$,\begin{equation}
\P(A_{1}\cap A_{2}\cap\cdots\cap A_{n})=\P(A_{1})\P(A_{2}|A_{1})\cdots\P(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).\end{equation}

\end{enumerate}
\end{prop}


The Multiplication Rule is very important because it allows us to
find probabilities in random experiments that have a sequential structure,
as the next example shows.
\begin{example}
In Example BLANK we drew two cards from a standard playing deck. Now
we may answer, what is $\P(\mbox{both Aces})$?\[
\P(\mbox{both Aces})=\P(A\cap B)=\P(A)\P(B|A)=\frac{4}{52}\cdot\frac{3}{51}\approx0.00452.\]

\end{example}

\subsection{How to do it with \textsf{R}}

Continuing Example BLANK, we set up the probability space by way of
a three step process. First we employ the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cards!\inputencoding{utf8}
function to get a data frame \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!L!\inputencoding{utf8}
with two columns: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rank!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!suit!\inputencoding{utf8}.
Both columns are stored internally as factors with 13 and 4 levels,
respectively.

Next we sample two cards randomly from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!L!\inputencoding{utf8}
data frame by way of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
function. It returns a list \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!M!\inputencoding{utf8}
which contains all possible pairs of rows from \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!L!\inputencoding{utf8}
(there are \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!choose(52,2)!\inputencoding{utf8}
of them). The sample space for this experiment is exactly the list
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!M!\inputencoding{utf8}.

At long last we associate a probability model with the sample space.
This is right down the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probspace!\inputencoding{utf8}
function's alley. It assumes the equally likely model by default.
We call this result \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!N!\inputencoding{utf8}
which is an object of class \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ps!\inputencoding{utf8}
-- short for {}``probability space''. 

But do not be intimidated. The object \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!N!\inputencoding{utf8}
is nothing more than a list with two elements: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!outcomes!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}.
The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!outcomes!\inputencoding{utf8}
element is itself just another list, with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!choose(52,2)!\inputencoding{utf8}
entries, each one a data frame with two rows which correspond to the
pair of cards chosen. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
element is just a vector with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!choose(52,2)!\inputencoding{utf8}
entries all the same: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!1/choose(52,2)!\inputencoding{utf8}.

Putting all of this together we do

<<>>=
library(prob)
L <- cards()
M <- urnsamples(L, size = 2)
N <- probspace(M)
@

Now that we have the probability space \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!N!\inputencoding{utf8}
we are ready to do some probability. We use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
function, just like before. The only trick is to specify the event
of interest correctly, and recall that we were interested in $\P(\mbox{both Aces})$.
But if the cards are both Aces then the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rank!\inputencoding{utf8}
of both cards should be \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"A"!\inputencoding{utf8},
which sounds like a job for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!all!\inputencoding{utf8}
function:

<<>>=
prob(N, all(rank == "A"))
@

Note that this value matches what we found in Example BLANK, above.
We could calculate all sorts of probabilities at this point; we are
limited only by the complexity of the event's computer representation. 


\begin{example}
Consider an urn with 10 balls inside, 7 of which are red and 3 of
which are green. Select 3 balls successively from the urn. Let $A=\left\{ 1^{\text{st}}\mbox{ ball is red}\right\} $,
$B=\left\{ 2^{\text{nd}}\mbox{ ball is red}\right\} $, and $C=\left\{ 3^{\text{rd}}\mbox{ ball is red}\right\} $.
Then\[
\P(\mbox{all 3 balls are red})=\P(A\cap B\cap C)=\frac{7}{10}\cdot\frac{6}{9}\cdot\frac{5}{8}\approx0.2917.\]

\end{example}

\subsection{How to do it with \textsf{R}}

Example BLANK is similar to Example BLANK, but it is even easier.
We need to set up an urn (vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!L!\inputencoding{utf8})
to hold the balls, we sample from \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!L!\inputencoding{utf8}
to get the sample space (data frame \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!M!\inputencoding{utf8}),
and we associate a probability vector (column \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8})
with the outcomes (rows of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!M!\inputencoding{utf8})
of the sample space. The final result is a probability space (an ordinary
data frame \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!N!\inputencoding{utf8}).

It is easier for us this time because our urn is a vector instead
of a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cards()!\inputencoding{utf8}
data frame. Before there were two dimensions of information associated
with the outcomes (rank and suit) but presently we have only one dimension
(color).

<<>>=
library(prob)
L <- rep(c("red","green"), times = c(7,3))
M <- urnsamples(L, size = 3, replace = FALSE, ordered = TRUE)
N <- probspace(M)
@

Now let us think about how to set up the event $\left\{ \mbox{all 3 balls are red}\right\} $.
Rows of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!N!\inputencoding{utf8}
that satisfy this condition have \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!X1=="red" & X2=="red" & X3=="red"!\inputencoding{utf8},
but there must be an easier way. Indeed, there is. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!isrep!\inputencoding{utf8}
function (short for {}``is repeated'') in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package was written for this purpose. The command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!isrep(N,"red",3)!\inputencoding{utf8}
will test each row of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!N!\inputencoding{utf8}
to see whether the value \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8}
appears \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!3!\inputencoding{utf8}
times. The result is exactly what we need to define an event with
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
function. Observe

<<>>=
prob(N, isrep(N, "red", 3))
@

Note that this answer matches what we found in Example BLANK. Now
let us try some other probability questions. What is the probability
of getting two \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8}s?

<<>>=
prob(N, isrep(N, "red", 2))
@

Note that the exact value is $21/40$; we will learn a quick way to
compute this in Section BLANK. What is the probability of observing
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8},
then \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"green"!\inputencoding{utf8},
then \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8}?

<<>>=
prob(N, isin(N, c("red","green","red"), ordered = TRUE))
@

Note that the exact value is $7/20$ (do it with the Multiplication
Rule). What is the probability of observing \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"green"!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8},
in no particular order?

<<>>=
prob(N, isin(N, c("red","green","red")))
@

We already knew this. It is the probability of observing two \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!"red"!\inputencoding{utf8}s,
above.


\begin{example}
Consider two urns, the first with 5 red balls and 3 green balls, and
the second with 2 red balls and 6 green balls. Your friend randomly
selects one ball from the first urn and transfers it to the second
urn, without disclosing the color of the ball. You select one ball
from the second urn. What is the probability that the selected ball
is red? Let $A=\left\{ \mbox{transferred ball is red}\right\} $ and
$B=\left\{ \mbox{selected ball is red}\right\} $. Write\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)\end{align*}
and notice that $A\cap B$ and $A^{c}\cap B$ are disjoint. Therefore\begin{align*}
\P(B) & =\P(A\cap B)+\P(A^{c}\cap B)\\
 & =\P(A)\P(B|A)+\P(A^{c})\P(B|A^{c})\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9}\\
 & =\frac{21}{72}\ \end{align*}
(which is 7/24 in lowest terms).
\end{example}

\begin{example}
We saw the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrTestDrive!\inputencoding{utf8}
data set in Chapter BLANK in which a two-way table of the smoking
status versus the gender was

<<echo = FALSE>>=
.Table <- xtabs(~smoke+gender, data=RcmdrTestDrive)
addmargins(.Table) # Table with Marginal Distributions
remove(.Table)
@

If one person were selected at random from the data set, then we see
from the two-way table that $\P(\mbox{Female})=70/168$ and $\P(\mbox{Smoker})=32/168$.
Now suppose that one of the subjects quits smoking, but we do not
know the person's gender. If we select one subject at random, what
now is $\P(\mbox{Female})$? Let $A=\left\{ \mbox{the quitter is a female}\right\} $
and $B=\left\{ \mbox{selected person is a female}\right\} $. Write\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)\end{align*}
and notice that $A\cap B$ and $A^{c}\cap B$ are disjoint. Therefore\begin{align*}
\P(B) & =\P(A\cap B)+\P(A^{c}\cap B)\\
 & =\P(A)\P(B|A)+\P(A^{c})\P(B|A^{c})\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9}\\
 & =\frac{21}{72}\ \end{align*}
(which is 7/24 in lowest terms).

\end{example}
Using the same reasoning, one can return to Example BLANK and show
that $\P(\left\{ \mbox{second card is an Ace}\right\} )=4/52$. 


\section{Independent Events\label{sec:Independent-Events}}

Toss a coin twice. The sample space is $S=\left\{ HH,\ HT,\ TH,\ TT\right\} $.
We know that $\P(1^{\text{st}}\mbox{ toss is }H)=2/4$, $\P(2^{\text{nd}}\mbox{ toss is }H)=2/4$,
and $\P(\mbox{both }H)=1/4$. Then\begin{align*}
\P(2^{\text{nd}}\mbox{ toss is }H\ |\ 1^{\text{st}}\mbox{ toss is }H) & =\frac{\P(\mbox{both }H)}{\P(1^{\text{st}}\mbox{ toss is }H)}\\
 & =\frac{1/4}{2/4}\\
 & =\P(2^{\text{nd}}\mbox{ toss is }H).\end{align*}
Intuitively, this means that the information that the first toss is
$H$ has no bearing on the probability that the second toss is $H$.
The coin does not remember the result of the first toss. 
\begin{defn}
Events $A$ and $B$ are said to be \emph{independent} if \begin{equation}
\P(A\cap B)=\P(A)\P(B).\end{equation}
Otherwise, the events are said to be \emph{dependent}. 
\end{defn}
The connection with the above example stems from the following. We
know from Section BLANK that when $\P(B)>0$ we may write \[
\P(A|B)=\frac{\P(A\cap B)}{\P(B)}.\]
In the case that $A$ and $B$ are independent, the numerator of the
fraction factors so that $\P(B)$ cancels with the result:\[
\P(A|B)=\P(A)\quad\mbox{when \mbox{\emph{A},\emph{ B}} are independent.}\]
The interpretation in the case of independence is that the information
that the event $B$ occurred does not influence the probability of
the event $A$ occurring. Similarly, $\P(B|A)=\P(B)$, and so the
occurrence of the event $A$ likewise does not affect the probability
of event $B$. It may seem more natural to define $A$ and $B$ to
be independent when $\P(A|B)=\P(A)$; however, the conditional probability
$\P(A|B)$ is only defined when $\P(B)>0$. Our definition is not
limited by this restriction. It can be shown that when $\P(A),\ \P(B)>0$
the two notions of independence are equivalent.
\begin{prop}
If the events $A$ and $B$ are independent then
\begin{itemize}
\item $A$ and $B^{c}$ are independent,
\item $A^{c}$ and $B$ are independent,
\item $A^{c}$ and $B^{c}$ are independent.
\end{itemize}
\end{prop}
\begin{proof}
Suppose that $A$ and $B$ are independent. We will show the second
one; the others are similar. We need to show that\[
\P(A^{c}\cap B)=\P(A^{c})\P(B).\]
To this end, note that the Multiplication Rule BLANK implies \begin{eqnarray*}
\P(A^{c}\cap B) & = & \P(B)\P(A^{c}|B),\\
 & = & \P(B)[1-\P(A|B)],\\
 & = & \P(B)\P(A^{c}).\end{eqnarray*}
\end{proof}
\begin{defn}
The events $A$, $B$, and $C$ are \emph{mutually independent} if
the following four conditions are met: \begin{eqnarray*}
\P(A\cap B) & = & \P(A)\P(B),\\
\P(A\cap C) & = & \P(A)\P(C),\\
\P(B\cap C) & = & \P(B)\P(C),\end{eqnarray*}
and\[
\P(A\cap B\cap C)=\P(A)\P(B)\P(C).\]
If only the first three conditions hold then $A$, $B$, and $C$
are said to be independent \emph{pairwise}. Note that pairwise independence
is not the same as mutual independence when the number of events is
larger than two.
\end{defn}
We can now deduce the pattern for $n$ events, $n>3$. The events
will be mutually independent only if they satisfy the product equality
pairwise, then in groups of three, in groups of four, and so forth,
up to all $n$ events at once. For $n$ events, there will be $2^{n}-n-1$
equations that must be satisfied (see Exercise BLANK). Although these
requirements for a set of events to be mutually independent may seem
stringent, the good news is that for most of the situations considered
in this book the conditions will all be met (or at least we will suppose
that they are).
\begin{example}
Toss ten coins. What is the probability of observing at least one
Head? Answer: Let $A_{i}=\left\{ \mbox{the }i^{\text{th}}\mbox{ coin shows }H\right\} ,\ i=1,2,\ldots,10$.
Supposing that we toss the coins in such a way that they do not interfere
with each other, this is one of the situations where all of the $A_{i}$
may be considered mutually independent due to the nature of the tossing.
Of course, the only way that there will not be at least one Head showing
is if all tosses are Tails. Therefore,\begin{align*}
\P(\mbox{at least one }H) & =1-\P(\mbox{all }T),\\
 & =1-\P(A_{1}^{c}\cap A_{2}^{c}\cap\cdots\cap A_{10}^{c}),\\
 & =1-\P(A_{1}^{c})\P(A_{2}^{c})\cdots\P(A_{10}^{c}),\\
 & =1-\left(\frac{1}{2}\right)^{10},\end{align*}
which is approximately $0.9990234$.
\end{example}

\subsection{How to do it with \textsf{R}}
\begin{example}
Toss ten coins. What is the probability of observing at least one
Head?

<<>>=
S <- tosscoin(10, makespace = TRUE)
A <- subset(S, isrep(S, vals = "T", nrep = 10))
1 - prob(A)
@

Compare this answer to what we got in Example BLANK.

\end{example}

\subsection*{Independent, Repeated Experiments}

Generalizing from above it is common to repeat a certain experiment
multiple times under identical conditions and in an independent manner.
We have seen many examples of this already: tossing a coin repeatedly,
rolling a die or dice, \textit{etc}.

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!iidspace!\inputencoding{utf8}
function was designed specifically for this situation. It has three
arguments: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
which is a vector of outcomes, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ntrials!\inputencoding{utf8},
which is an integer telling how many times to repeat the experiment,
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
to specify the probabilities of the outcomes of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
in a single trial. 


\begin{example}
An unbalanced coin (continued, see Example BLANK). It was easy enough
to set up the probability space for one unbalanced toss, however,
the situation becomes more complicated when there are many tosses
involved. Clearly, the outcome $HHH$ should not have the same probability
as $TTT$, which should again not have the same probability as $HTH$.
At the same time, there is symmetry in the experiment in that the
coin does not remember the face it shows from toss to toss, and it
is easy enough to toss the coin in a similar way repeatedly.

We may represent tossing our unbalanced coin three times with the
following: 

<<echo=TRUE,print=TRUE>>= 
iidspace(c("H","T"), ntrials = 3, probs = c(0.7, 0.3)) 
@ 

As expected, the outcome $HHH$ has the largest probability, while
$TTT$ has the smallest. (Since the trials are independent, $\P(HHH)=0.7^{3}$
and $\P(TTT)=0.3^{3}$, \textit{etc}.) Note that the result of the
function call is a probability space, not a sample space (which we
could construct already with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!tosscoin!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!urnsamples!\inputencoding{utf8}
functions). The same procedure could be used to model an unbalanced
die or any other experiment that may be represented with a vector
of possible outcomes.

\end{example}
Note that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!iidspace!\inputencoding{utf8}
will assume \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
has equally likely outcomes if no \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
argument is specified. Also note that the argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
is a \emph{vector}, not a data frame. Something like \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!iidspace(tosscoin(1),...)!\inputencoding{utf8}
would give an error.


\section{Bayes' Rule\label{sec:Bayes'-Rule}}

We mentioned the subjective view of probability in Section BLANK.
In this section we introduce a rule that allows us to update our probabilities
when new information becomes available. 
\begin{thm}
\textbf{\emph{(Bayes' Rule).}} Let $B_{1}$, $B_{2}$, \ldots{},
$B_{n}$ be mutually exclusive and exhaustive and let $A$ be an event
with $\P(A)>0$. Then \begin{equation}
\P(B_{k}|A)=\frac{\P(B_{k})\P(A|B_{k})}{\sum_{i=1}^{n}\P(B_{i})\P(A|B_{i})},\quad k=1,2,\ldots,n.\end{equation}
\end{thm}
\begin{proof}
The proof follows from looking at $\P(B_{k}\cap A)$ in two different
ways. For simplicity, suppose that $P(B_{k})>0$ for all $k$. Then\[
\P(A)\P(B_{k}|A)=\P(B_{k}\cap A)=\P(B_{k})\P(A|B_{k}).\]
Since $\P(A)>0$ we may divide through to obtain \[
\P(B_{k}|A)=\frac{\P(B_{k})\P(A|B_{k})}{\P(A)}.\]
Now remembering that $\left\{ B_{k}\right\} $ is a partition, the
Theorem of Total Probability BLANK gives the denominator of the last
expression to be\[
\P(A)=\sum_{k=1}^{n}\P(B_{k}\cap A)=\sum_{k=1}^{n}\P(B_{k})\P(A|B_{k}).\]

\end{proof}
What does it mean? Usually in applications we are given (or know)
\emph{a priori} probabilities $\P(B_{k})$. We go out and collect
some data, which we represent by the event $A$. We want to know:
how do we \textbf{update} $\P(B_{k})$ to $\P(B_{k}|A)$? The answer:
Bayes' Rule.
\begin{example}
\emph{Misfiling Assistants}. In this problem, there are three assistants
working at a company: Moe, Larry, and Curly. Their primary job duty
is to file paperwork in the filing cabinet when papers become available.
The three assistants have different work schedules:

\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Workload & 60\% & 30\% & 10\%\tabularnewline
\end{tabular}

That is, Moe works 60\% of the time, Larry works 30\% of the time,
and Curly does the remaining 10\%, and they file documents at approximately
the same speed. Suppose a person were to select one of the documents
from the cabinet at random. Let $M$ be the event\[
M=\left\{ \mbox{Moe filed the document}\right\} \]
and let $L$ and $C$ be the events that Larry and Curly, respectively,
filed the document. What are these events' respective probabilities?
In the absence of additional information, reasonable prior probabilities
would just be

\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Prior Probability & $\P(M)=0.60$ & $\P(L)=0.30$ & $\P(C)=0.10$\tabularnewline
\end{tabular}

Now, the boss comes in one day, opens up the file cabinet, and selects
a file at random. The boss discovers that the file has been misplaced.
The boss is so angry at the mistake that (s)he threatens to fire the
one who erred. The question is: who misplaced the file?

The boss decides to use probability to decide, and walks straight
to the workload schedule. (S)he reasons that, since the three employees
work at the same speed, the probability that a randomly selected file
would have been filed by each one would be proportional to his workload.
The boss notifies \textbf{Moe} that he has until the end of the day
to empty his desk.

But Moe argues in his defense that the boss has ignored additional
information. Moe's likelihood of having misfiled a document is smaller
than Larry's and Curly's, since he is a diligent worker who pays close
attention to his work. Moe admits that he works longer than the others,
but he doesn't make as many mistakes as they do. Thus, Moe recommends
that - before making a decision - the boss should update the probability
(initially based on workload alone) to incorporate the likelihood
of having observed a misfiled document.

And, as it turns out, the boss has information about Moe, Larry, and
Curly's filing accuracy in the past (due to historical performance
evaluations). The performance information may be represented by the
following table:

%
\begin{table}[H]
\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Misfile Rate & 0.003 & 0.007 & 0.010\tabularnewline
\end{tabular}
\end{table}


In other words, on the average, Moe misfiles 0.3\% of the documents
he is supposed to file. Notice that Moe was correct: he is the most
accurate filer, followed by Larry, and lastly Curly. If the boss were
to make a decision based only on the worker's overall accuracy, then
\textbf{Curly} should get the axe. But Curly hears this and interjects
that he only works a short period during the day, and consequently
makes mistakes only very rarely; there is only the tiniest chance
that he misfiled this particular document.

The boss would like to use this updated information to update the
probabilities for the three assistants, that is, (s)he wants to use
the additional likelihood that the document was misfiled to update
his/her beliefs about the likely culprit. Let $A$ be the event that
a document is misfiled. What the boss would like to know are the three
probabilities \[
\P(M|A),\mbox{ }\P(L|A),\mbox{ and }\P(C|A).\]
We will show the calculation for $\P(M|A)$, the other two cases being
similar. We use Bayes' Rule in the form\[
\P(M|A)=\frac{\P(M\cap A)}{\P(A)}.\]
Let's try to find $\P(M\cap A)$, which is just $\P(M)\cdot\P(A|M)$
by the Multiplication Rule. We already know $\P(M)=0.6$ and $\P(A|M)$
is nothing more than Moe's misfile rate, given above to be $\P(A|M)=0.003$.
Thus, we compute\[
\P(M\cap A)=(0.6)(0.003)=0.0018.\]
Using the same procedure we may calculate\[
\P(L|A)=0.0021\mbox{ and }\P(C|A)=0.0010.\]
Now let's find the denominator, $\P(A)$. The key here is the notion
that if a file is misplaced, then either Moe or Larry or Curly must
have filed it; there is no one else around to do the misfiling. Further,
these possibilities are mutually exclusive. We may use the Theorem
of Total Probability BLANK to write\[
\P(A)=\P(A\cap M)+\P(A\cap L)+\P(A\cap C).\]
Luckily, we have computed these above. Thus\[
\P(A)=0.0018+0.0021+0.0010=0.0049.\]
Therefore, Bayes' Rule yields\[
\P(M|A)=\frac{0.0018}{0.0049}\approx0.37.\]
This last quantity is called the posterior probability that Moe misfiled
the document, since it incorporates the observed data that a randomly
selected file was misplaced (which is governed by the misfile rate).
We can use the same argument to calculate

%
\begin{table}[H]
\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Posterior Probability & $\P(M|A)\approx0.37$ & $\P(L|A)\approx0.43$ & $\P(C|A)\approx0.20$\tabularnewline
\end{tabular}
\end{table}


The conclusion: \textbf{Larry} gets the axe. What is happening is
an intricate interplay between the time on the job and the misfile
rate. It is not obvious who the winner (or in this case, loser) will
be, and the statistician needs to consult Bayes' Rule to determine
the best course of action.
\end{example}

\begin{example}
Suppose the boss gets a change of heart and does not fire anybody.
But the next day (s)he randomly selects another file and again finds
it to be misplaced. To decide whom to fire now, the boss would use
the same procedure, with one small change. (S)he would not use the
prior probabilities $ $60\%, 30\%, and 10\%; those are old news.
Instead, she would replace the prior probabilities with the posterior
probabilities just calculated. After the math she will have new posterior
probabilities, updated even more from the day before.

In this way, probabilities found by Bayes' rule are always on the
cutting edge, always updated with respect to the best information
available at the time.
\end{example}

\subsection{How to do it with \textsf{R}}

There are not any special functions for Bayes' Rule in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prob!\inputencoding{utf8}
package, but problems like the ones above are easy enough to do by
hand.
\begin{example}
Misfiling assistants (continued, see Example BLANK). We store the
prior probabilities and the likelihoods in vectors and go to town.

<<>>=
prior <- c(0.6, 0.3, 0.1)
like <- c(0.003, 0.007, 0.010)
post <- prior * like
post / sum(post)
@

\end{example}


Compare these answers with what we got in Example BLANK. We would
replace \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prior!\inputencoding{utf8}
with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!post!\inputencoding{utf8}
in a future calculation. We could raise \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!like!\inputencoding{utf8}
to a power to see how the posterior is affected by future document
mistakes. (Do you see why? Think back to Section BLANK.)


\begin{example}
Let us incorporate the posterior probability (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!post!\inputencoding{utf8})
information from the last example and suppose that the assistants
misfile seven more documents. Using Bayes' Rule, what would the new
posterior probabilities be?

<<>>=
newprior <- post
post <- newprior * like^7
post / sum(post)
@

We see that the individual with the highest probability of having
misfiled all eight documents given the observed data is no longer
Larry, but Curly. 
\end{example}


There are two important points. First, we did not divide \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!post!\inputencoding{utf8}
by the sum of its entries until the very last step; we do not need
to calculate it, and it will save us computing time to postpone normalization
until absolutely necessary, namely, until we finally want to interpret
them as probabilities.

Second, the reader might be wondering what the boss would get if (s)he
skipped the intermediate step of calculating the posterior after only
one misfiled document. What if she started from the \emph{original}
prior, then observed eight misfiled documents, and calculated the
posterior? What would she get? It must be the same answer, of course.

<<>>=
fastpost <- prior * like^8
fastpost / sum(fastpost)
@

Compare this to what we got in Example BLANK.




\section{Random Variables\label{sec:Random-Variables}}

We already know about experiments, sample spaces, and events. In this
section, we are interested in a \emph{number} that is associated with
the experiment. We conduct a random experiment $E$ and after learning
the outcome $\omega$ in $S$ we calculate a number $X$. That is,
to each outcome $\omega$ in the sample space we associate a number
$X(\omega)=x$. 
\begin{defn}
A \emph{random variable} $X$ is a function $X:S\to\R$ that associates
to each outcome $\omega\in S$ exactly one number $X(\omega)=x$. 
\end{defn}
We usually denote random variables by uppercase letters such as $X$,
$Y$, and $Z$, and we denote their observed values by lowercase letters
$x$, $y$, and $z$. Just as $S$ is the set of all possible outcomes
of $E$, we call the set of all possible values of $X$ the \emph{support}
of $X$ and denote it by $S_{X}$.
\begin{example}
Let $E$ be the experiment of flipping a coin twice. We have seen
that the sample space is $S=\left\{ HH,\ HT,\ TH,\ TT\right\} $.
Now define the random variable $X=\mbox{the number of heads}$. That
is, for example, $X(HH)=2$, while $X(HT)=1$. We may make a table
of the possibilities:

%
\begin{table}[H]
\begin{tabular}{c|cccc}
$\omega\in S$ & $HH$ & $HT$ & $TH$ & $TT$\tabularnewline
\hline
$X(\omega)=x$ & 2 & 1 & 1 & 0\tabularnewline
\end{tabular}
\end{table}


Taking a look at the second row of the table, we see that the support
of $X$ -- the set of all numbers that $X$ assumes -- would be $S_{X}=\left\{ 0,1,2\right\} $.
\end{example}

\begin{example}
Let $E$ be the experiment of flipping a coin repeatedly until observing
a Head. The sample space would be $S=\left\{ H,\ TH,\ TTH,\ TTTH,\ \ldots\right\} $.
Now define the random variable $Y=\mbox{the number of Tails before the first head}$.
Then the support of $Y$ would be $S_{Y}=\left\{ 0,1,2,\ldots\right\} $.
\end{example}

\begin{example}
Let $E$ be the experiment of tossing a coin in the air, and define
the random variable $Z=\mbox{the time (in seconds) until the coin hits the ground}$.
In this case, the sample space is inconvenient to describe. Yet the
support of $Z$ would be $(0,\infty)$. Of course, it is reasonable
to suppose that the coin will return to Earth in a short amount of
time; in practice, the set $(0,\infty)$ is admittedly too large.
However, we will soon see that in similar situations it is mathematically
convenient to study the extended set rather than restrict it to a
smaller one.
\end{example}
There are important differences between the supports of $X$, $Y$,
and $Z$. The support of $X$ is a finite collection of elements that
can be inspected all at once. And while the support of $Y$ cannot
be exhaustively written down, nevertheless its elements can be listed
in a naturally ordered sequence. Random variables with supports similar
to those of $X$ and $Y$ are called \emph{discrete random variables}.
We study these in Chapter BLANK.

In contrast, the support of $Z$ is a continuous interval, containing
all rational and irrational positive real numbers. For this reason%
\footnote{This isn't really the reason, but it serves as an effective litmus
test at the introductory level. See Billingsley or Resnick.%
}, random variables with supports like $Z$ are called \emph{continuous
random variables}, to be studied in Chapter BLANK.


\subsection{How to do it with \textsf{R}}

The primary vessel for this task is the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!addrv!\inputencoding{utf8}
function. There are two ways to use it, and we will describe both.


\subsubsection*{Supply a Defining Formula}

The first method is based on the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!transform!\inputencoding{utf8}
function. See \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!?transform!\inputencoding{utf8}.
The idea is to write a formula defining the random variable inside
the function, and it will be added as a column to the data frame.
As an example, let us roll a 4-sided die three times, and let us define
the random variable $U=X1-X2+X3$. 

<<echo=TRUE,print=FALSE>>= 
S <- rolldie(3, nsides = 4, makespace = TRUE) 
S <- addrv(S, U = X1-X2+X3) 
@ 

Now let's take a look at the values of $U$. In the interest of space,
we will only reproduce the first few rows of $S$ (there are $4^{3}=64$
rows in total). 

<<echo=TRUE,print=TRUE>>= 
head(S)
@ 

We see from the $U$ column it is operating just like it should. We
can now answer questions like

<<echo=TRUE,print=TRUE>>= 
prob(S, U > 6) 
@ 


\subsubsection*{Supply a Function}

Sometimes we have a function laying around that we would like to apply
to some of the outcome variables, but it is unfortunately tedious
to write out the formula defining what the new variable would be.
The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!addrv!\inputencoding{utf8}
function has an argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!FUN!\inputencoding{utf8}
specifically for this case. Its value should be a legitimate function
from \textsf{R}, such as \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sum!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!median!\inputencoding{utf8},
\emph{etc}. Or, you can define your own function. Continuing the previous
example, let's define $V=\max(X1,X2,X3)$ and $W=X1+X2+X3$. 

<<echo=TRUE,print=FALSE>>= 
S <- addrv(S, FUN = max, invars = c("X1","X2","X3"), name = "V") 
S <- addrv(S, FUN = sum, invars = c("X1","X2","X3"), name = "W") 
head(S) 
@ 

Notice that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!addrv!\inputencoding{utf8}
has an \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!invars!\inputencoding{utf8}
argument to specify exactly to which columns one would like to apply
the function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!FUN!\inputencoding{utf8}.
If no input variables are specified, then \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!addrv!\inputencoding{utf8}
will apply \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!FUN!\inputencoding{utf8}
to all non-\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}
columns. Further, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!addrv!\inputencoding{utf8}
has an optional argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!name!\inputencoding{utf8}
to give the new variable; this can be useful when adding several random
variables to a probability space (as above). If not specified, the
default name is {}``\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!X!\inputencoding{utf8}''.


\subsubsection*{Marginal Distributions}

As we can see above, often after adding a random variable $V$ to
a probability space one will find that $V$ has values that are repeated,
so that it becomes difficult to understand what the ultimate behavior
of $V$ actually is. We can use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!marginal!\inputencoding{utf8}
function to aggregate the rows of the sample space by values of $V$,
all the while accumulating the probability associated with $V$'s
distinct values. Continuing our example from above, suppose we would
like to focus entirely on the values and probabilities of $V=\max(X1,X2,X3)$. 

<<echo=TRUE,print=TRUE>>= 
marginal(S, vars = "V") 
@ 

We could save the probability space of $V$ in a data frame and study
it further, if we wish. As a final remark, we can calculate the marginal
distributions of multiple variables desired using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!vars!\inputencoding{utf8}
argument. For example, suppose we would like to examine the joint
distribution of $V$ and $W$. 

<<echo=TRUE,print=TRUE>>= 
marginal(S, vars = c("V", "W")) 
@ 

Note that the default value of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!vars!\inputencoding{utf8}
is the names of all columns except \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!probs!\inputencoding{utf8}.
This can be useful if there are duplicated rows in the probability
space.


\section{Chapter Exercises}

\setcounter{thm}{0}

<<echo = FALSE, results = hide>>=
rnorm(1)
@
\begin{xca}
Prove the assertion of Example BLANK. The number of conditions that
the events $A_{1}$, $A_{2}$, \ldots{}, $A_{n}$ must satisfy in
order to be mutually independent is $2^{n}-n-1$. (\emph{Hint}: think
about Pascal's triangle.)
\end{xca}

\paragraph*{Answer:}

The events must satisfy the product equalities two at a time, of which
there are ${n \choose 2}$, then they must satisfy an additional ${n \choose 3}$
conditions three at a time, and so on, until they satisfy the ${n \choose n}=1$
condition including all $n$ events. In total, there are \[
{n \choose 2}+{n \choose 3}+\cdots+{n \choose n}=\sum_{k=0}^{n}{n \choose k}-\left[{n \choose 0}+{n \choose 1}\right]\]
conditions to be satisfied, but the binomial series in the expression
on the right is the sum of the entries of the $n$$^{\text{th}}$
row of Pascal's triangle, which is $2^{n}$.



<<echo = FALSE, results = hide>>=
rnorm(1)
@
\begin{xca}
ddfsdf
\end{xca}



\chapter{Discrete Distributions\label{cha:Discrete-Distributions}}

In this chapter we introduce random variables, and in particular,
discrete random variables. We discuss probability mass functions and
introduce some special expectations, namely, the mean, variance and
standard deviation. Some of the more importand discrete distributions
are discussed in detail, and the more general concept of expectation
is defined, which paves the way for moment generating functions.

We give special attention to the empirical distribution since it plays
such a fundamental role with respect to resampling and Chapter BLANK;
it will also be needed in Section BLANK where we discuss the Kolmogorov-Smirnov
test. Following this is a section in which we introduce a catalogue
of discrete random variables that can be used to model experiments.

There are some comments on simulation, and we mention transformations
of random variables in the discrete case.

What do I want them to know?
\begin{itemize}
\item a buttload of discrete models
\item the idea of expectation and how to calculate it
\item moment generating functions
\item the dpqr family of functions, and their distr equivalents
\item what a PMF is, supports,
\end{itemize}

\section{Discrete Random Variables\label{sec:Discrete-Random-Variables}}


\subsection{Probability Mass Functions\label{sub:Probability-Mass-Functions}}

Discrete random variables are characterized by their supports which
take the form \begin{equation}
S_{X}=\{u_{1},u_{2},\ldots,u_{k}\}\mbox{ or }S_{X}=\{u_{1},u_{2},u_{3}\ldots\}.\end{equation}
Every discrete random variable $X$ has associated with it a probability
mass function (PMF) $f_{X}:S_{X}\to[0,1]$ defined by \begin{equation}
f_{X}(x)=\P(X=x),\quad x\in S_{X}.\end{equation}
Since values of the PMF represent probabilities, we know from Chapter
BLANK that PMFs enjoy certain properties. In particular, all PMFs
satisfy 
\begin{enumerate}
\item $f_{X}(x)>0$ for $x\in S$,
\item $\sum_{x\in S}f_{X}(x)=1$, and
\item $\P(X\in A)=\sum_{x\in A}f_{X}(x)$, for any event $A\subset S$.\end{enumerate}
\begin{example}
\label{exa:Toss-a-coin}Toss a coin 3 times. The sample space would
be\[
S=\left\{ HHH,\ HTH,\ THH,\ TTH,\ HHT,\ HTT,\ THT,\ TTT\right\} .\]
Now let $X$ be the number of Heads observed. Then $X$ has support
$S_{X}=\left\{ 0,1,2,3\right\} $. Assuming that the coin is fair
and was tossed in exactly the same way each time, it is not unreasonable
to suppose that the outcomes in the sample space are all equally likely.
What is the PMF of $X$? Notice that $X$ is zero exactly when the
outcome $TTT$ occurs, and this event has probability $1/8$. Therefore,
$f_{X}(0)=1/8$, and the same reasoning shows that $f_{X}(3)=1/8$.
Exactly three outcomes result in $X=1$, thus, $f_{X}(1)=3/8$ and
$f_{X}(3)$ holds the remaining $3/8$ probability (the total is 1).
We can represent the PMF with a table:

%
\begin{table}[H]
\begin{tabular}{c|cccc|c}
$x\in S_{X}$ & 0 & 1 & 2 & 3 & Total\tabularnewline
\hline
$f_{X}(x)=\P(X=x)$ & 1/8 & 3/8 & 3/8 & 1/8 & 1\tabularnewline
\end{tabular}
\end{table}

\end{example}

\subsection{Mean, Variance, and Standard Deviation\label{sub:Mean,-Variance,-and}}

There are numbers associated with PMFs. One important example is the
mean $\mu$, also known as $\E X$:\begin{equation}
\mu=\E X=\sum_{x\in S}xf_{X}(x).\end{equation}
Another important number is the variance:\begin{equation}
\sigma^{2}=\E(X-\mu)^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x),\end{equation}
which can be computed (see Exercise BLANK) with the alternate formula
$\sigma^{2}=\E X^{2}-(\E X)^{2}$. Directly defined from the variance
is the standard deviation $\sigma=\sqrt{\sigma^{2}}$. 
\begin{example}
\label{exa:We-will-calculate}We will calculate the mean of $X$ in
Example \ref{exa:Toss-a-coin}.\[
\mu=\sum_{x=0}^{3}xf_{X}(x)=0\cdot\frac{1}{8}+1\cdot\frac{3}{8}+2\cdot\frac{3}{8}+3\cdot\frac{1}{8}=3.5.\]
We interpret $\mu=3.5$ by reasoning that if we were to repeat the
random experiment many times, independently each time, observe many
corresponding outcomes of the random variable $X$, and take the sample
mean of the observations, then the calculated value would fall close
to 3.5. The approximation would get better as we observe more and
more values of $X$ (another form of the Law of Large Numbers; see
Chapter BLANK). Another way it is commonly stated is that $X$ is
3.5 {}``on the average'' or {}``in the long run''.\end{example}
\begin{rem}
Note that although we say $X$ is 3.5 on the average, we must keep
in mind that our $X$ never actually equals 3.5 (in fact, it is impossible
for $X$ to equal 3.5).
\end{rem}
Related to the probability mass function $f_{X}(x)=\P(X=x)$ is another
important function called the cumulative distribution function (CDF),
$F_{X}$. It is defined by the formula\begin{equation}
F_{X}(t)=\P(X\leq t),\quad-\infty<t<\infty.\end{equation}


We know that all PMFs satisfy certain properties, and a similar statement
may be made for CDFs. In particular, any CDF $F_{X}$ satisfies
\begin{itemize}
\item $F_{X}$ is nondecreasing ($t_{1}\leq t_{2}$ implies $F_{X}(t_{1})\leq F_{X}(t_{2})$).
\item $F_{X}$ is right-continuous ($\lim_{t\to a^{+}}F_{X}(t)=F_{X}(a)$
for all $a\in\R$).
\item $\lim_{t\to-\infty}F_{X}(t)=0$ and $\lim_{t\to\infty}F_{X}(t)=1$.
\end{itemize}
We say that $X$ has the distribution $F_{X}$ and we write $X\sim F_{X}$.
In an abuse of notation we will also write $X\sim f_{X}$ and for
the named distributions the PMF or CDF will be identified by the family
name instead of the defining formula.


\subsection{How to do it with \textsf{R}}

The mean and variance of a discrete random variable is easy to compute
at the console. Let's return to Example BLANK. We will start by defining
a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
containing the support of $X$, and a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!f!\inputencoding{utf8}
to contain the values of $f_{X}$ at the respective outcomes in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}:

<<keep.source = TRUE>>=
x <- c(0,1,2,3)
f <- c(1/8, 3/8, 3/8, 1/8)
@

To calculate the mean $\mu$, we need to multiply the corresponding
values of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!f!\inputencoding{utf8}
and add them. This is easily accomplished in \textsf{R} since operations
on vectors are performed \emph{element-wise} (see Section BLANK): 

<<>>=
mu <- sum(x * f)
mu
@

To compute the variance $\sigma^{2}$, we subtract the value of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mu!\inputencoding{utf8}
from each entry in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8},
square the answers, multiply by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!f!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sum!\inputencoding{utf8}.
The standard deviation $\sigma$ is simply the square root of $\sigma^{2}$.

<<keep.source = TRUE>>=
sigma2 <- sum((x-mu)^2 * f)
sigma2
sigma <- sqrt(sigma2)
sigma
@

Finally, we may find the values of the CDF $F_{X}$ on the support
by accumulating the probabilities in $f_{X}$ with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cumsum!\inputencoding{utf8}
function. 

<<>>=
F = cumsum(f)
F
@

As easy as this is, it is even easier to do with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distrEx!\inputencoding{utf8}
package. We define a random variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!X!\inputencoding{utf8}
as an object, then compute things from the object such as mean, variance,
and standard deviation with the functions \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!E!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!var!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sd!\inputencoding{utf8}:

<<keep.source = TRUE>>=
library(distrEx)     # note: distrEx depends on distr
X <- DiscreteDistribution(supp = 0:3, prob = c(1,3,3,1)/8)
E(X); var(X); sd(X)
@


\section{The Discrete Uniform Distribution\label{sec:The-Discrete-Uniform}}

We have seen the basic building blocks of discrete distributions and
we now study particular models that statisticians often encounter
in the field. Perhaps the most fundamental of all is the \emph{discrete
uniform} distribution.

A random variable $X$ with the discrete uniform distribution on the
integers $1,2,\ldots,m$ has PMF\begin{equation}
f_{X}(x)=\frac{1}{m},\quad x=1,2,\ldots,m.\end{equation}
We write $X\sim\mathsf{disunif}(m)$. A random experiment where this
distribution occurs is the choice of an integer at random between
1 and 100, inclusive. Let $X$ be the number chosen. Then $X\sim\mathsf{disunif}(m=100)$
and \[
\P(X=x)=\frac{1}{100},\quad x=1,\ldots,100.\]
We find a direct formula for the mean of $X\sim\mathsf{disunif}(m)$:\begin{equation}
\mu=\sum_{x=1}^{m}xf_{X}(x)=\sum_{x=1}^{m}x\cdot\frac{1}{m}=\frac{1}{m}(1+2+\cdots+m)=\frac{m+1}{2},\end{equation}
where we have used the famous identity $1+2+\cdots+m=m(m+1)/2$. That
is, if we repeatedly choose integers at random from 1 to $m$ then,
on the average, we expect to get $(m+1)/2$. To get the variance we
first calculate\[
\E X^{2}=\frac{1}{m}\sum_{x=1}^{m}x^{2}=\frac{1}{m}\frac{m(m+1)(2m+3)}{6}=\frac{(m+1)(2m+1)}{6},\]
 and finally,\begin{equation}
\sigma^{2}=\E X^{2}-(\E X)^{2}=\frac{(m+1)(2m+1)}{6}-\left(\frac{m+1}{2}\right)^{2}=\cdots=\frac{m^{2}-1}{12}.\end{equation}

\begin{example}
Roll a die and let $X$ be the upward face showing. Then $m=6$, $\mu=7/2=3.5$,
and $\sigma^{2}=(6^{2}-1)/12=35/12$.
\end{example}

\subsection{How to do it with \textsf{R}}


\paragraph*{From the console:}

One can choose an integer at random with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!sample!\inputencoding{utf8}
function. The general syntax to simulate a discrete uniform random
variable is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!sample(x, size, replace = TRUE)!\inputencoding{utf8}.

The argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!x!\inputencoding{utf8}
identifies the numbers from which to randomly sample. If \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!x!\inputencoding{utf8}
is a number, then sampling is done from 1 to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!x!\inputencoding{utf8}.
The argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!size!\inputencoding{utf8}
tells how big the sample size should be, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!replace!\inputencoding{utf8}
tells whether or not numbers should be replaced in the urn after having
been sampled. The default option is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!replace = FALSE!\inputencoding{utf8}
but for discrete uniforms the sampled values should be replaced. Some
examples follow.


\subsection{Examples}
\begin{itemize}
\item To roll a fair die 3000 times: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!sample(6, size = 3000, replace = TRUE)!\inputencoding{utf8}
\item To choose 27 random numbers from 30 to 70 : \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!sample(30:70, size = 27, replace = TRUE)!\inputencoding{utf8}
\item To flip a fair coin 1000 times: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!sample(c("H","T"), size = 1000, replace = TRUE)!\inputencoding{utf8}
\end{itemize}

\paragraph*{With the \textsf{R} Commander:}

Follow the sequence \textsf{Probability $\triangleright$ Discrete
Distributions $\triangleright$ Discrete Uniform distribution $\triangleright$
Simulate Discrete uniform variates\ldots{}. }

Suppose we would like to roll a fair die 3000 times. In the \inputencoding{latin9}\lstinline[breaklines=true,showstringspaces=false,tabsize=2]!Number of samples!\inputencoding{utf8}
field we enter \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!1!\inputencoding{utf8}.
Next, we describe what interval of integers to be sampled. Since there
are six faces numbered 1 through 6, we set \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!from = 1!\inputencoding{utf8},
we set \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!to = 6!\inputencoding{utf8},
and set \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!by = 1!\inputencoding{utf8}
(to indicate that we travel from 1 to 6 in increments of 1 unit).
We will generate a list of 3000 numbers selected from among 1, 2,
\ldots{}, 6, and we store the results of the simulation. For the
time being, we select \inputencoding{latin9}\lstinline[breaklines=true,showstringspaces=false,tabsize=2]!New Data set!\inputencoding{utf8}.
Click \textsf{OK}.

Since we are defining a new data set, the \textsf{R} Commander requests
a name for the data set. The default name is \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!Simset1!\inputencoding{utf8},
although in principle you could name it whatever you like (according
to \textsf{R}'s rules for object names). We wish to have a list that
is 3000 long, so we set \inputencoding{latin9}\lstinline[breaklines=true,showstringspaces=false,tabsize=2]!Sample Size = 3000!\inputencoding{utf8}
and click \textsf{OK}.

In the \textsf{R} Console window, the \textsf{R} Commander should
tell you that \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!Simset1!\inputencoding{utf8}
has been initialized, and it should also alert you that \inputencoding{latin9}\lstinline[breaklines=true,showstringspaces=false,tabsize=2]!There was 1 discrete uniform variate sample stored in Simset 1.!\inputencoding{utf8}.
To take a look at the rolls of the die, we click \textsf{View data
set} and a window opens. 

The default name for the variable is \inputencoding{latin9}\lstinline[showstringspaces=false,tabsize=2]!disunif.sim1!\inputencoding{utf8}.


\section{The Binomial Distribution\label{sec:The-Binomial-Distribution}}

The binomial distribution is based on a \emph{Bernoulli trial}, which
is a random experiment in which there are only two possible outcomes:
success ($S$) and failure ($F$). We conduct the Bernoulli trial
and let \[
X=\begin{cases}
1 & \mbox{if the outcome is \ensuremath{S}},\\
0 & \mbox{if the outcome is \ensuremath{F}}.\end{cases}\]
If the probability of success is $p$ then the probability of failure
must be $1-p=q$ and the PMF of $X$ is\begin{equation}
f_{X}(x)=p^{x}(1-p)^{1-x},\quad x=0,1.\end{equation}
It is easy to calculate $\mu=\E X=p$ and $\E X^{2}=p$ so that $\sigma^{2}=p-p^{2}=p(1-p)$.


\subsection{The Binomial Model\label{sub:The-Binomial-Model}}

The Binomial model has three defining properties:
\begin{itemize}
\item Bernoulli trials are conducted $n$ times,
\item the trials are independent,
\item the probability of success $p$ does not change between trials.
\end{itemize}
If $X$ counts the number of successes in the $n$ independent trials,
then the PMF of $X$ is 

\begin{equation}
f_{X}(x)={n \choose x}p^{x}(1-p)^{n-x},\quad x=0,1,2,\ldots,n.\end{equation}
We say that $X$ has a \emph{binomial distribution} and we write $X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)$.
It is clear that $f_{X}(x)\geq0$ for all $x$ in the support because
the value is the product of nonnegative numbers. We next check that
$\sum f(x)=1$:\[
\sum_{x=0}^{n}{n \choose x}p^{x}(1-p)^{n-x}=[p+(1-p)]^{n}=1^{n}=1.\]
We next find the mean:

\begin{alignat*}{1}
\mu= & \sum_{x=0}^{n}x\,{n \choose x}p^{x}(1-p)^{n-x},\\
= & \sum_{x=1}^{n}x\,\frac{n!}{x!(n-x)!}p^{x}q^{n-x},\\
= & n\cdot p\sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}q^{n-x},\\
= & np\,\sum_{x-1=0}^{n-1}{n-1 \choose x-1}p^{(x-1)}(1-p)^{(n-1)-(x-1)},\\
= & np.\end{alignat*}
 Using a similar argument, we find that $\E X(X-1)=n(n-1)p^{2}$ (see
Exercise BLANK). Therefore\begin{alignat*}{1}
\sigma^{2}= & \E X(X-1)+\E X-[\E X]^{2}\\
= & n(n-1)p^{2}+np-(np)^{2}\\
= & n^{2}p^{2}-np^{2}+np-n^{2}p^{2}\\
= & np-np^{2}=np(1-p).\end{alignat*}



\subsection{How to do it with \textsf{R}}

The corresponding \textsf{R} function for the PMF is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dbinom(x, size = n, prob = p)!\inputencoding{utf8},
and the corresponding function for the CDF is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!pbinom(x, size = n, prob = p)!\inputencoding{utf8}. 
\begin{example}
Consider a four child family. Each child may be either a boy ($B$)
or a girl ($G$). For simplicity we suppose that $\P(B)=\P(G)=1/2$
and that the genders of the children are determined independently.
If we let $X$ count the number of $B$'s, then $X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)$.
Further, $\P(X=2)$ is\[
f_{X}(2)={4 \choose 2}(1/2)^{2}(1/2)^{2}=\frac{6}{2^{4}}.\]
We can calculate it in \textsf{R} Commander under the \textsf{Binomial
Distribution} menu with the \textsf{Binomial probabilities} menu item.

<<echo = FALSE>>=
A <- data.frame(Pr=dbinom(0:4, size = 4, prob = 0.5))
rownames(A) <- 0:4 
A
@

\end{example}
We know that the $\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)$
distribution is supported on the integers 0, 1, 2, 3, and 4; thus
the table is complete. We can read off the answer to be $\P(X=2)=0.3750$.


\begin{example}
Roll 12 dice simultaneously, and let $X$ denote the number of 6's
that appear. We wish to find the probability of getting seven, eight,
or nine 6's. If we let $S=\left\{ \mbox{get a 6 on one roll}\right\} $,
then $\P(S)=1/6$ and the rolls constitute Bernoulli trials; thus
$X\sim\mathsf{binom}(\mathtt{size=}12,\ \mathtt{prob=}1/6)$ and our
task is to find $\P(7\leq X\leq9)$. This is just\[
\P(7\leq X\leq9)=\sum_{x=7}^{9}{12 \choose x}(1/6)^{x}(5/6)^{12-x}.\]
 Again, one method to solve this problem would be to generate a probability
mass table and add up the relevant rows. However, an alternative method
is to notice that $\P(7\leq X\leq9)=\P(X\leq9)-\P(X\leq6)=F_{X}(9)-F_{X}(6)$,
so we could get the same answer by using the \textsf{Binomial tail
probabilities\ldots{} }menu in the \textsf{R} Commander or the following
from the command line: 

<<keep.source = TRUE>>=
pbinom(9, size = 12, prob = 1/6) - pbinom(6, size = 12, prob = 1/6)
diff(pbinom(c(6,9), size = 12, prob = 1/6))  # same thing
@

\end{example}

\begin{example}
Toss a coin three times and let $X$ be the number of Heads observed.
We know from before that $X\sim\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)$
which implies the following PMF:

%
\begin{table}
\begin{tabular}{c|cccc}
$x=\mbox{\#of Heads}$ & 0 & 1 & 2 & 3\tabularnewline
\hline
$f(x)=\P(X=x)$ & 1/8 & 3/8 & 3/8 & 1/8\tabularnewline
\end{tabular}
\end{table}


Our next goal is to write down the CDF of $X$ explicitly. The first
case is easy: it is impossible for $X$ to be negative, so if $x<0$
then we should have $\P(X\leq x)=0$. Now choose a value $x$ satisfying
$0\leq x<1$, say, $x=0.3$. The only way that $X\leq x$ could happen
would be if $X=0$, therefore, $\P(X\leq x)$ should equal $\P(X=0)$,
and the same is true for any $0\leq x<1$. Similarly, for any $1\leq x<2$,
say, $x=1.73$, the event $\left\{ X\leq x\right\} $ is exactly the
event $\left\{ X=0\mbox{ or }X=1\right\} $. Consequently, $\P(X\leq x)$
should equal $\P(X=0\mbox{ or }X=1)=\P(X=0)+\P(X=1)$. Continuing
in this fashion, we may figure out the values of $F_{X}(x)$ for all
possible inputs $-\infty<x<\infty$, and we may summarize our observations
with the following piecewise defined function:\[
F_{X}(x)=\P(X\leq x)=\begin{cases}
0, & x<0,\\
\frac{1}{8}, & 0\leq x<1,\\
\frac{1}{8}+\frac{3}{8}=\frac{4}{8}, & 1\leq x<2,\\
\frac{4}{8}+\frac{3}{8}=\frac{7}{8}, & 2\leq x<3,\\
1, & x\geq3.\end{cases}\]
In particular, the CDF of $X$ is defined for the entire real line,
$\R$. The CDF is right continuous and nondecreasing. A graph of the
$\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)$ CDF is shown
in Figure BLANK.

\end{example}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig = true, height = 4, width = 6>>=
plot(0, xlim = c(-1.2, 4.2), ylim = c(-0.04, 1.04), type = "n", xlab = "number of successes", ylab = "cumulative probability")
abline(h = c(0,1), lty = 2, col = "grey")
lines(stepfun(0:3, pbinom(-1:3, size = 3, prob = 0.5)), verticals = FALSE, do.p = FALSE)
points(0:3, pbinom(0:3, size = 3, prob = 0.5), pch = 16, cex = 1.2)
points(0:3, pbinom(-1:2, size = 3, prob = 0.5), pch = 1, cex = 1.2)
@
\par\end{centering}

\caption{Graph of the $\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)$
CDF}

\end{figure}



\begin{example}
Another way to do Example BLANK is with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
family of packages. They use an object oriented approach to random
variables, that is, a random variable is stored in an object \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!X!\inputencoding{utf8},
and then questions about the random variable translate to functions
on and involving \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!X!\inputencoding{utf8}.
Random variables with distributions from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!base!\inputencoding{utf8}
package are specified by capitalizing the name of the distribution:

<<keep.source = TRUE>>=
library(distr)
X <- Binom(size = 3, prob = 1/2)
X
@

The analogue of the \inputencoding{latin9}\lstinline[showstringspaces=false]!dbinom!\inputencoding{utf8}
function for \inputencoding{latin9}\lstinline[showstringspaces=false]!X!\inputencoding{utf8}
is the \inputencoding{latin9}\lstinline[showstringspaces=false]!d(X)!\inputencoding{utf8}
function, and the analogue of the \inputencoding{latin9}\lstinline[showstringspaces=false]!pbinom!\inputencoding{utf8}
function is the \inputencoding{latin9}\lstinline[showstringspaces=false]!p(X)!\inputencoding{utf8}
function. Compare the following:

<<keep.source = TRUE>>=
d(X)(1)   # pmf of X evaluated at x = 1
p(X)(2)   # cdf of X evaluated at x = 2
@

\end{example}
Random variables defined via the \inputencoding{latin9}\lstinline[showstringspaces=false]!distr!\inputencoding{utf8}
package may be \emph{plotted}, which will return graphs of the PMF,
CDF, and quantile function (introduced in Section BLANK). See Figure
BLANK for an example.

%
\begin{figure}[H]
\begin{centering}
<<echo = FALSE, fig=true, height = 3, width = 6>>=
plot(X)
@
\par\end{centering}

\caption{The \textsf{binom}(\texttt{size} = 3, \texttt{prob} = 0.5) CDF}

\end{figure}


%
\begin{table}
\begin{centering}
\begin{tabular}{cccc}
\multicolumn{1}{c}{} &  & $\mathtt{X<-Binom(size=}n\mathtt{,\ prob=}p\mathtt{)}$\tabularnewline
$\mathsf{dbinom}(x,\,\mathtt{size}=n,\,\mathtt{prob}=p)$ & $\P(X=x)$ & PMF & $\mathsf{d}(\mathtt{d(X)})(x)$\tabularnewline
$\mathsf{pbinom}(x,\,\mathtt{size}=n,\,\mathtt{prob}=p)$ & $\P(X\leq x)$ & CDF & $\mathsf{p}(\mathtt{X})(x)$\tabularnewline
$\mathsf{rbinom}(k,\,\mathtt{size}=n,\,\mathtt{prob}=p)$ &  & random variates & $\mathsf{r}(\mathtt{X})(k)$\tabularnewline
 &  &  & \tabularnewline
\end{tabular}
\par\end{centering}

\caption{Correspondence between base \textsf{R} and \texttt{distr} with $X\sim\mathsf{dbinom}(\mathtt{size}=n,\,\mathtt{prob}=p)$}

\end{table}



\section{Expectation and Moment Generating Functions\label{sec:Expectation-and-Moment}}


\subsection{The Expectation Operator\label{sub:The-Expectation-Operator}}

We next generalize some of the concepts from Section BLANK. There
we saw that every%
\footnote{Not every, only those PMFs for which the (potentially infinite) series
converges.%
} PMF has two important numbers associated with it:\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x),\quad\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x).\end{equation}
Intuitively, for repeated observations of $X$ we would expect the
sample mean to closely approximate $\mu$ as the sample size increases
without bound. For this reason we call $\mu$ the \emph{expected value}
of $X$ and we write $\mu=\E X$, where $\E$ is an \emph{expectation
operator}.
\begin{defn}
More generally, given a function $g$ we define the \emph{expected
value of} $g(X)$ by\begin{equation}
\E\: g(X)=\sum_{x\in S}g(x)f_{X}(x),\end{equation}
provided the (potentially infinite) series $\sum_{x}|g(x)|f(x)$ converges,
and in that case we say that $\E g(X)$ \emph{exists}. 
\end{defn}
In this notation the variance is $\sigma^{2}=\E(X-\mu)^{2}$ and we
prove the identity \begin{equation}
\E(X-\mu)^{2}=\E X^{2}-(\E X)^{2}\end{equation}
in Exercise BLANK. Intuitively, for repeated observations of $X$
we would expect the sample mean of the $g(X)$ values to closely approximate
$\E\ g(X)$ as the sample size increases without bound.

Let us take the analogy further. If we expect $g(X)$ to be close
to $\E g(X)$ on the average, where would we expect $3g(X)$ to be
on the average? It could only be $3\E g(X)$. The following theorem
makes this idea precise.
\begin{thm}
For any functions $g$ and $h$, any random variable $X$, and any
constant $c$: 
\begin{enumerate}
\item $\E\: c=c$,
\item $\E[c\cdot g(X)]=c\E g(X)$
\item $\E[g(X)+h(X)]=\E g(X)+\E h(X)$,
\end{enumerate}
provided $\E g(X)$ and $\E h(X)$ exist.

\end{thm}
\begin{proof}
Go directly from the definition. For example,\[
\E[c\cdot g(X)]=\sum_{x\in S}c\cdot g(x)f_{X}(x)=c\cdot\sum_{x\in S}g(x)f_{X}(x)=c\E g(X).\]

\end{proof}

\subsection{Moment Generating Functions\label{sub:Moment-Generating-Functions}}
\begin{defn}
Given a random variable $X$, its \emph{moment generating function}
(abbreviated MGF) is defined by the formula\begin{equation}
M_{X}(t)=\E\:\me^{tX}=\sum_{x\in S}\me^{tx}f_{X}(x),\end{equation}
provided the (potentially infinite) series exists and is finite for
all $t$ in a neighborhood of zero (that is, for all $-\epsilon<t<\epsilon$,
for some $\epsilon>0$). 
\end{defn}
Note that for any MGF $M_{X}$, \begin{equation}
M_{X}(0)=\E\me^{0\cdot X}=\E1=1.\end{equation}
We will calculate the MGF for the two distributions introduced above.
\begin{example}
$X\sim\mathsf{disunif}(m)$. 

Since $f(x)=1/m$, the MGF takes the form\[
M(t)=\sum_{x=1}^{m}\me^{tx}\frac{1}{m}=\frac{1}{m}(\me^{t}+\me^{2t}+\cdots+\me^{mt}),\quad\mbox{for any \ensuremath{t}.}\]

\end{example}

\begin{example}
$X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)$.
\end{example}
\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{n}\me^{tx}{n \choose x}p^{x}(1-p)^{n-x},\\
= & \sum_{x=0}^{n-x}{n \choose x}(p\me^{t})^{x}q^{n-x},\\
= & (p\me^{t}+q)^{n},\quad\mbox{for any \ensuremath{t}.}\end{alignat*}



\subsection*{Applications}

There are two uses of moment generating functions that will be used
in this book. The first is the fact that the MGF may be used to accurately
identify probability distributions, which rests on the following:
\begin{thm}
The moment generating function, if it exists in a neighborhood of
zero, determines a probability distribution \emph{uniquely}. \end{thm}
\begin{proof}
Unfortunately, the proof of such a theorem is beyond the scope of
a text like this one. Interested readers could consult BLANK.
\end{proof}


We will see an example of Theorem BLANK in action.
\begin{example}
Suppose we encounter a random variable which has MGF \[
M_{X}(t)=(0.3+0.7\me^{t})^{13}.\]
Then $X\sim\mathsf{binom}(\mathtt{size}=13,\,\mathtt{prob}=0.7)$.
\end{example}


An MGF is also known as a {}``Laplace Transform'' and is used in
that context in many branches of science and engineering.


\subsection*{Why is it called a Moment Generating Function?}

This brings us to the second powerful use of MGFs. Many of the models
we study have a simple MGF indeed which allows us to determine the
mean, variance, and even higher moments very quickly. Let us see why.
We already know that

\begin{alignat*}{1}
M(t)= & \sum_{x\in S}\me^{tx}f(x),\end{alignat*}
Take the derivative with respect to $t$ to get\begin{equation}
M'(t)=\frac{\diff}{\diff t}\left(\sum_{x\in S}\me^{tx}f(x)\right)=\sum_{x\in S}\ \frac{\diff}{\diff t}\left(\me^{tx}f(x)\right)=\sum_{x\in S}x\me^{tx}f(x),\end{equation}
and so if we plug in zero for $t$ we see \begin{equation}
M'(0)=\sum_{x\in S}x\me^{0}f(x)=\sum_{x\in S}xf(x)=\mu=\E X.\end{equation}
Similarly, $M''(t)=\sum x^{2}\me^{tx}f(x)$ so that $M''(0)=\E X^{2}$.
And in general, we can see%
\footnote{We are glossing over some significant mathematical details in our
derivation. Suffice it to say that when the MGF exists in a neighborhood
of $t=0$, the exchange of differentiation and summation is valid
in that neighborhood, and our remarks hold true.%
} that\begin{equation}
M_{X}^{(r)}(0)=\E X^{r}=\mbox{\ensuremath{r^{th}}moment of \ensuremath{X}about the origin.}\end{equation}
These are also known as \emph{raw moments} and are sometimes denoted
$\mu_{r}'$. In addition to these are the so called \emph{central
moments} $\mu_{r}$ defined by\begin{equation}
\mu_{r}=\E(X-\mu)^{r},\quad r=1,2,\ldots\end{equation}

\begin{example}
Let $X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\mbox{ with \ensuremath{M(t)=(q+p\me^{t})^{n}}}$.
We calculated the mean and variance of a binomial random variable
in Section BLANK by means of the binomial series. But look how quickly
we find the mean and variance with the moment generating function.

\begin{alignat*}{1}
M'(t)= & n(q+p\me^{t})^{n-1}p\me^{t}\left|_{t=0}\right.,\\
= & n\cdot1^{n-1}p,\\
= & np.\end{alignat*}
And\begin{alignat*}{1}
M''(0)= & n(n-1)[q+p\me^{t}]^{n-2}(p\me^{t})^{2}+n[q+p\me^{t}]^{n-1}p\me^{t}\left|_{t=0}\right.,\\
\E X^{2}= & n(n-1)p^{2}+np.\end{alignat*}
Therefore \begin{alignat*}{1}
\sigma^{2}= & \E X^{2}-(\E X)^{2},\\
= & n(n-1)p^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=npq.\end{alignat*}
\end{example}
\begin{rem}
We learned in this section that $M^{(r)}(0)=\E X^{r}$. We remember
from Calculus II that certain functions $f$ can be represented by
a Taylor series expansion about a point $a$, which takes the form\begin{equation}
f(x)=\sum_{r=0}^{\infty}\frac{f^{(r)}(a)}{r!}(x-a)^{r},\quad\mbox{for all \ensuremath{|x-a|<R},}\end{equation}
where $R$ is called the \emph{radius of convergence} of the series
(see Appendix BLANK). Now we may combine this information to say that
if an MGF exists for all $t$ in the interval $(-\epsilon,\epsilon)$,
then we may write\begin{equation}
M_{X}(t)=\sum_{r=0}^{\infty}\frac{\E X^{r}}{r!}t^{r},\quad\mbox{for all \ensuremath{|t|<\epsilon}.}\end{equation}

\end{rem}

\subsection{How to do it with \textsf{R}}

The \inputencoding{latin9}\lstinline[showstringspaces=false]!distrEx!\inputencoding{utf8}
package provides an expectation operator \inputencoding{latin9}\lstinline[showstringspaces=false]!E!\inputencoding{utf8}
which can be used on random variables that have been defined in the
ordinary \inputencoding{latin9}\lstinline[showstringspaces=false]!distr!\inputencoding{utf8}
sense:

<<>>=
X <- Binom(size = 3, prob = 0.45)
library(distrEx)
E(X)
E(3*X + 4)
@

For discrete random variables with finite support, the expectation
is simply computed with direct summation. In the case that the random
variable has infinite support and the function is crazy, then the
expectation is not computed directly, rather, it is estimated by first
generating a random sample from the underlying model and next computing
a sample mean of the function of interest. 

There are methods for other population parameters:

<<>>=
var(X)
sd(X)
@

There are even methods for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!IQR!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!mad!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!skewness!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!kurtosis!\inputencoding{utf8}.


\section{The Empirical Distribution\label{sec:The-Empirical-Distribution}}

Do an experiment $n$ times and observe $n$ values $x_{1}$, $x_{2}$,
\ldots{}, $x_{n}$ of a random variable $X$. For simplicity in most
of the discussion that follows it will be convenient to suppose that
the observed values are distinct, but comparable remarks remain valid
even when the observed values are repeated. 
\begin{defn}
The \emph{empirical cumulative distribution function} $F_{n}$ (written
ECDF) is the probability distribution that places probability mass
$1/n$ on each of the values $x_{1}$, $x_{2}$, \ldots{}, $x_{n}$.
The empirical PMF takes the form\begin{equation}
f_{X}(x)=\frac{1}{n},\quad x\in\left\{ x_{1},x_{2},...,x_{n}\right\} .\end{equation}
If the value $x_{i}$ is repeated $k$ times, the mass at $x_{i}$
is accumulated to $k/n$.
\end{defn}


The mean of the empirical distribution is\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x)=\sum_{i=1}^{n}x_{i}\cdot\frac{1}{n}\end{equation}
and we recognize this last quantity to be the sample mean, $\xbar$.
The variance of the empirical distribution is\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x)=\sum_{i=1}^{n}(x_{i}-\xbar)^{2}\cdot\frac{1}{n}\end{equation}
and this last quantity looks very close to what we already know to
be the sample variance.\[
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\xbar)^{2}.\]
The \emph{empirical quantile function} is the inverse of the ECDF.
See Section BLANK.


\subsection{How to do it with \textsf{R}}

The empirical distribution is not directly available as a distribution
in the same way that the other base probability distributions are,
but there are plenty of resources available. 

Given a data vector of observed values \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!x!\inputencoding{utf8},
we can see the empirical CDF with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true]!ecdf!\inputencoding{utf8}
function:

<<>>=
x <- c(4, 7, 9, 11, 12)
ecdf(x)
@

The above shows that the returned value of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!ecdf(x)!\inputencoding{utf8}is
not a number but rather a \emph{function}. It is not usually used
in this form, by itself. More commonly it is used as an intermediate
step in a more complicated calculation, for instance, in hypothesis
testing (see Section BLANK) or resampling (see Chapter BLANK). It
is nevertheless instructive to see what the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false]!ecdf!\inputencoding{utf8}
looks like, and there is a special plot method for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!ecdf!\inputencoding{utf8}
objects.

<<eval = FALSE>>=
plot(ecdf(x))
@

%
\begin{figure}[H]
\begin{centering}
<<echo = FALSE, fig=true, height = 3, width = 6>>=
plot(ecdf(x))
@
\par\end{centering}

\caption{The empirical CDF}

\end{figure}


See Figure BLANK. The graph is of a right-continuous function with
jumps exactly at the locations stored in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}.
There are no repeated values in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!x!\inputencoding{utf8}
so all of the jumps are equal to $1/5=0.2$.

The empirical PDF is not usually of particular interest in itself,
but if we really wanted we could define a function to serve as the
empirical PDF:

<<keep.source = TRUE>>=
epdf <- function(x) function(t){sum(x %in% t)/length(x)}
x <- c(0,0,1)
epdf(x)(0)       # should be 2/3
@

To simulate from the empirical distribution supported on the vector
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!x!\inputencoding{utf8},
we can simply use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!sample!\inputencoding{utf8}
function.

<<>>=
x <- c(0,0,1)
sample(x, size = 7, replace = TRUE)       # should be 2/3
@

We can get the empirical quantile function in \textsf{R} with \inputencoding{latin9}\lstinline[breaklines=true,showstringspaces=false]!quantile(x, probs = p, type = 1)!\inputencoding{utf8}.

As we hinted above, the real significance of the empirical distribution
is associated with its appearance in more sophisticated applications.
We will explore some of these in later chapters -- see, for instance,
Chapter BLANK.


\section{Other Discrete Distributions\label{sec:Other-Discrete-Distributions}}

The binomial and discrete uniform distributions are very popular,
and rightly so; they are simple and form the foundation for many,
many more complicated distributions. But these only apply to a limited
range of problems. In this section we introduce some situations for
which we need more than the uniform and binomial.


\subsection{Dependent Bernoulli Trials\label{sec:Non-Bernoulli-Trials}}


\subsubsection*{The Hypergeometric Distribution\label{sub:Hypergeometric-Distribution}}

Consider an urn with 7 white balls and 5 black balls. Let our random
experiment be to randomly select 4 balls, without replacement, from
the urn. Then the probability of observing 3 white balls (and thus
1 black ball) would be \begin{equation}
\P(3W,1B)=\frac{{7 \choose 3}{5 \choose 1}}{{12 \choose 4}}.\end{equation}
In general, consider sampling without replacement $K$ times from
an urn with $M$ white balls and $N$ black balls. Let $X$ count
the number of white balls in the sample. The PMF of $X$ is 

\[
f_{X}(x)=\frac{{M \choose x}{N \choose K-x}}{{M+N \choose K}},\quad x=0,1,2,\ldots,K.\]
We say that $X$ has a \emph{hypergeometric distribution} and write
$X\sim\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=N,\,\mathtt{k}=K)$.
It is shown in Exercise BLANK that \begin{equation}
\mu=K\frac{M}{M+N},\quad\sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}.\end{equation}


The associated \textsf{R} function is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]!dhyper(x, m = M, n = N, k = K)!\inputencoding{utf8}
. The other functions are

\inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]
	phyper(q, m = M, n = N, k = K)
\end{lstlisting}
\inputencoding{utf8}

\inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]
	qhyper(p, m = M, n = N, k = K, lower.tail = TRUE)
\end{lstlisting}
\inputencoding{utf8}

\inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=4]
	rhyper(num, m = M, n = N, k = K)
\end{lstlisting}
\inputencoding{utf8}and these give the CDF, quantiles, and random variates, respectively.
\begin{example}
Suppose in a certain shipment of 250 Pentium processors there are
17 defective processors. A quality control consultant randomly collects
5 processors for inspection to determine whether or not they are defective.
Let $X$ denote the number of defectives in the sample. \end{example}
\begin{enumerate}
\item Find the probability of exactly 3 defectives in the sample, that is,
find $\P(X=3)$. 


\emph{Solution:} We know that $X\sim\mathsf{hyper}(\mathtt{m}=17,\,\mathtt{n}=233,\,\mathtt{k}=5)$.
So the required probability is just\[
f_{X}(3)=\frac{{17 \choose 3}{233 \choose 2}}{{250 \choose 5}}.\]
To calculate it in \textsf{R} we just type 

<<>>=
dhyper(3, m = 17, n = 233, k = 5)
@

To find it with the \textsf{R} Commander we click Probability $\to$
Discrete Distributions $\to$ Hypergeometric distribution$\to$ Hypergeometric
probabilities\emph{\ldots{}}.We fill in the parameters $m=17$, $n=233$,
and $k=5$. Click OK, and the following table is shown in the window.

<<>>=
A <- data.frame(Pr=dhyper(0:4, m = 17, n = 233, k = 5))
rownames(A) <- 0:4 
A
@

We wanted $\P(X=3)$, and this is found from the table to be approximately
0.0024. The value is rounded to the fourth decimal place.

We know from our above discussion that the sample space should be
$x=0,1,2,3,4,5$, yet, in the table the probabilities are only displayed
for $x=1,2,3$, and 4. What is happening? As it turns out, the \textsf{R}
Commander will only display probabilities that are 0.00005 or greater.
Since $x=5$ is not shown, it suggests that the outcome has a tiny
probability. To find its exact value we may use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false,tabsize=4]!dhyper!\inputencoding{utf8}
function:

<<>>=
dhyper(5, m = 17, n = 233, k = 5)
@

In other words, $\P(X=5)\approx0.0000007916049$, a small number indeed.

\item Find the probability that there are at most 2 defectives in the sample,
that is, compute $\P(X\leq2)$.


\emph{Solution:} Since $\P(X\leq2)=\P(X=0,1,2)$, one way to do this
would be to add the 0, 1, and 2 entries in the above table. this gives
$0.7011+0.2602+0.0362=0.9975$. Our answer should be correct up to
the accuracy of 4 decimal places. However, a more precise method is
provided by the \textsf{R} Commander. Under the \textsf{Hypergeometric
distribution} menu we select \textsf{Hypergeometric tail probabilities}\textsf{\emph{\ldots{}}}.
We fill in the parameters $m$, $n$, and $k$ as before, but in the
\textsf{Variable value(s)} dialog box we enter the value 2. We notice
that the \texttt{Lower tail} option is checked, and we leave that
alone. Click \textsf{OK}.

<<>>=
phyper(2, m = 17, n = 233, k = 5)
@

And thus $\P(X\leq2)\approx0.9975771$. We have confirmed that the
above answer was correct up to four decimal places.

\item Find $\P(X>1)$. 


The table did not give us the explicit probability $\P(X=5)$, so
we can not use the table to give us this probability. We need to use
another method. Since $\P(X>1)=1-\P(X\leq1)=1-F_{X}(1)$, we can find
the probability with \textsf{Hypergeometric tail probabilities}\textsf{\emph{\ldots{}}}.
We enter 1 for \textsf{Variable Value(s)}, we enter the parameters
as before, and in this case we choose the \texttt{Upper tail} option.
This results in the following output.

<<>>=
phyper(1, m = 17, n = 233, k = 5, lower.tail = FALSE)
@

In general, the \texttt{Upper tail} option of a tail probabilities
dialog computes $\P(X>x)$ for all given \textsf{Variable Value(s)}
$x$.

\item Generate $100,000$ observations of the random variable $X$.


We can randomly simulate as many observations of $X$ as we want in
\textsf{R} Commander. Simply choose \textsf{Simulate hypergeometric
variates\ldots{}} in the \textsf{Hypergeometric distribution} dialog.

In the \textsf{Number of samples} dialog, type 1. Enter the parameters
as above. Under the \textsf{Store Values} section, make sure \textsf{New
Data set} is selected. Click \textsf{OK}. 

A new dialog should open, with the default name \texttt{Simset1}.
We could change this if we like, according to the rules for \textsf{R}
object names. In the sample size box, enter 100000. Click \textsf{OK}. 

In the Console Window, \textsf{R} Commander should issue an alert
that \texttt{Simset1} has been initialized, and in a few seconds,
it should also state that 100,000 hypergeometric variates were stored
in \texttt{hyper.sim1}. We can view the sample by clicking the \textsf{View
Data Set}\texttt{ }button on the \textsf{R} Commander interface.

We know from our formulas that $\mu=K\cdot M/(M+N)=5*17/250=0.34$.
We can check our formulas using the fact that with repeated observations
of $X$ we would expect about 0.34 defectives on the average. To see
how our sample reflects the true mean, we can compute the sample mean

\texttt{\textcolor{blue}{Rcmdr> mean(Simset2\$hyper.sim1, na.rm=TRUE)}}~\\
\texttt{\textcolor{blue}{{[}1{]} 0.340344}}

\texttt{\textcolor{blue}{Rcmdr> sd(Simset2\$hyper.sim1, na.rm=TRUE)}}~\\
\texttt{\textcolor{blue}{{[}1{]} 0.5584982}}

\texttt{\textcolor{blue}{$\vdots$}}

We see that when given many independent observations of $X$, the
sample mean is very close to the true mean $\mu$. We can repeat the
same idea and use the sample standard deviation to estimate the true
standard deviation of $X$. From the output above our estimate is
0.5584982, and from our formulas we get\[
\sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}\approx0.3117896,\]
 with $\sigma=\sqrt{\sigma^{2}}\approx0.5583811944$. Our estimate
was pretty close.

From the console we can generate random hypergeometric variates with
the \inputencoding{latin9}\lstinline!rhyper!\inputencoding{utf8}
function, as demonstrated below.

<<>>=
rhyper(10, m = 17, n = 233, k = 5)
@

\end{enumerate}

\subsubsection*{Sampling With and Without Replacement\label{sub:Sampling-With-and}}

Suppose that we have a large urn with, say, $M$ white balls and $N$
black balls. We take a sample of size $n$ from the urn, and let $X$
count the number of white balls in the sample. If we sample:
\begin{description}
\item [{without~replacement,}] then $X\sim\mathsf{hyper}(\mathtt{m=}M,\,\mathtt{n}=N,\,\mathtt{k}=n)$
and has mean and variance\begin{alignat*}{1}
\mu= & n\frac{M}{M+N},\\
\sigma^{2}= & n\frac{MN}{(M+N)^{2}}\frac{M+N-n}{M+N-1},\\
= & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right)\frac{M+N-n}{M+N-1}.\end{alignat*}
On the other hand, if we sample
\item [{with~replacement,}] then $X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=M/(M+N))$
with mean and variance\begin{alignat*}{1}
\mu= & n\frac{M}{M+N},\\
\sigma^{2}= & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right).\end{alignat*}

\end{description}
We see that both sampling procedures have the same mean, and the method
with the larger variance is the {}``with replacement'' scheme. The
factor in which the variances differ,\begin{equation}
\frac{M+N-n}{M+N-1}\end{equation}
is called a \emph{finite population correction}. For a fixed sample
size $n$, as $M,N\to\infty$ it is clear that the correction goes
to 1, that is, for infinite populations the sampling schemes are essentially
the same with respect to mean and variance.


\subsection{Waiting Time Distributions\label{sec:Waiting-Time-Distributions}}

Another important class of problems is associated with the amount
of time it takes for a specified event of interest to occur.


\subsubsection*{The Geometric Distribution\label{sub:The-Geometric-Distribution}}

Suppose that we conduct Bernoulli trials repeatedly, noting the successes
and failures. Let $X$ be the number of failures before a success.
If $\P(S)=p$ then $X$ has PMF

\begin{equation}
f_{X}(x)=p(1-p)^{x},\quad x=0,1,2,\ldots\end{equation}
(Why?) We say that $X$ has a \emph{Geometric distribution} and we
write $X\sim\mathsf{geom}(\mathtt{prob}=p)$. 

Again it is clear that $f(x)\geq0$ and we check that $\sum f(x)=1$
see Equation BLANK in Appendix BLANK):\begin{alignat*}{1}
\sum_{x=0}^{\infty}p(1-p)^{x}= & p\sum_{x=0}^{\infty}q^{x}=p\frac{1}{1-q}=1.\end{alignat*}
The associated \textsf{R} function is \textsf{\textbf{dgeom}}\texttt{(x,
prob} \texttt{\textcolor{red}{=}} \texttt{p)}. The other functions
are

\textsf{\textbf{pgeom}}\texttt{(q, prob, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{qgeom}}\texttt{(p, prob, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{rgeom}}\texttt{(n, prob)}

and these give the CDF, quantiles, and random variates, respectively.
We will find in the next section that the mean and variance are\begin{equation}
\mu=\frac{1-p}{p}=\frac{q}{p},\quad\sigma^{2}=\frac{q}{p^{2}}\end{equation}

\begin{example}
The Pittsburgh Steelers place kicker, Jeff Reed, made 81.2\% of his
attempted field goals in his career up to 2006. Assuming that his
successive field goal attempts are approximately Bernoulli trials,
find the probability that Jeff misses at least 5 field goals before
his first successful goal.\emph{ }

\emph{Solution}: If $X=$ the number of missed goals until Jeff's
first success, then $X\sim\mathsf{geom}(\mathtt{prob}=0.812)$ and
we want $\P(X\geq5)=\P(X>4)$. We can find this in \textsf{R} with 

<<>>=
pgeom(4, prob = 0.812, lower.tail = FALSE)
@

\end{example}
\begin{note}
Some books use a slightly different definition of the geometric distribution.
They consider Bernoulli trials and let $Y$ count instead the number
of trials until a success, so that $Y$ has PMF\begin{equation}
f_{Y}(y)=p(1-p)^{y-1},\quad y=1,2,3,\ldots\end{equation}
When they say {}``geometric distribution'', this is what they mean.
It is not hard to see that the two definitions are related. In fact,
if $X$ denotes our geometric and $Y$ theirs, then $Y=X+1$. Consequently,
they have $\mu_{Y}=\mu_{X}+1$ and $\sigma_{Y}^{2}=\sigma_{X}^{2}$.
\end{note}

\subsubsection*{The Negative Binomial Distribution\label{sub:The-Negative-Binomial}}

We may generalize the problem and consider the case where we wait
for \emph{more} than one success. Suppose that we conduct Bernoulli
trials repeatedly, noting the respective successes and failures. Let
$X$ count the number of failures before $r$ successes. If $\P(S)=p$
then $X$ has PMF

\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}p^{r}(1-p)^{x},\quad x=0,1,2,\ldots\end{equation}
We say that $X$ has a \emph{Negative Binomial distribution} and we
write $X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)$. 

As usual it should be clear that $f_{X}(x)\geq0$ and the fact that
$\sum f_{X}(x)=1$ follows from Calculus where we found the following
generalization of the geometric series using Maclaurin's series expansion:\begin{alignat*}{1}
\frac{1}{1-t}= & \sum_{k=0}^{\infty}t^{k},\quad\mbox{for \ensuremath{-1<t<1}},\mbox{ and}\\
\frac{1}{(1-t)^{r}}= & \sum_{k=0}^{\infty}{r+k-1 \choose r-1}t^{k},\quad\mbox{for \ensuremath{-1<t<1}}.\end{alignat*}
Therefore \begin{equation}
\sum_{x=0}^{\infty}f_{X}(x)=p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}q^{x}=p^{r}(1-q)^{-r}=1,\end{equation}
since $|q|=|1-p|<1$. The associated \textsf{R} function is \textsf{\textbf{dnbinom}}\texttt{(x,
size} \texttt{\textcolor{red}{=}} \texttt{r, prob} \texttt{\textcolor{red}{=}}
\texttt{p)}. The other functions are

\textsf{\textbf{pnbinom}}\texttt{(q, size, prob, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{qnbinom}}\texttt{(p, size, prob, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{rnbinom}}\texttt{(n, size, prob)}

and these give the CDF, quantiles, and random variates, respectively.
\begin{example}
We flip a coin repeatedly and let $X$ count the number of Tails until
we get seven Heads. What is $\P(X=5)?$

\emph{Solution:} We know that $X\sim\mathsf{nbinom}(\mathtt{size}=7,\,\mathtt{prob}=1/2)$.
\[
\P(X=5)=f_{X}(5)={7+5-1 \choose 7-1}(1/2)^{7}(1/2)^{5}={11 \choose 6}2^{-12}\]
and we can get this in \textsf{R} with
\end{example}
<<>>=
dnbinom(5, size = 7, prob = 0.5)
@

Let us next compute the MGF of $X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)$.

\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{\infty}\me^{tx}\ {r+x-1 \choose r-1}p^{r}q^{x}\\
= & p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}[q\me^{t}]^{x}\\
= & p^{r}(1-qe^{t})^{-r},\quad\mbox{provided \ensuremath{|q\me^{t}|<1,}}\end{alignat*}
and so \begin{equation}
M_{X}(t)=\left(\frac{p}{1-q\me^{t}}\right)^{r},\quad\mbox{for \ensuremath{q\me^{t}<1}}.\end{equation}
We see that $q\me^{t}<1$ when $t<-\ln(1-p)$.
\begin{example}
Let $X\sim\mathsf{nbinom}(\mathtt{size}=r,\mathtt{prob}=p)\mbox{ with \ensuremath{M(t)=p^{r}(1-q\me^{t})^{-r}}}$.
We proclaimed above the values of the mean and variance. Now we are
equipped with the tools to find these directly.

\begin{alignat*}{1}
M'(t)= & p^{r}(-r)(1-q\me^{t})^{-r-1}(-q\me^{t}),\\
= & rq\me^{t}p^{r}(1-q\me^{t})^{-r-1},\\
= & \frac{rq\me^{t}}{1-q\me^{t}}M(t),\mbox{ and so }\\
M'(0)= & \frac{rq}{1-q}\cdot1=\frac{rq}{p}.\end{alignat*}
Thus $\mu=rq/p$. We next find $\E X^{2}$ by calculating\begin{alignat*}{1}
M''(0)= & \left.\frac{rq\me^{t}(1-q\me^{t})-rq\me^{t}(-q\me^{t})}{(1-q\me^{t})^{2}}M(t)+\frac{rq\me^{t}}{1-q\me^{t}}M'(t)\right|_{t=0},\\
= & \frac{rqp+rq^{2}}{p^{2}}\cdot1+\frac{rq}{p}\left(\frac{rq}{p}\right),\\
= & \frac{rq}{p^{2}}+\left(\frac{rq}{p}\right)^{2}.\end{alignat*}
Finally we may say $\sigma^{2}=M''(0)-[M'(0)]^{2}=rq/p^{2}.$
\end{example}

\begin{example}
A random variable has MGF\[
M_{X}(t)=\left(\frac{0.19}{1-0.81\me^{t}}\right)^{31}.\]
Then $X\sim\mathsf{nbinom}(\mathtt{size}=31,\,\mathtt{prob}=0.19)$.\end{example}
\begin{note}
As with the Geometric distribution, some books use a slightly different
definition of the Negative Binomial distribution. They consider Bernoulli
trials and let $Y$ be the number of trials until $r$ successes,
so that $Y$ has PMF\begin{equation}
f_{Y}(y)={y-1 \choose r-1}p^{r}(1-p)^{y-r},\quad y=r,r+1,r+2,\ldots\end{equation}
It is again not hard to see that if $X$ denotes our Negative Binomial
and $Y$ theirs, then $Y=X+r$. Consequently, they have $\mu_{Y}=\mu_{X}+r$
and $\sigma_{Y}^{2}=\sigma_{X}^{2}$.
\end{note}

\subsection{Arrival Processes\label{sec:Arrival-Processes}}


\subsubsection*{The Poisson Distribution\label{sub:The-Poisson-Distribution}}

This is a distribution associated with {}``rare events'', for reasons
which will become clear in a moment. The events might be:
\begin{itemize}
\item traffic accidents,
\item typing errors, or
\item customers arriving in a bank.
\end{itemize}
Let $\lambda$ be the average number of events in the time interval
$[0,1]$. Let the random variable $X$ count the number of events
occurring in the interval. Then under certain reasonable conditions
it can be shown that

\begin{equation}
f_{X}(x)=\P(X=x)=\me^{-\lambda}\frac{\lambda^{x}}{x!},\quad x=0,1,2,\ldots\end{equation}
We use the notation $X\sim\mathsf{pois}(\mathtt{lambda}=\lambda)$.
The associated \textsf{R} function is \textsf{\textbf{dpois}}\texttt{(x,
lambda} \texttt{\textcolor{red}{=}} \texttt{l)}. The other functions
are

\textsf{\textbf{ppois}}\texttt{(q, lambda, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{qpois}}\texttt{(p, lambda, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{rpois}}\texttt{(n, lambda)}

and these give the CDF, quantiles, and random variates, respectively. 


\paragraph*{What are the reasonable conditions?}

Divide $[0,1]$ into subintervals of length $1/n$. 


\paragraph*{Assumptions:}
\begin{itemize}
\item The probability of an event occurring in a particular subinterval
is $\approx\lambda/n$.
\item The probability of two or more events occurring in any subinterval
is $\approx0$.
\item occurrences in disjoint subintervals are independent.\end{itemize}
\begin{rem}
If $X$ counts the number of events in the interval $[0,t]$ and $\lambda$
is the average number that occur in unit time, then $X\sim\mathsf{pois}(\mathtt{lambda}=\lambda t)$,
that is,\begin{equation}
\P(X=x)=\me^{-\lambda t}\frac{(\lambda t)^{x}}{x!},\quad x=0,1,2,3\ldots\end{equation}
\end{rem}
\begin{example}
On the average, five cars arrive at a particular car wash every hour.
Let $X$ count the number of cars that arrive from 10AM to 11AM. Then
$X\sim\mathsf{pois}(\mathtt{lambda}=5)$. Also, $\mu=\sigma^{2}=5$.
What is the probability that no car arrives during this period? 

Solution: The probability that no car arrives is \[
\P(X=0)=\me^{-5}\frac{5^{0}}{0!}=\me^{-5}\approx0.0067.\]


\end{example}

\begin{example}
Suppose the car wash above is in operation from 8AM to 6PM, and we
let $Y$ be the number of customers that appear in this period. Since
this period covers a total of 10 hours, from Remark BLANK we get that
$Y\sim\mathsf{pois}(\mathtt{lambda}=5\ast10=50)$. What is the probability
that there are between 48 and 50 customers, inclusive?

Solution: We want $\P(48\leq Y\leq50)=\P(X\leq50)-\P(X\leq47)$. See
Example BLANK: 

<<>>=
diff(ppois(c(47, 50), lambda = 50))
@

\end{example}

\section{Simulating Discrete Random Variables}

For many of the basic distributions, it is a simple application of
the r-method.


\section{Functions of Discrete Random Variables}

We have built a large catalogue of discrete distributions, but the
tools of this section will give us the ability to consider infinitely
many more. Given a random variable $X$ and a given function $h$,
we may consider $Y=h(X)$. Since the values of $X$ are determined
by chance, so are the values of $Y$. The question is, what is the
PMF of the random variable $Y$? The answer, of course, depends on
$h$. In the case that $h$ is one-to-one (see Appendix BLANK), the
solution can be found by simple substitution.
\begin{example}
Let $X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)$. We
saw in Section BLANK that $X$ represents the number of failures until
$r$ successes in a sequence of Bernoulli trials. Suppose now that
instead we were interested in counting the number of trials (successes
and failures) until the $r$$^{\text{th}}$ success occurs, which
we will denote by $Y$. In a given performance of the experiment,
the number of failures ($X$) and the number of successes ($r$) together
will comprise the total number of trials ($Y$), or in other words,
$X+r=Y$. We may let $h$ be defined by $h(x)=x+r$ so that $Y=h(X)$,
and we notice that $h$ is linear and hence one-to-one. Finally, $X$
takes values $0,\ 1,\ 2,\ldots$ implying that the support of $Y$
would be $\left\{ r,\ r+1,\ r+2,\ldots\right\} $. Solving for $X$
we get $X=Y-r$. Examining the PMF of $X$\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}p^{r}(1-p)^{x}\end{equation}
we can substitute $x=y-r$ to get\begin{eqnarray*}
f_{Y}(y) & = & f_{X}(y-r)\\
 & = & {r+(y-r)-1 \choose r-1}p^{r}(1-p)^{y-r}\\
 & = & {y-1 \choose r-1}p^{r}(1-p)^{y-r},\quad y=r,\, r+1,\ldots\end{eqnarray*}

\end{example}


Even when the function $h$ is not one-to-one, we may still find the
PMF of $Y$ simply by accumulating, for each $y$, the probability
of all the $x$'s that are mapped to that $y$.


\begin{prop}
Let $X$ be a discrete random variable with PMF $f_{X}$ supported
on the set $S_{X}$. Let $Y=h(X)$ for some function $h$. Then $Y$
has PMF $f_{Y}$ defined by\[
f_{Y}(y)=\sum_{\{x\in S_{X}|\, h(x)=y\}}f_{X}(x)\]
 \end{prop}
\begin{example}
Let $X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)$, and
let $Y=(X-1)^{2}$. Let us consider a table of values.

%
\begin{table}[H]
\begin{tabular}{c|ccccc}
$x$ & 0 & 1 & 2 & 3 & 4\tabularnewline
\hline
$f_{X}(x)$ & $1/16$ & $1/4$ & $6/16$ & $1/4$ & $1/16$\tabularnewline
\hline
$y=(x-2)^{2}$ & 1 & 0 & 1 & 4 & 9\tabularnewline
\end{tabular}
\end{table}


From this we see that $Y$ has support $S_{Y}=\{0,1,4,9\}$. We also
see that $h(x)=(x-1)^{2}$ is not one-to-one on the support of $X$,
because both $x=0$ and $x=2$ are mapped by $h$ to $y=1$. Nevertheless,
we see that $Y=0$ only when $X=1$, which has probability $1/4$;
therefore, $f_{Y}(0)$ must equal $1/4$. A similar approach works
for $y=4$ and $y=9$. And $Y=1$ exactly when $X=0$ or $X=2$, which
has total probability $7/16$. In summary, the PMF of $Y$ may be
written:

\begin{tabular}{c|cccc}
$y$ & 0 & 1 & 4 & 9\tabularnewline
\hline
$f_{X}(x)$ & $1/4$ & $7/16$ & $1/4$ & $1/16$\tabularnewline
\end{tabular}

Note that there is not a special name for the distribution of $Y$,
it simply serves as an example of what to do when the transformation
of a random variable is not one-to-one. The method is the same for
more complicated problems.\end{example}
\begin{prop}
If $X$ is a random variable with $\E X=\mu$ and $\mbox{Var}(X)=\sigma^{2}$,
then the mean and variance of $Y=mX+b$ is \[
\mu_{Y}=m\mu+b,\quad\sigma_{Y}^{2}=m^{2}\sigma^{2},\quad\sigma_{Y}=|m|\sigma\]

\end{prop}

\section{Chapter Exercises}

\setcounter{thm}{0}
\begin{enumerate}
\item Suppose that there are \Sexpr{rnorm(1)}.
\item A recent national study showed that approximately 44.7\% of college
students have used Wikipedia as a source in at least one of their
term papers. Let $X$ equal the number of students in a random sample
of size $n=31$ who have used Wikipedia as a source. 

\begin{enumerate}
\item How is $X$ distributed? \[
X\sim\mathsf{binom}(\mathtt{size}=31,\,\mathtt{prob}=0.447)\]

\item Sketch the probability mass function (roughly).


\begin{center}
<<echo = FALSE, fig=true, height = 3, width = 6>>=
xmin <- qbinom(.0005, size=31 , prob=0.447) 
xmax <- qbinom(.9995, size=31 , prob=0.447) 
.x <- xmin:xmax 
plot(.x, dbinom(.x, size=31, prob=0.447), xlab="Number of Successes", ylab="Probability Mass",    main="Binomial Dist'n: Trials = 31, Prob of success = 0.447", type="h") 
points(.x, dbinom(.x, size=31, prob=0.447), pch=16) 
abline( h = 0, lty = 2, col = "grey" ) 
remove(.x, xmin, xmax)
@
\par\end{center}

\item Sketch the cumulative distribution function (roughly).


\begin{center}
<<echo = FALSE, fig=true, height = 3, width = 6>>=
xmin <- qbinom(.0005, size=31 , prob=0.447) 
xmax <- qbinom(.9995, size=31 , prob=0.447) 
.x <- xmin:xmax 
plot( stepfun(.x, pbinom((xmin-1):xmax, size=31, prob=0.447)), verticals=FALSE, do.p=FALSE, xlab="Number of Successes", ylab="Cumulative Probability", main="Binomial Dist'n: Trials = 31, Prob of success = 0.447") 
points( .x, pbinom(xmin:xmax, size=31, prob=0.447), pch = 16, cex=1.2 ) 
points( .x, pbinom((xmin-1):(xmax-1), size=31, prob=0.447), pch = 1,    cex=1.2 ) 
abline( h = 1, lty = 2, col = "grey" ) 
abline( h = 0, lty = 2, col = "grey" ) 
remove(.x, xmin, xmax) 
@
\par\end{center}

\item Find the probability that $X$ is equal to 17.


<<>>=
dbinom(17, size = 31, prob = 0.447)
@

\item Find the probability that $X$ is at most 13.


<<>>=
pbinom(13, size = 31, prob = 0.447)
@

\item Find the probability that $X$ is bigger than 11.


<<>>=
pbinom(11, size = 31, prob = 0.447, lower.tail = FALSE)
@

\item Find the probability that $X$ is at least 15.


<<>>=
pbinom(14, size = 31, prob = 0.447, lower.tail = FALSE)
@

\item Find the probability that $X$ is between 16 and 19, inclusive.


<<>>=
sum(dbinom(16:19, size = 31, prob = 0.447))
diff(pbinom(c(19,15), size = 31, prob = 0.447, lower.tail = FALSE))
@

\item Give the mean of $X$, denoted $\E X$.


<<>>=
library(distrEx)
X = Binom(size = 31, prob = 0.447)
E(X)
@

\item Give the variance of $X$.


<<>>=
var(X)
@

\item Give the standard deviation of $X$.


<<>>=
sd(X)
@

\item Find $\E(4X+51.324)$


<<>>=
E(4*X + 51.324)
@

\end{enumerate}
\end{enumerate}
<<echo = FALSE, results = hide>>=
rnorm(1)
@
\begin{xca}
For the following situations, decide what the distribution of $X$
should be. In nearly every case, there are additional assumptions
that should be made for the distribution to apply; identify those
assumptions (which may or may not hold in practice.)\end{xca}
\begin{enumerate}
\item We shoot basketballs at a basketball hoop, and count the number of
shots until we make a goal. Let $X$ denote the number of missed shots.
On a normal day we would typically make about 37\% of the shots.
\item In a local lottery in which a three digit number is selected randomly,
let $X$ be the number selected.
\item We drop a styrofoam cup to the floor twenty times, each time recording
whether the cup comes to rest perfectly right side up, or not. Let
$X$ be the number of times the cup lands perfectly right side up.
\item We toss a piece of trash at the garbage can from across the room.
If we miss the trash can, we retrieve the trash and try again, continuing
to toss until we make the shot. Let $X$ denote the number of missed
shots. 
\item Working for the border patrol, we inspect shipping cargo as when it
enters the harbor looking for contraband. A certain ship comes to
port with 557 cargo containers. Standard practice is to select 10
containers randomly and inspect each one very carefully, classifying
it as either having contraband or not. Let $X$ count the number of
containers that illegally contain contraband.
\item At the same time every year, some migratory birds land in a bush outside
for a short rest. On a certain day, we look outside and let $X$ denote
the number of birds in the bush.
\item We count the number of rain drops that fall in a circular area on
a sidewalk during a ten minute period of a thunder storm.
\item We count the number of moth eggs on our window screen.
\item We count the number of blades of grass in a one square foot patch
of land.
\item We count the number of pats on a baby's back until (she) burps.
\end{enumerate}


<<echo = FALSE, results = hide>>=
rnorm(1)
@
\begin{xca}
Find the constant $c$ so that the given function is a valid PDF of
a random variable $X$.\end{xca}
\begin{enumerate}
\item $f(x)=Cx^{n},\quad0<x<1$.
\item $f(x)=Cx\me^{-x},\quad0<x<\infty$.
\item $f(x)=\me^{-(x-C)},\quad7<x<\infty.$
\item $f(x)=Cx^{3}(1-x)^{2},\quad0<x<1.$
\item ${\displaystyle f(x)=C(1+x^{2}/4)^{-1}},\quad-\infty<x<\infty.$\end{enumerate}
\begin{xca}
Show that $\E(X-\mu)^{2}=\E X^{2}-\mu^{2}$. Hint: expand the quantity
$(X-\mu)^{2}$ and distribute the expectation on the resulting terms.
\end{xca}



\chapter{Continuous Distributions\label{cha:Continuous-Distributions}}

The focus of the last chapter was on random variables whose support
can be written down in a list of values (finite or countably infinite),
such as the number of successes in a sequence of Bernoulli trials.
Now we move to random variables whose support is a whole range of
values, say, an interval $(a,b)$. It is shown in later classes that
it is impossible to write all of the numbers down in a list; there
are simply too many of them. 

This chapter begins with continuous random variables and the associated
PDFs and CDFs The continuous uniform distribution is highlighted,
along with the Gaussian, or normal, distribution. Some mathematical
details pave the way for a catalogue of models.

What do I want them to know?

want to introduce quantile functions somewhere here


\section{Continuous Random Variables\label{sec:Continuous-Random-Variables}}


\subsection{Probability Density Functions\label{sub:Probability-Density-Functions}}

Continuous random variables have supports that look like\begin{equation}
S_{X}=[a,b]\mbox{ or }(a,b),\end{equation}
or unions of intervals of the above form. Examples of random variables
that are often taken to be continuous are:
\begin{itemize}
\item the height or weight of an individual,
\item other physical measurements such as the length or size of an object,
and
\item measurements of length of time are usually considered continuous.
\end{itemize}
Every continuous random variable $X$ has a \emph{probability density
function} (PDF) denoted $f_{X}$ associated with it%
\footnote{Not true. There are pathological random variables with no density
function. (This is one of the crazy things that can happen in the
world of Measure Theory). But in this book we will not get even close
to these anomolous beasts, and regardless it can be proved that the
CDF always exists.%
} that satisfies three basic properties:
\begin{enumerate}
\item $f_{X}(x)>0$ for $x\in S_{X}$,
\item $\int_{x\in S_{X}}f_{X}(x)\,\diff x=1$, and
\item $\P(X\in A)=\int_{x\in A}f_{X}(x)\:\diff x$, for an event $A\subset S_{X}$.\label{enu: contrvcond3}\end{enumerate}
\begin{rem}
We can say the following about continuous random variables:
\begin{itemize}
\item Usually, the set $A$ in \ref{enu: contrvcond3} takes the form of
an interval, for example, $A=[c,d]$, in which case\begin{equation}
\P(X\in A)=\int_{c}^{d}f_{X}(x)\:\diff x.\end{equation}

\item It follows that the probability that $X$ falls in a given interval
is simply the \emph{area under the curve }of $f_{X}$ over the interval.
\item Since the area of a line $x=c$ in the plane is zero, then for a continuous
r.v., $\P(X=c)=0$ for any value $c$. In other words, the chance
that $X$ equals a particular value $c$ is zero, and this is true
for any number $c$. Therefore, when $a<b$ all of the following probabilities
are the same: \begin{equation}
\P(a\leq X\leq b)=\P(a<X\leq b)=\P(a\leq X<b)=\P(a<X<b).\end{equation}

\item The PDF $f_{X}$ can sometimes be greater than 1. This is in contrast
to the discrete case; every nonzero value of a PMF is a probability
and is restricted to lie in the interval $[0,1]$.
\end{itemize}
\end{rem}
We met the cumulative distribution function, $F_{X}$, in Chapter
BLANK. Recall that it is defined by $F_{X}(t)=\P(X\leq t)$. While
in the discrete case the CDF is unwieldly, in the continuous case
the CDF has a relatively convenient form:\begin{equation}
F_{X}(t)=\P(X\leq t)=\int_{-\infty}^{t}f_{X}(x)\:\diff x,\quad-\infty<t<\infty.\end{equation}
We know that all PDFs satisfy certain properties, and a similar statement
may be made for CDFs. In particular, any continuous CDF $F_{X}$ satisfies
\begin{itemize}
\item $F_{X}$ is nondecreasing , that is, $t_{1}\leq t_{2}$ implies $F_{X}(t_{1})\leq F_{X}(t_{2})$.
\item $F_{X}$ is continuous (see Appendix BLANK). Note the distinction
from the discrete case: CDFs of discrete random variables are not
continuous, they are only right continuous.
\item $\lim_{t\to-\infty}F_{X}(t)=0$ and $\lim_{t\to\infty}F_{X}(t)=1$.
\end{itemize}
For continuous random variables, there is a convenient relationship
between the CDF and PDF. Consider the derivative of $F_{X}$:\begin{equation}
F'_{X}(t)=\frac{\diff}{\diff t}F_{X}(t)=\frac{\diff}{\diff t}\,\int_{-\infty}^{t}f_{X}(x)\,\diff x=f_{X}(t),\end{equation}
the last equality being true by the Fundamental Theorem of Calculus,
part (2) (see Appendix BLANK). In short, $(F_{X})'=f_{X}$ in the
continuous case%
\footnote{In the discrete case, $f_{X}(x)=F_{X}(x)-\lim_{t\to x^{-}}F_{X}(t)$.%
}. 


\subsection{Expectation of Continuous Random Variables\label{sub:Expectation-of-Continuous}}

For a continuous random variable $X$ the expected value of $g(X)$
is \begin{equation}
\E g(X)=\int_{x\in S}g(x)f_{X}(x)\:\diff x.\end{equation}
One important example is the mean $\mu$, also known as $\E X$:\begin{equation}
\mu=\E X=\int_{x\in S}xf_{X}(x)\:\diff x.\end{equation}
Also there is the variance\begin{equation}
\sigma^{2}=\E(X-\mu)^{2}=\int_{x\in S}(x-\mu)^{2}f_{X}(x)\,\diff x,\end{equation}
which can be computed with the alternate formula $\sigma^{2}=\E X^{2}-(\E X)^{2}$.
In addition, there is the standard deviation $\sigma=\sqrt{\sigma^{2}}$.
The moment generating function is given by\begin{equation}
M_{X}(t)=\E\:\me^{tX}=\int_{-\infty}^{\infty}\me^{tx}f_{X}(x)\:\diff x,\end{equation}
provided the integral exists (is finite) for all $t$ in a neighborhood
of $t=0$.
\begin{example}
Let the random variable $X$ have PDF \[
f_{X}(x)=3x^{2},\quad0\leq x\leq1.\]
We will see later that such a PDF belongs to the \emph{Beta} family
of distributions. We can show that $\int_{-\infty}^{\infty}f(x)\diff x=1$.\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\diff x & =\int_{0}^{1}3x^{2}\:\diff x\\
 & =\left.x^{3}\right|_{x=0}^{1}\\
 & =1^{3}-0^{3}\\
 & =1.\end{align*}
This being said, we may find $\P(0.14\leq X<0.71)$.\begin{align*}
\P(0.14\leq X<0.71) & =\int_{0.14}^{0.71}3x^{2}\diff x,\\
 & =\left.x^{3}\right|_{x=0.14}^{0.71}\\
 & =0.71^{3}-0.14^{3}\\
 & \approx0.355167.\end{align*}
We can find the mean and variance in an identical manner.\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\diff x & =\int_{0}^{1}x\cdot3x^{2}\:\diff x,\\
 & =\frac{3}{4}x^{4}|_{x=0}^{1},\\
 & =\frac{3}{4}.\end{align*}
It would perhaps be best to calculate the variance with the shortcut
formula $\sigma^{2}=\E X^{2}-\mu^{2}$:
\end{example}
\begin{align*}
\E X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\diff x & =\int_{0}^{1}x^{2}\cdot3x^{2}\:\diff x\\
 & =\left.\frac{3}{5}x^{5}\right|_{x=0}^{1}\\
 & =3/5.\end{align*}
showing that $\sigma^{2}=3/5-(3/4)^{2}=3/80$.


\begin{example}
Let the random variable $X$ have PDF \[
f_{X}(x)=\frac{3}{x^{4}},\quad x>1.\]
Then we can show that $\int_{-\infty}^{\infty}f(x)\diff x=1$:\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\diff x & =\int_{1}^{\infty}\frac{3}{x^{4}}\:\diff x\\
 & =\lim_{t\to\infty}\int_{1}^{t}\frac{3}{x^{4}}\:\diff x\\
 & =\lim_{t\to\infty}\ \left.3\,\frac{1}{-3}x^{-3}\right|_{x=1}^{t}\\
 & =-\left(\lim_{t\to\infty}\frac{1}{t^{3}}-1\right)\\
 & =1.\end{align*}
This being said, we may find $\P(3.4\leq X<7.1)$\begin{align*}
\P(3.4\leq X<7.1) & =\int_{3.4}^{7.1}3x^{-4}\diff x\\
 & =\left.3\,\frac{1}{-3}x^{-3}\right|_{x=3.4}^{7.1}\\
 & =-1(7.1^{-3}-3.4^{-3})\\
 & \approx0.0226487123.\end{align*}
We can find the mean and variance in an identical manner.\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\diff x & =\int_{1}^{\infty}x\cdot\frac{3}{x^{4}}\:\diff x\\
 & =\left.3\,\frac{1}{-2}x^{-2}\right|_{x=1}^{\infty}\\
 & =-\frac{3}{2}\left(\lim_{t\to\infty}\frac{1}{t^{2}}-1\right)\\
 & =\frac{3}{2}.\end{align*}
It would perhaps be best to calculate the variance with the shortcut
formula $\sigma^{2}=\E X^{2}-\mu^{2}$:\begin{align*}
\E X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\diff x & =\int_{1}^{\infty}x^{2}\cdot\frac{3}{x^{4}}\:\diff x\\
 & =\left.3\:\frac{1}{-1}x^{-1}\right|_{x=1}^{\infty}\\
 & =-3\left(\lim_{t\to\infty}\frac{1}{t^{2}}-1\right)\\
 & =3,\end{align*}
showing that $\sigma^{2}=3-(3/2)^{2}=3/4$.
\end{example}

\subsection{How to do it with \textsf{R}}

There exist utilities to calculate probabilities and expectations
for general continuous random variables, but it is better to find
a built-in model, if possible. Sometimes it is not possible. We show
how to do it the long way, and the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!distr!\inputencoding{utf8}
package way.
\begin{example}
Let $X$ have PDF $f(x)=3x^{2}$, $0<x<1$ and find $\P(0.14\leq X\leq0.71)$.

<<>>=
f <- function(x) 3*x^2
integrate(f, lower = 0.14, upper = 0.71)
@

Compare this to the answer we found in Example BLANK. We could integrate
the function $xf(x)=$ \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!3*x^3!\inputencoding{utf8}
from zero to one to get the mean, and use the shortcut $\sigma^{2}=\E X^{2}-\left(\E X\right)^{2}$
for the variance. 

\end{example}

\begin{example}
Let $X$ have PDF $f(x)=3/x^{4}$, $x>1$. We may integrate the function
$xf(x)=$ \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!3/x^3!\inputencoding{utf8}
from zero to infinity to get the mean of $X$.

<<>>=
g <- function(x) 3/x^3
integrate(g, lower = 1, upper = Inf)
@

Compare this to the answer we got in Example BLANK. Use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!-Inf!\inputencoding{utf8}
for $-\infty$.

\end{example}

\begin{example}
Let us redo Example BLANK with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!distr!\inputencoding{utf8}
package. The method is similar to Example BLANK in Chapter BLANK.
We define an absolutely continuous random variable:

<<>>=
library(distr)
f <- function(x) 3*x^2
X <- AbscontDistribution(d = f, low1 = 0, up1 = 1)
p(X)(0.71) - p(X)(0.14)
@

Compare this answer to what we found in Example BLANK. Now let us
try expectation with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!distrEx!\inputencoding{utf8}
package:

<<>>=
library(distrEx)
E(X)
var(X)
3/80
@

Compare these answers to the ones we found in Example BLANK. Why are
they different? Because the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,showstringspaces=false]!distrEx!\inputencoding{utf8}
package resorts to numerical methods when it encounters a model it
does not recognize. This means that the answers we get for calculations
may not exactly match the theoretical values. Be careful.

\end{example}



\section{The Continuous Uniform Distribution\label{sec:The-Continuous-Uniform}}

A random variable $X$ with the continous uniform distribution on
the interval $(a,b)$ has PDF\begin{equation}
f_{X}(x)=\frac{1}{b-a},\quad a<x<b.\end{equation}
The associated \textsf{R} function is $\mathsf{dunif}(\mathtt{min}=a,\,\mathtt{max}=b)$.
We write $X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)$. Due
to the particularly simple form of this PDF we can also write down
explicitly a formula for the CDF $F_{X}$:\begin{equation}
F_{X}(t)=\begin{cases}
0, & t<0,\\
\frac{t-a}{b-a}, & a\leq t<b,\\
1, & t\geq b.\end{cases}\end{equation}


The continuous uniform distribution is the continuous analogue of
the discrete uniform distribution; it is used to model experiments
whose outcome is an interval of numbers that are {}``equally likely''
in the sense that any two intervals of equal length in the support
have the same probability associated with them.
\begin{example}
Choose a number in {[}0,1{]} at random, and let $X$ be the number
chosen. Then $X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)$. 
\end{example}
The mean of $X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)$
is relatively simple to calculate:

\begin{align*}
\mu=\E X & =\int_{-\infty}^{\infty}xf_{X}(x)\diff x,\\
 & =\int_{a}^{b}x\frac{1}{b-a}\diff x,\\
 & =\frac{1}{b-a}\ \frac{x^{2}}{2}|_{x=a}^{b},\\
 & =\frac{1}{b-a}\ \frac{b^{2}-a^{2}}{2},\\
 & =\frac{b+a}{2},\end{align*}
using the popular formula for the difference of squares. The variance
of a the $\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)$ distribution
is left to Exercise BLANK.


\section{The Normal Distribution\label{sec:The-Normal-Distribution}}

Has PDF\begin{equation}
f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left[\frac{-(x-\mu)^{2}}{2\sigma^{2}}\right],\quad-\infty<x<\infty.\end{equation}


The associated \textsf{R} function is \textsf{dnorm}\texttt{(x, mean}
\texttt{\textcolor{red}{=}} \texttt{0, sd} \texttt{\textcolor{red}{=}}
\texttt{1)}. We write $X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$.
The familiar bell-shaped curve, the normal distribution is also called
the Gaussian distribution because the German mathematician C.~F.~Gauss
largely contributed to its mathematical development. This distribution
is by far the most important distribution, continuous or discrete.
The normal model is observed to match the results of measured quantities
in nature.

Special case: when $\mu=0$ and $\sigma=1$ the random variable is
said to have a \emph{standard normal} distribution and we write $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$.
The lowercase greek letter phi ($\phi$) is used to denote the standard
normal PDF and the capital greek letter phi $\Phi$ is used to denote
the CDF: for $-\infty<z<\infty$,\begin{equation}
\phi(z)=\frac{1}{\sqrt{2\pi}}\,\me^{-z^{2}/2},\quad\Phi(t)=\int_{-\infty}^{t}\phi(z)\,\diff z\end{equation}
 
\begin{prop}
If $X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ then
\begin{equation}
Z=\frac{X-\mu}{\sigma}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).\end{equation}

\end{prop}
The MGF of $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
is relatively easy to derive:\begin{eqnarray*}
M_{Z}(t) & = & \int_{-\infty}^{\infty}\me^{tz}\frac{1}{\sqrt{2\pi}}\me^{-z^{2}/2}\diff z,\\
 & = & \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{1}{2}\left(z^{2}+2tz+t^{2}\right)+\frac{t^{2}}{2}\right\} \diff z,\\
 & = & \me^{t^{2}/2}\left(\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\me^{-[z-(-t)]^{2}/2}\diff z\right),\end{eqnarray*}
and the last integral is in the parentheses is equal to 1, since it
is the integral of a $\mathsf{norm}(\mathtt{mean}=-t,\,\mathtt{sd}=1)$
density. Therefore,\begin{equation}
M_{Z}(t)=\me^{-t^{2}/2},\quad-\infty<t<\infty.\end{equation}



\begin{example}
The MGF of $X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$
is then not difficult either because \[
Z=\frac{X-\mu}{\sigma},\mbox{ or rewriting, }X=\sigma Z+\mu.\]
Therefore: \[
M_{X}(t)=\E\me^{tX}=\E\me^{t(\sigma Z+\mu)}=\E\me^{\sigma tX}\me^{\mu}=\me^{t\mu}M_{Z}(\sigma t),\]
and we know that $M_{Z}(t)=\me^{t^{2}/2}$, thus substituting we get\[
M_{X}(t)=\me^{t\mu}\me^{(\sigma t)^{2}/2}=\exp\left\{ \mu t+\sigma^{2}t^{2}/2\right\} ,\]
for $-\infty<t<\infty$.\end{example}
\begin{fact}
The same argument above shows that if $X$ has MGF $M_{X}(t)$ then
the MGF of $Y=a+bX$ is \begin{equation}
M_{Y}(t)=\me^{ta}M_{X}(bt).\end{equation}

\end{fact}

\begin{example}
The 68-95-99.7 Rule. We saw in Section BLANK that if an empirical
distribution is approximately mound shaped, then there are specific
proportions of the observations which fall at varying distances from
the (sample) mean. We can see where these come from -- and obtain
more precise proportions -- with the following:
\end{example}
<<>>=
pnorm(1:3)-pnorm(-(1:3))
@


\begin{example}
Let the random experiment consist of a person taking an IQ test, and
let $X$ be the score on the test. The scores on such a test are typically
standardized to have a mean of 100 and a standard deviation of 15.
What is $\P(85\leq X\leq115)$?
\end{example}

\subsection{Normal Quantiles and the Quantile Function}

Now that we have experience with the pdistr family of functions, we
can solve virtually any problem.

Until now, we have been given two values and our task has been to
find the area under the PDF between those values. In this section,
we go in reverse: we are given an area, and we would like to find
the value(s) that correspond to that area. 

What is the lowest possible IQ score that a person can have, and still
be in the top 1\% of all IQ scores?

Introduce Given area, find the value

The definition of the quantile function%
\footnote{The precise definition of the quantile function is $Q_{X}(p)=\inf\left\{ x:\ F_{X}(x)\geq p\right\} $,
so that at least it is well defined (though perhaps infinite) for
the values $p=0$ and $p=1$.%
} is related to the inverse of the cumulative distribution function:\begin{equation}
Q_{X}(p)=\min\left\{ x:\ F_{X}(x)\geq p\right\} ,\quad0<p<1.\end{equation}

\begin{rem}
Here are some properties of quantile functions:
\begin{enumerate}
\item The quantile function is defined and finite for all $0<p<1$.
\item $Q_{X}$ is left-continuous (see Appendix BLANK). For discrete random
variables it is a step function, and for continuous random variables
it is a continuous function.
\item In the continuous case the graph of $Q_{X}$ may be obtained by reflecting
the graph of $F_{X}$ about the line $y=x$. In the discrete case,
before reflecting one should: 1) connect the dots to get rid of the
jumps -- this will make the graph look lik a set of stairs, 2) erase
the horizontal lines so that only vertical lines remain, and finally
3) swap the open circles with the solid dots. Please see Figure BLANK
for a comparison. 
\item The two limits \[
\lim_{p\to0^{+}}Q_{X}(p)\quad\mbox{and}\quad\lim_{p\to1^{-}}Q_{X}(p)\]
always exist, but may be infinite (that is, sometimes $\lim_{p\to0}Q(p)=-\infty$
and/or $\lim_{p\to1}Q(p)=\infty$).
\end{enumerate}
\end{rem}

\subsection{How to do it with \textsf{R}}

Use the q prefix to the distributions. Note that for the ECDF the
quantile function is exactly the $Q_{x}(p)=$\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!quantile(x, probs = !\inputencoding{utf8}$p$
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!, type = 1)!\inputencoding{utf8}
function. 
\begin{defn}
The symbol $z_{\alpha}$ denotes the value satisfying the equation
$\P(Z>z_{\alpha})=\alpha$, where $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$.
It can be calculated in one of two equivalent ways: \inputencoding{latin9}\lstinline!qnorm(!\inputencoding{utf8}$1-\alpha$\inputencoding{latin9}\lstinline!)!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline!qnorm(!\inputencoding{utf8}$\alpha$\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!, lower.tail = FALSE)!\inputencoding{utf8}.
\end{defn}

\section{Functions of Continuous Random Variables}

The goal of this section is to study methods for determining the distribution
of $U=g(X)$ based on the distribution of $X$. In the discrete case
all we needed to do was back substitute for $x=g^{-1}(u)$ in the
PMF of $X$ (sometimes accumulating probability mass along the way).
But the continuous case we are required to be somewhat more sophisticated
in our efforts. Now would be a good time to review Appendix BLANK.


\subsection{The PDF Method}
\begin{prop}
Let $X$ have PDF $f_{X}$ and let $g$ be a function which is one-to-one
with a differentiable inverse $g^{-1}$. Then the PDF of $U=g(X)$
is given by\begin{equation}
f_{U}(u)=f_{X}\left[g^{-1}(u)\right]\ \left|\frac{\diff}{\diff u}g^{-1}(u)\right|.\end{equation}
\end{prop}
\begin{rem}
There are a few tricks that can help when changing variables:
\begin{itemize}
\item The formula in Equation BLANK is nice, but does not really make any
sense. It is better to write in the intuitive form\begin{equation}
f_{U}(u)=f_{X}(x)\left|\frac{\diff x}{\diff u}\right|.\end{equation}

\end{itemize}
\end{rem}
\begin{example}
Let $X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$,
and let $Y=\me^{X}$. What is the PDF of $Y$? 

Solution: Notice first that $\me^{x}>0$ for any $x$, so the support
of $Y$ is $(0,\infty)$. Since the transformation is monotone, we
can solve $y=\me^{x}$ for $x$ to get $x=\ln\, y$, giving $\diff x/\diff y=1/y$.
Therefore, for any $y>0$,\[
f_{Y}(y)=f_{X}(\ln y)\cdot\left|\frac{1}{y}\right|=\frac{1}{\sigma\sqrt{2\pi}}\exp\left\{ \frac{(\ln y-\mu)^{2}}{2\sigma^{2}}\right\} \cdot\frac{1}{y},\]
where we have dropped the absolute value bars since $y>0$. The random
variable $Y$ is said to have a \emph{lognormal distribution}; see
Section BLANK.
\end{example}

\begin{example}
Suppose $X\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ and
let $Y=4-3X$. What is the PDF of $Y$?
\end{example}
The support of $X$ is $(-\infty,\infty)$, and as $x$ goes from
$-\infty$ to $\infty$, the quantity $y=4-3x$ also traverses $(-\infty,\infty)$.
Solving for $x$ in the equation $y=4-3x$ yields $x=-(y-4)/3$ giving
$\diff x/\diff y=-1/3$. And since\[
f_{X}(x)=\frac{1}{\sqrt{2\pi}}\me^{-x^{2}/2},\quad-\infty<x<\infty,\]
 we have \begin{eqnarray*}
f_{Y}(y) & = & f_{X}\left(\frac{y-4}{3}\right)\cdot\left|-\frac{1}{3}\right|,\quad-\infty<y<\infty,\\
 & = & \frac{1}{3\sqrt{2\pi}}\me^{-(y-4)^{2}/2\cdot3^{2}},\quad-\infty<y<\infty.\end{eqnarray*}
We recognize the PDF of $Y$ to be that of a $\mathsf{norm}(\mathtt{mean}=4,\,\mathtt{sd}=3)$
distribution. Indeed, we may use an identical argument as the above
to prove the following fact:
\begin{fact}
If $X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ and
if $Y=a+bX$ for constants $a$ and $b$, with $b\neq0$, then $Y\sim\mathsf{norm}(\mathtt{mean}=a+b\mu,\,\mathtt{sd}=|b|\sigma)$. 
\end{fact}
Note that it is sometimes easier to \emph{postpone} solving for the
inverse transformation $x=x(u)$. Instead, leave the transformation
in the form $u=u(x)$ and calculate the derivative of the \emph{original}
transformation\begin{equation}
\diff u/\diff x=g'(x).\end{equation}
Once this is known, we can get the PDF of $U$ with\begin{equation}
f_{U}(u)=f_{X}(x)\left|\frac{1}{\diff u/\diff x}\right|.\end{equation}
In many cases there are cancellations and the work is shorter. Of
course, it is not always true that\begin{equation}
\frac{\diff x}{\diff u}=\frac{1}{\diff u/\diff x},\end{equation}
but for the well-behaved examples in this book the trick works just
fine.
\begin{rem}
In the case that $g$ is not monotone we cannot apply Proposition
BLANK directly. However, hope is not lost. Rather, we break the support
of $X$ into pieces such that $g$ is monotone on each one. We apply
Proposition BLANK on each piece, and finish up by adding the results
together.
\end{rem}

\subsection{The CDF method}

We know from Section BLANK that $f_{X}=F_{X}'$ in the continuous
case. Starting from the equation $F_{Y}(y)=\P(Y\leq y)$, we may substitute
$g(X)$ for $Y$, then solve for $X$ to obtain $\P[X\leq g^{-1}(y)]$,
which is just another way to write $F_{X}[g^{-1}(y)]$. Differentiating
this last quantity with respect to $y$ will yield the PDF of $Y$.
\begin{example}
Suppose $X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)$ and
suppose that we let $Y=-\ln\, X$. What is the PDF of $Y$?

The support set of $X$ is $(0,1),$ and $y$ traverses $(0,\infty)$
as $x$ ranges from $0$ to $1$, so the support set of $Y$ is $S_{Y}=(0,\infty)$.
For any $y>0$, we consider\[
F_{Y}(y)=\P(Y\leq y)=\P(-\ln\, X\leq y)=\P(X\geq\me^{-y})=1-\P(X<\me^{-y}),\]
where the next to last equality follows because the exponential function
is \emph{monotone} (this point will be revisited later). Now since
$X$ is continuous the two probabilities $\P(X<\me^{-y})$ and $\P(X\leq\me^{-y})$
are equal; thus\[
1-\P(X<\me^{-y})=1-\P(X\leq\me^{-y})=1-F_{X}(\me^{-y}).\]
Now recalling that the CDF of a $\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)$
random variable satisfies $F(u)=u$ (see Equation BLANK), we can say
\[
F_{Y}(y)=1-F_{X}(\me^{-y})=1-\me^{-y},\quad\mbox{for }y>0.\]
We have consequently found the formula for the CDF of $Y$; to obtain
the PDF $f_{Y}$ we need only differentiate $F_{Y}$:\[
f_{Y}(y)=\frac{\diff}{\diff y}\left(1-\me^{-y}\right)=0-\me^{-y}(-1),\]
or $f_{Y}(y)=\me^{-y}$ for $y>0$. This turns out to be a member
of the exponential family of distributions, see Section BLANK. 
\end{example}

\begin{example}
\textbf{\emph{The Probability Integral Transform}}. Given a continuous
random variable $X$ with strictly increasing CDF $F_{X}$, let the
random variable $Y$ be defined by $Y=F_{X}(X)$. Then the distribution
of $Y$ is $\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)$. 

We can see why this is true by employing the CDF method. Note that
the support of $Y$ is $(0,1)$. And for any $0<y<1$,\[
F_{Y}(y)=\P(Y\leq y)=\P(F_{X}(X)\leq y).\]
Now since $F_{X}$ is strictly increasing, it has a well defined inverse
function $F_{X}^{-1}$. Therefore\[
\P(F_{X}(X)\leq y)=\P(X\leq F_{X}^{-1}(y))=F_{X}[F_{X}^{-1}(y)]=y.\]
Summarizing, we have seen that $F_{Y}(y)=y$, $0<y<1$. But this is
exactly the CDF of a $\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)$
random variable. \end{example}
\begin{fact}
The Probability Integral Transform is true for all continuous random
variables with continuous CDFs, not just for those with strictly increasing
CDFs (but the proof is more complicated). The transform is \textbf{not}
true for discrete random variables, or for continuous random variables
having a discrete component (that is, with jumps in their CDF). \end{fact}
\begin{example}
Let $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ and let
$U=Z^{2}$. What is the PDF of $U$? 

Notice first that $Z^{2}\geq0$, and thus the support of $U$ is $[0,\infty)$.
And for any $u\geq0$, \[
F_{U}(u)=\P(U\leq u)=\P(Z^{2}\leq u).\]
But $Z^{2}\leq u$ occurs if and only if $-\sqrt{u}\leq Z\leq\sqrt{u}$.
The last probability above is simply the area under the standard normal
PDF from $-\sqrt{u}$ to $\sqrt{u}$, and since $\phi$ is symmetric
about 0, we have\[
\P(Z^{2}\leq u)=2\P(0\leq Z\leq\sqrt{u})=2\left[F_{Z}(\sqrt{u})-F_{Z}(0)\right]=2\Phi(\sqrt{u})-1,\]
since $\Phi(0)=1/2$. To find the PDF of $U$, we differentiate the
CDF, remembering that $\Phi'=\phi$.\[
f_{U}(u)=\left(2\Phi(\sqrt{u})-1\right)'=2\phi(\sqrt{u})\cdot\frac{1}{2\sqrt{u}}=u^{-1/2}\phi(\sqrt{u}).\]
Substituting,\[
f_{U}(u)=u^{-1/2}\frac{1}{\sqrt{2\pi}}\,\me^{-(\sqrt{u})^{2}/2}=(2\pi u)^{-1/2}\me^{-u},\quad u>0.\]
This is later what we will call a \emph{chi-square distribution with
1 degree of freedom}. See Section BLANK. 
\end{example}

\subsection{How to do it with R}

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
package has functionality to investigate transformations of univariate
distributions. There are exact results for ordinary transformations
of the standard distributions, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
takes advantage of these in many cases. For instance, the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
package can handle the transformation in Example BLANK quite nicely:

<<>>=
library(distr)
X <- Norm(mean = 0, sd = 1)
Y <- 4 - 3*X
Y
@

So \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
{}``knows'' that a linear transformation of a normal random variable
is again normal, and it even knows what the correct \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mean!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sd!\inputencoding{utf8}
should be. But it is impossible for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
to know everything, and it is not long before we venture outside of
the transformations that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
recognizes. Let us try Example BLANK:

<<>>=
Z <- exp(X)
Z
@

The result is an object of class \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!AbscontDistribution!\inputencoding{utf8},
which is one of the classes that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
uses to denote general distributions that it does not recognize (recall
that $Z$ has a lognormal distribution). A simplified description
of the process that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
undergoes when it encounters a transformation $Y=g(X)$ that it does
not recognize is:
\begin{enumerate}
\item Randomly generate many, many copies $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$ from the distribution of $X$,
\item Compute $Y_{1}=g(X_{1})$, $Y_{2}=g(X_{2})$, \ldots{}, $Y_{n}=g(X_{n})$
and store them for use.
\item Calculate the PDF, CDF, quantiles, and random variates using the simulated
values of $Y$.
\end{enumerate}
As long as the transformation is sufficiently nice, such as a linear
transformation, the exponential, absolute value, \emph{etc.}, the
d-p-q functions are calculated analytically based on the d-p-q functions
associated with $X$. But if we try a crazy transformation then we
are greeted by a warning:

<<>>=
W <- sin(exp(X) + 27)
W
@

The warning confirms that the d-p-q functions are not calculated analytically,
but are instead based on the randomly simulated values of $Y$. \emph{We
must be careful to remember this.} The nature of random simulation
means that we can get different answers to the same question: watch
what happens when we compute $\P(W\leq0.5)$ using the $W$ above,
then define $W$ again, and compute the (supposedly) same $\P(W\leq0.5)$
a few moments later.

<<>>=
p(W)(0.5)
W <- sin(exp(X) + 27)
p(W)(0.5)
@

The answers are not the same! Furthermore, if we repeated the process
we would get yet another answer for $\P(W\leq0.5)$. 

The answers were close, though. And the underlying randomly generated
$X$'s were not the same so it should hardly be a surprise that the
calculated $W$'s were not the same, either. This serves as a warning
(in concert with the one that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!distr!\inputencoding{utf8}
provides) that we should be careful to remember that complicated transformations
computed by \textsf{R} are only approximate and may fluctuate slightly
due to the nature of the way the estimates are calculated.


\section{Other Continuous Distributions\label{sec:Other-Continuous-Distributions}}


\subsection{Waiting Time Distributions\label{sub:Waiting-Time-Distributions}}

In some experiments, the random variable being measured is the time
until a certain event occurs. For example, a quality control specialist
may be testing a manufactured product to see how long it takes until
it fails. An efficiency expert may be recording the customer traffic
at a retail store to streamline scheduling of staff. 


\subsection*{The Exponential Distribution\label{sub:The-Exponential-Distribution}}

We say that $X$ has an \emph{exponential distribution} and write
$X\sim\mathsf{exp}(\mathtt{rate}=\lambda)$. 

\begin{equation}
f_{X}(x)=\lambda\me^{-\lambda x},\quad x>0\end{equation}
The associated \textsf{R} function is $\mathsf{dexp}(\mathtt{x},\,\mathtt{rate=1})$.
The parameter $\lambda$ measures the rate of arrivals (to be described
later) and must be positive. The other functions are BLANK and give
the CDF, quantiles, and random variates, respectively. The CDF is
given by the formula\begin{equation}
F_{X}(t)=1-\me^{-\lambda t},\quad t>0.\end{equation}
The mean is $\mu=1/\lambda$ and the variance is $\sigma^{2}=1/\lambda^{2}$. 

The exponential distribution is closely related to the Poisson distribution;
let us see how. We already know that if customers arrive at a store
according to a Poisson process with rate $\lambda$ and $Y$ counts
the number of customers arriving in the time interval $[0,t)$, then
$Y\sim\mathsf{pois}(\mathtt{lambda}=\lambda t).$ Now consider a different
question: let us start our clock at time 0 and stop the clock when
the first customer arrives. Let $X$ be the length of this random
time interval. Then $X\sim\mathsf{exp}(\mathtt{rate}=\lambda)$. This
can be seen with the following argument:\begin{align*}
\P(X>t) & =\P(\mbox{first arrival after time \emph{t}})\\
 & =\P(\mbox{no events in [0,\emph{t})})\\
 & =\P(Y=0)\\
 & =\me^{-\lambda t},\end{align*}
from the PMF of a Poisson distribution with parameter $\lambda t$.
In other words, $\P(X\leq t)=1-\me^{-\lambda t}$, which is exactly
the CDF of an $\mathsf{exp}(\mathtt{rate}=\lambda)$ distribution. 

This distribution is said to be \emph{memoryless} because exponential
random variables \textquotedbl{}forget\textquotedbl{} how old they
are at every new instant. That is, the probability that we must wait
an additional five hours for a customer to arrive, given that we have
already waited seven hours, is exactly the probability that we needed
to wait five hours for a customer from the time the store opened.
In mathematical symbols, for any $s,t>0$,\[
\P(X>s+t\,|\, X>t)=\P(X>s).\]
See Exercise BLANK.


\subsection*{The Gamma Distribution\label{sub:The-Gamma-Distribution}}

This is a generalization of the exponential distribution. We say that
$X$ has a gamma distribution and write $X\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)$.
It has PDF\[
f_{X}(x)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\: x^{\alpha-1}\me^{-\lambda x},\quad x>0.\]


The associated \textsf{R} function is \textsf{dgamma}\texttt{(x, shape}
\texttt{\textcolor{red}{=}} \texttt{1, rate} \texttt{\textcolor{red}{=}}
\texttt{1)}. The other functions are

\textsf{\textbf{pgamma}}\texttt{(q, shape, rate, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{qgamma}}\texttt{(p, shape, rate, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{rgamma}}\texttt{(n, shape, scale)}

and these give the CDF, quantiles, and random variates, respectively.

Remarks
\begin{itemize}
\item If $\alpha=1$ then $X\sim\mathsf{exp}(\mathtt{rate}=\lambda)$
\item $\alpha$ is called the shape parameter and $\lambda$ is called the
rate parameter.
\end{itemize}
As a motivation for the gamma distribution, recall that if $X$ measures
the length of time until the first event in a Poisson process with
rate $\lambda$ then $X\sim\mathsf{exp}(\mathtt{rate}=\lambda)$.
If we let $Y$ measure the length of time until the $\alpha^{\text{th}}$
event occurs, then $Y\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)$.
\begin{example}
At a car wash, two customers arrive per hour on the average. We decide
to measure how long it takes until the third customer arrives.
\end{example}

\subsection{The Chi Square, Student's $t$, and Snedecor's $F$ Distributions\label{sub:The-Chi-Square-t-F}}


\subsection*{The Chi Square Distribution\label{sub:The-Chi-Square}}

This is an important special case of the $\mathsf{gamma}$ distribution.
Let $\alpha=p/2$ and $\lambda=1/2$. The PDF is given by\[
f_{X}(x)=\frac{1}{\Gamma(p/2)2^{p/2}}\: x^{p/2-1}\me^{-x/2},\quad x>0\]


The associated \textsf{R} function is \textsf{dchisq}\texttt{(x, df)}.
The other functions are

\textsf{\textbf{pchisq}}\texttt{(q, df, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{qchisq}}\texttt{(p, df, lower.tail} \texttt{\textcolor{red}{=}}
\texttt{TRUE)}~\\
\textsf{\textbf{rchisq}}\texttt{(n, df)}

and these give the CDF, quantiles, and random variates, respectively.
A random variable $X$ with p.d.f.\[
f_{X}(x)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{p/2-1}\me^{-x/2},\quad x>0,\]
is said to have a chi-square distribution with $p$ degrees of freedom.
We write $X\sim\mathsf{chisq}(\mathtt{df}=p)$. Just as before, there
are functions \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dchisq!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!pchisq!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!qchisq!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rchisq!\inputencoding{utf8},
which compute the p.d.f., c.d.f., quantiles, and generate random variates,
respectively. In an obvious notation we may define $\chi_{[\gamma]}^{2}(p)$.
There is a parameter \texttt{df} for the degrees of freedom. See Figure
\ref{dchisq}.

%
\begin{figure}
\caption{Chi-Square densities with various df\label{dchisq}}

\end{figure}

\begin{rem}
Here are some useful things to know about the chi-square distribution.
\begin{enumerate}
\item If $Z\sim\mathtt{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$, then $Z^{2}\sim\mathsf{chisq}(\mathtt{df}=1)$.
This is important when it comes time to find the distribution of the
sample variance, $S^{2}$. See Theorem BLANK in Section BLANK.
\item The chi-square distribution is supported on the positive $x$-axis,
with a right-skewed distribution.
\item The $\mathsf{chisq}(\mathtt{df}=p)$ distribution is the same as a
$\mathsf{gamma}(\mathtt{shape}=p/2,\,\mathtt{rate}=1/2)$ distribution.
\end{enumerate}
\end{rem}
Introduce $\chi_{\alpha}^{2}(df)$


\subsection*{Student's $t$ distribution\label{sub:Student's-t-distribution}}

A random variable $X$ with p.d.f.

\begin{equation}
f_{X}(x)=\frac{\Gamma\left[(r+1)/2\right]}{\sqrt{r\pi}\,\Gamma(r/2)}\left(1+\frac{x^{2}}{r}\right)^{-(r+1)/2},\quad-\infty<x<\infty\end{equation}
is said to have Student's $t$ distribution with $r$ \emph{degrees
of freedom} ($\mathtt{df}$), and we write $X\sim\mathsf{t}(\mathtt{df}=r)$.
The associated \textsf{R} function is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dt(x, df)!\inputencoding{utf8}.
The shape of the p.d.f. is similar to the normal, but the tails are
considerably heavier. See Figure \ref{cap:Student's-t-densities}.
As with the Normal distribution, there are four functions in \textsf{R}
associated with the $t$ distribution, namely \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dt!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!pt!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!qt!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rt!\inputencoding{utf8},
which compute the p.d.f., c.d.f., quantiles, and generate random variates,
respectively.

Similar to that done for the normal we may define $t_{[\gamma]}^{(df)}$
as the number on the $x$-axis such that there is exactly $\gamma$
area under the $t(df)$ curve to its right.
\begin{example}
We find $t_{\alpha}(df)$ with the quantile function:
\end{example}
<<>>=
qt(0.01, df = 23, lower.tail = FALSE)
@

Notice the \texttt{df} parameter.
\begin{rem}
We can say the following:
\begin{enumerate}
\item The $\mathsf{t}(\mathtt{df}=r)$ distribution looks just like a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
distribution, except it has heavier tails.
\item The $\mathsf{t}(\mathtt{df}=1)$ distribution is also known as a standard
{}``Cauchy distribution'', which is implemented in \textsf{R} with
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dcauchy!\inputencoding{utf8}
function and its relatives. The Cauchy distribution is quite pathological
and is often a counterexample to many famous results. 
\item The standard deviation of $\mathsf{t}(\mathtt{df}=r)$ is undefined
(that is, infinite) unless $r>2$. When $r$ is more than 2, the standard
deviation is always bigger than one, but decreases to 1 as $r\to\infty$.
\item The $\mathsf{t}(\mathtt{df}=r)$ distribution approaches a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
distribution as $r\to\infty$. 
\end{enumerate}
\end{rem}

\subsection*{Snedecor's $F$ distribution\label{sub:Fisher's-F-distribution}}

A random variable $X$ with p.d.f.

\begin{equation}
f_{X}(x)=\frac{\Gamma[(m+n)/2]}{\Gamma(m/2)\Gamma(n/2)}\left(\frac{m}{n}\right)^{m/2}x^{m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2},\quad x>0.\end{equation}
is said to have an $F$ distribution with $(m,n)$ degrees of freedom.
We write $X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)$. The
associated \textsf{R} function is \textsf{df}\texttt{(x, df1} \texttt{,
df2)}. Just as before, there are functions \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!df!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!pf!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!qf!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rf!\inputencoding{utf8},
which compute the p.d.f., c.d.f., quantiles, and generate random variates,
respectively. In an obvious notation we may define $F_{[\gamma]}^{(m,n)}$There
are parameters \texttt{df1} and \texttt{df2} for the $(m,n)$ degrees
of freedom.
\begin{rem}
~
\begin{enumerate}
\item If $X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)$, then $(1/X)\sim\mathsf{f}(\mathtt{df1}=n,\,\mathtt{df2}=m)$.
Historically, this fact was especially convenient. In the old days,
statisticians used printed tables for their statistical calculations.
Since the $F$ tables were symmetric in $m$ and $n$, it meant that
publishers could cut the size of their printed tables in half. It
plays less of a role today, now that personal computers are widespread.
\item If $X\sim\mathsf{t}(\mathtt{df}=r)$, then $X^{2}\sim\mathsf{f}(\mathtt{df1}=1,\,\mathtt{df2}=r)$.
\end{enumerate}
\end{rem}
Introduce $\mathsf{f}_{\alpha}(\mathtt{df})$

There is a common misconception that the $F$ distribution was discovered
and/or named by Sir R.~.A.~\emph{F}isher. The mistake is perhaps
plausible because the $F$ distribution plays a significant role in
the analysis of variance, and Fisher is widely credited with the invention
and development of ANOVA (although not with the acronym, that was
Tukey).

However, the truth of the matter is that G.~W.~Snedecor discovered,
tabulated, and yes, introduced the notation for, the $F$ distribution
in Calculation and Interpretation of Analysis of Variance and Covariance
(1934) (David, 1995). Fisher had tabulated $z=\frac{1}{2}\ln F$ some
years earlier, and objected to the idea of calling the variance ratio
{}``$F$''. 

\href{http://jeff560.tripod.com/f.html}{ajskldfjalsdfladsjlad}


\subsection{Other Popular Distributions\label{sub:Other-Popular-Distributions}}


\subsubsection*{The Cauchy Distribution\label{sub:The-Cauchy-Distribution}}

This is a special case of the Student's $t$ distribution. It has
PDF\begin{equation}
f_{X}(x)=\frac{1}{\beta\pi}\left[1+\left(\frac{x-m}{\beta}\right)^{2}\right]^{-1},\quad-\infty<x<\infty\end{equation}


We write $X\sim\mathsf{cauchy}(\mathtt{location}=m,\,\mathtt{scale}=\beta)$.
The associated \textsf{R} function is \textsf{dcauchy}\texttt{(x,
location = 0, scale = 1)}. It is easy to see that a $\mathsf{cauchy}(\mathtt{location}=0,\,\mathtt{scale}=1)$
distribution is the same as a $\mathsf{t}(\mathtt{df}=1)$ distribution.
In general the $\mathsf{cauchy}$ distribution looks like a $\mathsf{norm}$
distribution but with very heavy tails. The mean (and variance) do
not exist, that is, they are infinite. The median is represented by
the $\mathtt{location}$ parameter, and the $\mathtt{scale}$ parameter
influences the spread of the distribution about its median.


\subsubsection*{The Beta Distribution\label{sub:The-Beta-Distribution}}

This is a generalization of the continuous uniform distribution.

\begin{equation}
f_{X}(x)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\: x^{\alpha-1}(1-x)^{\beta-1},\quad0<x<1\end{equation}
We write $X\sim\mathsf{beta}(\mathtt{shape1}=\alpha,\,\mathtt{shape2}=\beta)$.
The associated \textsf{R} function is $\mathsf{dbeta}(\mathtt{x},\,\mathtt{shape1},\,\mathtt{shape2})$.
The mean and variance are\begin{equation}
\mu=\frac{\alpha}{\alpha+\beta},\quad\sigma^{2}=\frac{\alpha\beta}{\left(\alpha+\beta\right)^{2}\left(\alpha+\beta+1\right)}.\end{equation}
This distribution comes up a lot in Bayesian statistics because it
is a good model for one's prior beliefs about a population proportion
$p$, $0\leq p\leq1$.


\subsubsection*{The Logistic Distribution\label{sub:The-Logistic-Distribution}}

\begin{equation}
f_{X}(x)=\frac{1}{\sigma}\exp\left(-\frac{x-\mu}{\sigma}\right)\left[1+\exp\left(-\frac{x-\mu}{\sigma}\right)\right]^{-2},\quad-\infty<x<\infty.\end{equation}
We write $X\sim\mathsf{logis}(\mathtt{location}=\mu,\,\mathtt{scale}=\sigma)$.
The associated \textsf{R} function is $\mathsf{dlogis}(\mathtt{x},\,\mathtt{location=0},\,\mathtt{scale=1})$.
The logistic distribution comes up in differential equations as a
model for population growth under certain assumptions. The mean is
$\mu$ and the variance is $\pi^{2}\sigma^{2}/3$.


\subsubsection*{The Lognormal Distribution\label{sub:The-Lognormal-Distribution}}

This is a distribution derived from the normal distribution (hence
the name). If $U\sim\mathtt{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$,
then $X=\me^{U}$has PDF\begin{equation}
f_{X}(x)=\frac{1}{\sigma x\sqrt{2\pi}}\exp\left[\frac{-(\ln x-\mu)^{2}}{2\sigma^{2}}\right],\quad0<x<\infty.\end{equation}
We write $X\sim\mathsf{lnorm}(\mathtt{meanlog}=\mu,\,\mathtt{sdlog}=\sigma)$.
The associated \textsf{R} function is $\mathsf{dlnorm}(\mathtt{x},\,\mathtt{meanlog=0},\,\mathtt{sdlog=1})$.
Notice that the support is concentrated on the positive $x$ axis;
the distribution is right-skewed with a heavy tail.


\subsubsection*{The Weibull Distribution\label{sub:The-Weibull-Distribution}}

This has PDF

\begin{equation}
f_{X}(x)=\frac{\alpha}{\beta}\left(\frac{x}{\beta}\right)^{\alpha-1}\exp\left(\frac{x}{\beta}\right)^{\alpha},\quad x>0.\end{equation}
We write $X\sim\mathsf{weibull}(\mathtt{shape}=\alpha,\,\mathtt{scale}=\beta)$.
The associated \textsf{R} function is $X\sim\mathsf{weibull}(\mathtt{x},\,\mathtt{shape},\,\mathtt{scale=1})$.


\subsection{How to do it with \textsf{R}}

There is some support of moments and moment generating functions for
some continuous probability distributions included in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!actuar!\inputencoding{utf8}
package. The convention is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!m!\inputencoding{utf8}
in front of the distribution name for raw moments, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mgf!\inputencoding{utf8}
in front of the distribution name for the moment generating function.
At the time of this writing, the following distributions are supported:
gamma, inverse gaussian, (non-central) chi-squared, exponential, and
uniform.
\begin{example}
Calculate the first four raw moments for $X\sim\mathsf{gamma}(\mathtt{shape}=13,\,\mathtt{rate}=1)$
and plot the moment generating function.

We load the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!actuar!\inputencoding{utf8}
package and use the functions \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mgamma!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mgfgamma!\inputencoding{utf8}:
\end{example}
<<>>=
library(actuar)
mgamma(1:4, shape = 13, rate = 1)
@

For the plot we can use the function in the following form:

<<>>=
plot(function(x){mgfgamma(x, shape = 13, rate = 1)}, from=-0.1, to=0.1, ylab = "gamma mgf")
@

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 5>>=
plot(function(x){mgfgamma(x, shape = 13, rate = 1)}, from=-0.1, to=0.1, ylab = "gamma mgf")
@
\par\end{centering}

\caption{Plot of the \textsf{gamma}(\texttt{shape} = 13, \texttt{rate} = 1)
MGF \label{fig:gamma-mgf}}

\end{figure}



\section{Chapter Exercises}

\setcounter{thm}{0}


\begin{xca}
Find the constant $c$ so that the given function is a valid PDF of
a random variable $X$.\end{xca}
\begin{enumerate}
\item $f(x)=Cx^{n},\quad0<x<1$.
\item $f(x)=Cx\me^{-x},\quad0<x<\infty$.
\item $f(x)=\me^{-(x-C)},\quad7<x<\infty.$
\item $f(x)=Cx^{3}(1-x)^{2},\quad0<x<1.$
\item ${\displaystyle f(x)=C(1+x^{2}/4)^{-1}},\quad-\infty<x<\infty.$
\end{enumerate}


<<echo = FALSE, results = hide>>=
rnorm(1)
@
\begin{xca}
For the following random experiments, decide what the distribution
of $X$ should be. In nearly every case, there are additional assumptions
that should be made for the distribution to apply; identify those
assumptions (which may or may not strictly hold in practice).\end{xca}
\begin{enumerate}
\item We throw a dart at a dart board. Let $X$ denote the squared linear
distance from the bullseye to the where the dart landed.
\item We randomly choose a textbook from the shelf at the bookstore and
let $P$ denote the proportion of the total pages of the book devoted
to exercises. 
\item We measure the time it takes for the water to completely drain out
of the kitchen sink.
\item We randomly sample strangers at the grocery store and \end{enumerate}
\begin{xca}
If $Z$ is $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$, find 
\begin{enumerate}
\item $\P(Z>2.64)$


<<>>=
pnorm(2.64, lower.tail = FALSE)
@

\item $\P(0\leq Z<0.87)$


<<>>=
pnorm(0.87) - 1/2
@

\item $\P(|Z|>1.39)$ (Hint: draw a picture!)


<<>>=
2 * pnorm(-1.39)
@

\end{enumerate}
\end{xca}

\begin{xca}
Calculate the variance of $X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)$.
Hint: First calculate $\E X^{2}$.

type the exercise here
\end{xca}

\begin{xca}
Prove the memoryless property for exponential random variables. That
is, for $X\sim\mathsf{exp}(\mathtt{rate}=\lambda)$ show that for
any $s,t>0$,\[
\P(X>s+t\,|\, X>t)=\P(X>s).\]

\end{xca}





\chapter{Multivariate Distributions\label{cha:Multivariable-Distributions}}

We have built up quite a catologue of distributions: discrete and
continuous. They were all univariate, however, meaning that we only
considered one random variable at a time. We can nevertheless imagine
many random variables associated with a single person: their height,
their weight, their wrist circumference (all continuous), or their
eye/hair color, shoe size, whether they are right handed, left handed,
or ambidextrous (all categorical), and we can even surmise reasonable
probability distributions to associate with each of these variables. 

But there is a difference: for a single person, these variables are
related. For instance, a person's height betrays a lot of information
about that person's weight.

The concept we are hinting at is the notion of \emph{dependence} between
random variables. It is the focus of this chapter to study this concept
in some detail. Along the way, we will pick up additional models to
add to our catalogue. Moreover, we will study certain classes of dependence,
and clarify the special case when there is no dependence, namely,
independence.

What do I want them to know?
\begin{enumerate}
\item joint distributions and marginal distributions (discrete and continuous)
\item joint expectation and marginal expectation
\item covariance and correlation
\item conditional distributions and conditional expectation
\item independence and exchangeability
\item popular discrete joint distribution (multinomial)
\item popular continuous distribution (multivariate normal)
\end{enumerate}

\section{Joint and Marginal Probability Distributions\label{sec:Joint-Probability-Distributions}}

Consider two discrete random variables $X$ and $Y$ with PMFs $f_{X}$
and $f_{Y}$ that are supported on the sample spaces $S_{X}$ and
$S_{Y}$, respectively. Let $S_{X,Y}$ denote the set of all possible
observed \emph{pairs} $(x,y)$, called the \emph{joint support set}
of $X$ and $Y$. Then the \emph{joint probability mass function}
of $X$ and $Y$ is the function $f_{X,Y}$ defined by\begin{equation}
f_{X,Y}(x,y)=\P(X=x,\, Y=y),\quad\mbox{for }(x,y)\in S_{X,Y}.\end{equation}
Every joint PMF satisfies\begin{equation}
f_{X,Y}(x,y)>0\mbox{ for all }(x,y)\in S_{X,Y},\end{equation}
and\begin{equation}
\sum_{(x,y)\in S_{X,Y}}f_{X,Y}(x,y)=1.\end{equation}
It is customary to extend the function $f_{X,Y}$ to be defined on
all of $\R^{2}$ by setting $f_{X,Y}(x,y)=0$ for $(x,y)\not\in S_{X,Y}$. 

In the context of this chapter, the PMFs $f_{X}$ and $f_{Y}$ are
called the \emph{marginal PMFs} of $X$ and $Y$, respectively. If
we are given only the joint PMF then we may recover each of the marginal
PMFs by using the Theorem of Total Probability (see BLANK): observe\begin{eqnarray}
f_{X}(x) & = & \P(X=x),\\
 & = & \sum_{y\in S_{Y}}\P(X=x,\, Y=y),\\
 & = & \sum_{y\in S_{Y}}f_{X,Y}(x,y).\end{eqnarray}
By interchanging the roles of $X$ and $Y$ it is clear that \begin{equation}
f_{Y}(y)=\sum_{x\in S_{Y}}f_{X,Y}(x,y).\end{equation}
Given the joint PMF we may recover the marginal PMFs, but the converse
is not true. Even if we have \emph{both} marginal distributions they
are not sufficient to determine the joint PMF; more information is
needed%
\footnote{We are not at a total loss, however. There are Frechet bounds which
pose limits on how large (and small) the joint distribution must be
at each point.%
}. 

Associated with the joint PMF is the \emph{joint cumulative distribution
function} $F_{X,Y}$ defined by\[
F_{X,Y}(x,y)=\P(X\leq x,\, Y\leq y),\quad\mbox{for }(x,y)\in\R^{2}.\]
The bivariate joint CDF is not quite as tractable as the univariate
CDFs, but in principle we could calculate it by adding up quantities
of the form BLANK. The joint CDF is typically not used in practice
due to its inconvenient form; one can usually get by with the joint
PMF alone.

We now introduce some examples of bivariate discrete distributions.
The first we have seen before, and the second is based on the first.
\begin{example}
Roll a fair die twice. Let $X$ be the face shown on the first roll,
and let $Y$ be the face shown on the second roll. We have already
seen this example in Chapter BLANK, Example BLANK. For this example,
it suffices to define\[
f_{X,Y}(x,y)=\frac{1}{36},\quad x=1,\ldots,6,\ y=1,\ldots,6.\]
In this example the marginal PMFs are given by $f_{X}(x)=1/6$, $x=1,2,\ldots,6$,
and $f_{Y}(y)=1/6$, $y=1,2,\ldots,6$, since\[
f_{X}(x)=\sum_{y=1}^{6}\frac{1}{36}=\frac{1}{6},\quad x=1,\ldots,6,\]
and the same computation with the letters switched works for $Y$. 
\end{example}

\begin{example}
Let the random experiment again be to roll a fair die twice, except
now let us define the random variables $U$ and $V$ by\begin{eqnarray*}
U & = & \mbox{the maximum of the two rolls, and }\\
V & = & \mbox{the sum of the two rolls.}\end{eqnarray*}
We see that the support of $U$ is $S_{U}=\left\{ 1,2,\ldots,6\right\} $
and the support of $V$ is $S_{V}=\left\{ 2,3,\ldots,12\right\} $.
We may represent the sample space with a matrix, and for each entry
in the matrix we may calculate the value that $U$ assumes. The result
is in Table BLANK. 

We may do a similar thing for $V$; see Table BLANK.

%
\begin{table}
\begin{centering}
\begin{tabular}{c|cccccc}
max & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline
1 & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
2 & 2 & 2 & 3 & 4 & 5 & 6\tabularnewline
3 & 3 & 3 & 3 & 4 & 5 & 6\tabularnewline
4 & 4 & 4 & 4 & 4 & 5 & 6\tabularnewline
5 & 5 & 5 & 5 & 5 & 5 & 6\tabularnewline
6 & 6 & 6 & 6 & 6 & 6 & 6\tabularnewline
\end{tabular}~~~~~~~~~\begin{tabular}{c|cccccc}
sum & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline
1 & 2 & 3 & 4 & 5 & 6 & 7\tabularnewline
2 & 3 & 4 & 5 & 6 & 7 & 8\tabularnewline
3 & 4 & 5 & 6 & 7 & 8 & 9\tabularnewline
4 & 5 & 6 & 7 & 8 & 9 & 10\tabularnewline
5 & 6 & 7 & 8 & 9 & 10 & 11\tabularnewline
6 & 7 & 8 & 9 & 10 & 11 & 12\tabularnewline
\end{tabular}
\par\end{centering}

\caption{Table of }

\end{table}


%
\begin{table}
\begin{centering}
\begin{tabular}{c|cccccc}
(max, sum) & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline
1 & (1,2) & (2,3) & (3,4) & (4,5) & (5,6) & (6,7)\tabularnewline
2 & (2,3) & (2,4) & (3,5) & (4,6) & (5,7) & (6,8)\tabularnewline
3 & (3,4) & (3,5) & (3,6) & (4,7) & (5,8) & (6,9)\tabularnewline
4 & (4,5) & (4,6) & (4,7) & (4,8) & (5,9) & (6,10)\tabularnewline
5 & (5,6) & (5,7) & (5,8) & (5,9) & (5,10) & (6,11)\tabularnewline
6 & (6,7) & (6,8) & (6,9) & (6,10) & (6,11) & (6,12)\tabularnewline
\end{tabular}
\par\end{centering}

\caption{Table of }

\end{table}

\end{example}
In the examples above, and in many other ones, the joint support can
be written as a product set of the support of $X$ {}``times'' the
support of $Y$, that is, it may be represented as a cartesian product
set, or rectangle, $S_{X,Y}=S_{X}\times S_{Y}$, where $S_{X}\times S_{Y}=\left\{ (x,y):\ x\in S_{X},\, y\in S_{Y}\right\} $.
As we shall see presently in Section BLANK, this form is a necessary
condition for $X$ and $Y$ to be independent (or alternatively exchangeable
when $S_{X}=S_{Y}$). But please note that in general it is not required
for $S_{X,Y}$ to be of rectangle form. Any discrete set $S_{X,Y}$
in the plane which has total mass 1 is the joint support set for some
pair of random variables $(X,Y)$.

Now continuing the reasoning we used for the discrete case, given
two continuous random variables $X$ and $Y$ there similarly exists%
\footnote{Strictly speaking, the joint density function does not necessarily
exist. But the joint CDF always exists.%
} a function $f_{X,Y}(x,y)$ associated with $X$ and $Y$ called the
\emph{joint probability density function} of $X$ and $Y$. Every
joint PDF satisfies\begin{equation}
f_{X,Y}(x,y)\geq0\mbox{ for all }(x,y)\in S_{X,Y},\end{equation}
and\begin{equation}
\iintop_{S_{X,Y}}f_{X,Y}(x,y)\,\diff x\,\diff y=1.\end{equation}


In the continuous case we do not have such a simple interpretation
for the joint PDF; however, we do have one for the joint CDF, namely,\[
F_{X,Y}(x,y)=\P(X\leq x,\, Y\leq y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)\,\diff v\,\diff u,\]
for $(x,y)\in\R^{2}$. If $X$ and $Y$ have the joint PDF $f_{X,Y}$,
then the marginal density of $X$ may be recovered by\begin{equation}
f_{X}(x)=\int_{S_{Y}}f_{X,Y}(x,y)\,\diff y,\quad x\in S_{X}\end{equation}
and the marginal PDF of $Y$ may be found with \begin{equation}
f_{Y}(y)=\int_{S_{X}}f_{X,Y}(x,y)\,\diff x,\quad y\in S_{Y}.\end{equation}



\subsection{How to do it with \textsf{R}}

We will show how to do Example BLANK using \textsf{R}; it is much
simpler to do the example with \textsf{R} than without. First we set
up the sample space with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rolldie!\inputencoding{utf8}
function. Next, we add random variables $U$ and $V$ with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!addrv!\inputencoding{utf8}
function. We take a look at the very top of the data frame (probability
space) to make sure that everything is operating according to plan.

<<>>=
S <- rolldie(2, makespace = TRUE)
S <- addrv(S, FUN = max, invars = c("X1","X2"), name = "U")
S <- addrv(S, FUN = sum, invars = c("X1","X2"), name = "V")
head(S)
@

Yes, the $U$ and $V$ columns have been added to the data frame and
have been computed correctly. This result would be fine as it is,
but the data frame has too many rows: there are repeated pairs $(u,v)$
which show up as repeated rows in the data frame. The goal is to aggregate
the rows of $S$ such that the result has exactly one row for each
unique pair $(u,v)$ with positive probability. This sort of thing
is exactly the task for which the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!marginal!\inputencoding{utf8}
function was designed. We may take a look at the joint distribution
of $U$ and $V$.

<<>>=
UV <- marginal(S, vars = c("U", "V"))
head(UV)
xtabs(round(probs,3) ~ V + U, data = UV)
@

(We have only shown the first few rows of the joint distribution.
The complete data frame has 11 rows.) Note that we can continue the
process and examine the marginal distributions of $U$ and $V$ separately.
We need only submit the following:

<<>>=
marginal(UV, vars = "U")
head(marginal(UV, vars = "V"))
@

You should check that the answers that we have obtained exactly match
the same (somewhat laborious) calculations that we completed in Example
BLANK.


\section{Joint and Marginal Expectation}

Given a function $g$ with arguments $(x,y)$ we would like to know
the long-run average behavior of $g(X,Y)$ and how to mathematically
calculate it. Expectation in this context is computed in the pedestrian
way. We simply integrate (sum) with respect to the joint probability
density (mass) function.

\begin{equation}
\E\, g(X,Y)=\iintop_{S_{X,Y}}g(x,y)\, f_{X,Y}(x,y)\,\diff x\,\diff y,\end{equation}
or in the discrete case\begin{equation}
\E\, g(X,Y)=\mathop{\sum\sum}\limits _{(x,y)\in S_{X,Y}}g(x,y)\, f_{X,Y}(x,y).\end{equation}



\subsection{Covariance and Correlation}

There are two very special cases of joint expectation: the \emph{covariance}
and the \emph{correlation}. These are numeric measures which help
us quantify the dependence between $X$ and $Y$. 
\begin{defn}
The \emph{covariance} of $X$ and $Y$ is \begin{equation}
\mbox{Cov}(X,Y)=\E(X-\E X)(Y-\E Y).\end{equation}

\end{defn}
By the way, there is a shortcut formula for covariance which is almost
as handy as the shortcut for the variance:\begin{equation}
\mbox{Cov}(X,Y)=\E(XY)-(\E X)(\E Y).\end{equation}
The proof is left to Exercise BLANK.

The Pearson product moment correlation between $X$ and $Y$ is the
covariance between $X$ and $Y$ rescaled to fall in the interval
$[-1,1]$. It is formally defined by \begin{equation}
\mbox{Corr}(X,Y)=\frac{\mbox{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}}\end{equation}
The correlation is usually denoted by $\rho_{X,Y}$ or simply $\rho$
if the random variables are clear from context. There are some important
facts about the correlation coefficient:
\begin{enumerate}
\item The range of correlation is $-1\leq\rho_{X,Y}\leq1$.
\item Equality holds above ($\rho_{X,Y}=\pm1$) if and only if $Y$ is a
linear function of $X$ with probability one.
\end{enumerate}

\subsection{How to do it with R}

<<>>=
Eu <- sum(S$U*S$probs)
Ev <- sum(S$V*S$probs)
sum(S$U*S$V*S$probs)
sum(S$U*S$V*S$probs)-Eu*Ev
@


\section{Conditional Distributions\label{sec:Conditional-Distributions}}

If $x\in S_{X}$ is such that $f_{X}(x)>0$, then we may define the
conditional density of $ $$Y|\, X=x$, denoted $f_{Y|x}$, by \[
f_{Y|x}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)},\quad y\in S_{Y}.\]
We define $f_{X|y}$ in a similar fashion.
\begin{example}
Let the joint PMF of $X$ and $Y$ be given by
\end{example}

\begin{example}
Let the joint PDF of $X$ and $Y$ be given by
\end{example}

\subsection*{Bayesian Connection}

Conditional distributions play a fundamental role in Bayesian probability
and statistics. There is a parameter $\theta$ which is of primary
interest, and about which we would like to learn. But rather than
observing $\theta$ directly, we instead observe a random variable
$X$ whose probability distribution depends on $\theta$. Using the
information we provided by $X,$ we would like to update the information
that we have about $\theta$.

Our initial beliefs about $\theta$ are represented by a probability
distribution, called the \emph{prior distribution}, denoted by $\pi$.
The PDF $f_{X|\theta}$ is called the \emph{likelihood function},
also called the \emph{likelihood of} $X$ \emph{conditional on} $\theta$.
Given an observation $X=x$, we would like to update our beliefs $\pi$
to a new distribution, called the \emph{posterior distribution of}
$ $$\theta$ \emph{given the observation} $X=x$, denoted $\pi_{\theta|x}$.
It may seem a mystery how to obtain $\pi_{\theta|x}$ based only on
the information provided by $\pi$ and $f_{X|\theta}$, but it should
not be. We have already studied this in Chapter BLANK where it was
called Bayes' Rule:\begin{equation}
\pi(\theta|x)=\frac{\pi(\theta)\, f(x|\theta)}{\int\pi(u)\, f(x|u)\diff u}.\end{equation}
Compare the above expression to Equation BLANK.
\begin{example}
Suppose the parameter $\theta$ is the $\P(\mbox{Heads})$ for a biased
coin. It could be any value from 0 to 1. Perhaps we have some prior
information about this coin, for example, maybe we have seen this
coin before and we have reason to believe that it shows Heads less
than half of the time. Suppose that we represent our beliefs about
$\theta$ with a $\mathsf{beta}(\mathtt{shape1}=1,\,\mathtt{shape2}=3)$
prior distribution, that is, we assume \[
\theta\sim\pi(\theta)=3(1-\theta)^{2},\quad0<\theta<1.\]
To learn more about $\theta$, we will do what is natural: flip the
coin. We will observe a random variable $X$ which takes the value
$1$ if the coin shows Heads, and 0 if the coin shows Tails. Under
these circumstances, $X$ will have a Bernoulli distribution, and
in particular, $X|\theta\sim\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=\theta)$:\[
f_{X|\theta}(x|\theta)=\theta^{x}(1-\theta)^{1-x},\quad x=0,1.\]
Based on the observation $X=x$, we will update the prior distribution
to the posterior distribution, and we will do so with Bayes' Rule:
it says\begin{eqnarray*}
\pi(\theta|x) & \propto & \pi(\theta)\, f(x|\theta),\\
 & = & \theta^{x}(1-\theta)^{1-x}\cdot3(1-\theta)^{2},\\
 & = & 3\,\theta^{x}(1-\theta)^{3-x},\quad0<\theta<1,\end{eqnarray*}
where the constant of proportionality is given by \[
\int3\, u^{x}(1-u)^{3-x}\diff u=\int3\, u^{(1+x)-1}(1-u)^{(4-x)-1}\diff u=3\,\frac{\Gamma(1+x)\Gamma(4-x)}{\Gamma[(1+x)+(4-x)]},\]
the integral being calculated by inspection of the formula for a $\mathsf{beta}(\mathtt{shape1}=1+x,\,\mathtt{shape2}=4-x)$
distribution. That is to say, our posterior distribution is precisely\[
\theta|x\sim\mathsf{beta}(\mathtt{shape1}=1+x,\,\mathtt{shape2}=4-x).\]


The Bayesian statistician uses the posterior distribution for all
matters concerning inference about $\theta$.\end{example}
\begin{rem}
We usually do not restrict ourselves to the observation of only one
$X$ conditional on $\theta$. In fact, it is common to observe an
entire sample $X_{1}$, $X_{2}$,\ldots{},$X_{n}$ conditional on
$\theta$ (which itself is often multidimensional). Do not be frightened,
however, because the intuition is the same. There is a prior distribution
$\pi(\theta)$, a likelihood $f(x_{1},x_{2},\ldots,x_{n}|\theta)$,
and a posterior distribution $\pi(\theta|x_{1},x_{2},\ldots,x_{n})$.
Bayes' Rule states that the relationship between the three may be
conveniently written as\[
\pi(\theta|x_{1},x_{2},\ldots,x_{n})\propto\pi(\theta)\, f(x_{1},x_{2},\ldots,x_{n}|\theta),\]
where, of course, the constant of proportionality is $\int\pi(u)\, f(x_{1},x_{2},\ldots,x_{n}|u)\,\diff u$.
Any good textbook on Bayesian Statistics will explain these notions
in detail; to the interested reader I recommend Gelman and this other
Bayesian book BLANK. 
\end{rem}

\section{Independent Random Variables}


\subsection{Independent Random Variables\label{sub:Independent-Random-Variables}}

We recall from Chapter BLANK that the events $A$ and $B$ are said
to be independent if\[
\P(A\cap B)=\P(A)\P(B).\]
If it happens that\[
\P(X=x,Y=y)=\P(X=x)\P(Y=y),\quad\mbox{for every }x\in S_{X},\ y\in S_{Y},\]
then we say that $X$ and $Y$ are \emph{independent random variables}.
Otherwise, we say that $X$ and $Y$ are \emph{dependent}. Using the
PMF notation from above, we see that independent discrete random variables
satisfy \[
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad\mbox{for every }x\in S_{X},\ y\in S_{Y}.\]
Now continuing the reasoning, given two continuous random variables
$X$ and $Y$ with joint PDF $f_{X,Y}$ and respective marginal PDFs
$f_{X}$ and $f_{Y}$ that are supported on the sets $S_{X}$ and
$S_{Y}$, if it happens that \[
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad\mbox{for every }x\in S_{X},\ y\in S_{Y},\]
then we say that $X$ and $Y$ are independent.
\begin{example}
In Example BLANK we considered the random experiment of rolling a
fair die twice. There we found the joint PMF to be\[
f_{X,Y}(x,y)=\frac{1}{36},\quad x=1,\ldots,6,\ y=1,\ldots,6,\]
and we found the marginal PMFs $f_{X}(x)=1/6$, $x=1,2,\ldots,6$,
and $f_{Y}(y)=1/6$, $y=1,2,\ldots,6$. Therefore in this experiment
$X$ and $Y$ are independent since for every $x$ and $y$ in the
joint support the joint PMF satisfies \[
f_{X,Y}(x,y)=\frac{1}{36}=\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=f_{X}(x)\, f_{Y}(y).\]

\end{example}

\begin{example}
In Example BLANK we considered the same experiment but different random
variables: $U$ and $V$. We can see that $U$ and $V$ are not independent
by finding a single pair $(u,v)$ where the independence equality
does not hold. There are many such pairs. One of them is $(6,12)$:\[
f_{U,V}(6,12)=\frac{1}{36}\neq\left(\frac{11}{36}\right)\left(\frac{1}{36}\right)=f_{U}(6)\, f_{V}(12).\]

\end{example}
Independent random variables are very useful to the mathematician.
They have many, many, tractable properties. We mention some of the
more important ones.
\begin{prop}
If $X$ and $Y$ are independent, then for any functions $u$ and
$v$, \[
\E\left(u(X)v(Y)\right)=\left(\E u(X)\right)\left(\E v(Y)\right).\]

\end{prop}
\begin{proof}
This is straightforward from the definition.\begin{eqnarray*}
\E\left(u(X)v(Y)\right) & = & \iint\, u(x)v(y)\, f_{X,Y}(x,y)\,\diff x\diff y\\
 & = & \iint\, u(x)v(y)\, f_{X}(x)\, f_{Y}(y)\,\diff x\diff y\\
 & = & \int u(x)\, f_{X}(x)\,\diff x\ \int v(y)\, f_{Y}(y)\,\diff y\end{eqnarray*}
and this last quantity is exactly $\left(\E u(X)\right)\left(\E v(Y)\right)$.
\end{proof}

\begin{cor}
If $X$ and $Y$ are independent, then $\mbox{Cov}(X,Y)=0$, and consequently,
$\mbox{Corr}(X,Y)=0$.
\end{cor}
\begin{proof}
When $X$ and $Y$ are independent then $\E XY=\E X\,\E Y$. And when
the covariance is zero the numerator of the correlation is 0. 
\end{proof}



\begin{rem}
Unfortunately, the converse of Corollary BLANK is not true. That is,
there are many random variables which are dependent even though their
covariance and correlation is zero. For more details, see Casella
BLANK. \end{rem}
\begin{cor}
If $X$ and $Y$ are independent, then the moment generating function
of $X+Y$ is \[
M_{X+Y}(t)=M_{X}(t)\cdot M_{Y}(t).\]

\end{cor}
\begin{proof}
Choose $u(x)=\me^{x}$ and $v(y)=\me^{y}$ in Proposition BLANK, and
remember the identity $\me^{t(x+y)}=\me^{tx}\,\me^{ty}$.
\end{proof}


Proposition BLANK is useful to us and we will receive mileage out
of it, but there is another fact which will play an even more important
role. Unfortunately, the proof is beyond the techniques presented
here. The inquisitive reader should consult Casella and Berger, Resnick,
\emph{etc}.
\begin{fact}
If $X$ and $Y$ are independent, then $u(X)$ and $v(Y)$ are independent
for any functions $u$ and $v$.
\end{fact}

\subsection{Combining Independent Random Variables\label{sub:Combining-Independent-Random}}
\begin{prop}
Let $X_{1}$ and $X_{2}$ be independent with respective population
means $\mu_{1}$ and $\mu_{2}$ and population standard deviations
$\sigma_{1}$ and $\sigma_{2}$. For given constants $a_{1}$ and
$a_{2}$, define $Y=a_{1}X_{1}+a_{2}X_{2}$. Then the mean and standard
deviation of $Y$ are given by the formulas\[
\mu_{Y}=a_{1}\mu_{1}+a_{2}\mu_{2},\quad\sigma_{Y}=\left(a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}\right)^{1/2}.\]

\end{prop}
\begin{proof}
We use Property BLANK of expectation:\[
\E Y=\E\left(a_{1}X_{1}+a_{2}X_{2}\right)=a_{1}\E X_{1}+a_{2}\E X_{2}=a_{1}\mu_{1}+a_{2}\mu_{2}.\]
For the standard deviation, we will find the variance and take the
square root at the end. And to calculate the variance we will first
compute $\E Y^{2}$ with an eye toward using the identity $\sigma_{Y}^{2}=\E Y^{2}-\left(\E Y\right)^{2}$
as a final step. \[
\E Y^{2}=\E\left(a_{1}X_{1}+a_{2}X_{2}\right)^{2}=\E\left(a_{1}^{2}X_{1}^{2}+a_{2}^{2}X_{2}^{2}+2a_{1}a_{2}X_{1}X_{2}\right).\]
Using linearity of expectation the $\E$ distributes through the sum.
Now $\E X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}$, for $i=1$ and 2 and
$\E X_{1}X_{2}=\E X_{1}\E X_{2}=\mu_{1}\mu_{2}$ because of independence.
Thus\begin{eqnarray*}
\E Y^{2} & = & a_{1}^{2}(\sigma_{1}^{2}+\mu_{1}^{2})+a_{2}^{2}(\sigma_{2}^{2}+\mu_{2}^{2})+2a_{1}a_{2}\mu_{1}\mu_{2},\\
 & = & a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}+\left(a_{1}^{2}\mu_{1}^{2}+a_{2}^{2}\mu_{2}^{2}+2a_{1}a_{2}\mu_{1}\mu_{2}\right).\end{eqnarray*}
But notice that the expression in the parentheses is exactly \[
\left(a_{1}\mu_{1}+a_{2}\mu_{2}\right)^{2}=\left(\E Y\right)^{2},\]
and the proof is complete. 
\end{proof}



\section{Exchangeable Random Variables\label{sec:Exchangeable-Random-Variables}}

Two random variables $X$ and $Y$ are said to be \emph{exchangeable}
if their joint CDF is a symmetric function of its arguments:\[
F_{X,Y}(x,y)=F_{X,Y}(y,x),\quad\mbox{for all }(x,y)\in\R^{2}.\]
When the joint density $f$ exists, we may equivalently say that $X$
and $Y$ are exchangeable if $f(x,y)=f(y,x)$ for all $(x,y)$.

Exchangeable random variables exhibit symmetry in the sense that a
person may exchange one for the other, with no substantive changes
to their random behavior. While independence speaks to a \emph{lack
of influence} between the two variables, exchangeability seeks to
capture the \emph{symmetry} between them, in the sense that one variable
may be exchanged for the other without any substantive change to the
joint distribution.
\begin{example}
Here is another one, somewhat more complicated that the one above.\begin{multline}
f_{X,Y}(x,y)=(1+\alpha)\lambda^{2}\me^{-\lambda(x+y)}+\alpha(2\lambda)^{2}\me^{-2\lambda(x+y)}-2\alpha\lambda^{2}\left(\me^{-\lambda(2x+y)}+\me^{-\lambda(x+2y)}\right).\end{multline}
It is straightforward and tedious to check that $\iint f=1$. We may
see immediately that $f_{X,Y}(x,y)=f_{X,Y}(y,x)$ for all $(x,y)$,
which confirms that $X$ and $Y$ are exchangeable. Here, $\alpha$
is said to be an association parameter. This particular example is
one from the Farlie-Gumbel-Morgenstern family of distributions; see
BLANK.

It is a misconception that exchangeability is a weaker condition than
independence. In fact, the two notions are incommensurable. But one
direct connection between the two is made clear by DeFinetti's Thereom.
See Section BLANK for details.
\end{example}

\section{The Bivariate Normal Distribution\label{sec:The-Bivariate-Normal}}

The bivariate normal PDF is given by the unwieldly formula\begin{multline}
f_{X,Y}(x,y)=\frac{1}{2\pi\,\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left\{ -\frac{1}{2(1-\rho^{2})}\left[\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}+\cdots\right.\right.\\
\left.\left.\cdots+2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right]\right\} ,\end{multline}
for $(x,y)\in\R^{2}$. We write $(X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)$,
where\begin{equation}
\upmu=(\mu_{X},\,\mu_{Y})^{T},\quad\sum=\left(\begin{array}{cc}
\sigma_{X}^{2} & \rho\sigma_{X}\sigma_{Y}\\
\rho\sigma_{X}\sigma_{Y} & \sigma_{Y}^{2}\end{array}\right).\end{equation}
See Appendix BLANK. The vector notation allows for a more compact
rendering of the joint PDF:\begin{equation}
f_{X,Y}(\mathbf{x})=\frac{1}{2\pi\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}\left(\mathbf{x}-\upmu\right)^{\top}\Sigma^{-1}\left(\mathbf{x}-\upmu\right)\right\} ,\end{equation}
where in an abuse of notation we have written $\mathbf{x}$ for $(x,y)$.
Note that the formula only holds when $\rho\neq\pm1$.
\begin{rem}
In Remark BLANK we noted that just because random variables are uncorrelated,
it does not necessarily mean that they are independent. However, there
is an important exception to this rule: the normal distribution. Indeed,
$(X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)$
are independent if and only if $\rho=0$. 
\end{rem}

\begin{rem}
Inspection of the joint PDF shows that if $\mu_{X}=\mu_{Y}$ and $\sigma_{X}=\sigma_{Y}$
then $X$ and $Y$ are exchangeable.
\end{rem}
The bivariate normal MGF is\begin{equation}
M_{X,Y}(\mathbf{t})=\exp\left(\upmu^{\top}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\top}\Sigma\mathbf{t}\right),\end{equation}
where $\mathbf{t}=(t_{1},t_{2})$.

The bivariate normal distribution may be intimidating at first but
it turns out to be very tractable compared to other multivariate distributions.
An example of this is the following fact about the marginals.
\begin{fact}
If $(X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)$
then\begin{equation}
X\sim\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\mbox{ and }Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}).\end{equation}

\end{fact}
From this we immediately get that $\E X=\mu_{X}$ and $\mbox{Var}(X)=\sigma_{X}^{2}$
(and the same is true for $Y$ with the letters switched). And it
should be no surprise that the correlation between $X$ and $Y$ is
exactly $\mbox{Corr}(X,Y)=\rho$.
\begin{prop}
The conditional distribution of $Y|\, X=x$ is $\mathsf{norm}(\mathtt{mean}=\mu_{Y|x},\,\mathtt{sd}=\sigma_{Y|x})$,
where\begin{equation}
\mu_{Y|x}=\mu_{Y}+\rho\frac{\sigma_{Y}}{\sigma_{X}}\left(x-\mu_{X}\right),\mbox{ and }\sigma_{Y|x}=\sigma_{Y}\sqrt{1-\rho^{2}}.\end{equation}

\end{prop}

\subsection{How to do it with \textsf{R}}

Use package \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mvtnorm!\inputencoding{utf8}
or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!mnormt!\inputencoding{utf8}%
\footnote{Another way to do this is with the function curve3d in the emdbook
package. It looks like this:
\begin{lyxcode}
library(emdbook);~library(mvtnorm)~~~\#~note:~the~order~matters

mu~<-~c(0,0);~~sigma~<-~diag(2)

f~<-~function(x,y)~dmvnorm(c(x,y),~mean~=~mu,~sigma~=~sigma)

curve3d(f(x,y),~from~=~c(-3,-3),~to~=~c(3,3),~theta~=~-30,~phi~=~30)
\end{lyxcode}
The code above is slightly shorter than that using persp and is easier
to understand. One must be careful, however. If the library calls
are swapped then the code will not work because both packages emdbook
and mvtnorm have a function called {}``dmvnorm''; one must load
them to the search path in the correct order or \textsf{R} will use
the wrong one (the arguments are named differently and the underlying
algorithms are different). %
} 

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 8, width = 6>>=
library(mvtnorm)
x <- y <- seq(from = -3, to = 3, length.out = 30)
f <- function(x,y) dmvnorm(cbind(x,y), mean = c(0,0), sigma = diag(2))
z <- outer(x, y, FUN = f)
persp(x, y, z, theta = -30, phi = 30, ticktype = "detailed")
@
\par\end{centering}

\caption{Capture-recapture experiment\label{fig:mvnorm-pdf}}

\end{figure}



\section{The Multinomial Distribution\label{sec:Multinomial}}

What do I want them to know about the multinomial distribution.
\begin{itemize}
\item It is discrete.
\item the support set is finite, called a simplex.
\item expected values.
\item correlation and covariance
\item marginal distributions
\item how to generate randomly
\end{itemize}
When $p_{1}=$

We write $(X_{1},\ldots,X_{k})\sim\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{\mathrm{k}\times1})$. 
\begin{example}
Suppose Barack Obama wants to have dinner \url{http://pewresearch.org/pubs/773/fewer-voters-identify-as-republicans}
\end{example}

\subsection{How to do it with \textsf{R}}

There is support for the multinomial distribution in base R, namely
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stats!\inputencoding{utf8}
package. The relevant functions are \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dmultinom!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rmultinom!\inputencoding{utf8}.

<<>>=
library(combinat)
tmp <- t(xsimplex(3, 6))
p <- apply(tmp, MARGIN = 1, FUN = dmultinom, prob = c(36,27,37))
library(prob)
S <- probspace(tmp, probs = p)
ProbTable <- xtabs(probs ~ X1 + X2, data = S)
round(ProbTable, 3)
@

Do some examples of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rmultinom!\inputencoding{utf8}

Here is another way to do it%
\footnote{Another way to do the plot is with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scatterplot3d!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scatterplot3d!\inputencoding{utf8}
package. It looks like this:
\begin{lyxcode}
library(scatterplot3d)

X~<-~t(as.matrix(expand.grid(0:6,~0:6)))

X~<-~X{[}~,~colSums(X)~<=~6{]};~X~<-~rbind(X,~6~-~colSums(X))

Z~<-~round(apply(X,~2,~function(x)~dmultinom(x,~prob~=~1:3)),~3)

A~<-~data.frame(x~=~X{[}1,~{]},~y~=~X{[}2,~{]},~probability~=~Z)

scatterplot3d(A,~type~=~{}``h'',~lwd~=~3,~box~=~FALSE)
\end{lyxcode}
The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scatterplot3d!\inputencoding{utf8}
graph looks better in this example, but the code is clearly more difficult
to understand. And with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cloud!\inputencoding{utf8}
one can easily do conditional plots of the form \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cloud(z ~ x + y|f)!\inputencoding{utf8},
where \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!f!\inputencoding{utf8}
is a factor.%
}

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
library(lattice)
print(cloud(probs ~ X1 + X2, data = S, type = c("p","h"), lwd = 2, pch = 16, cex = 1.5), screen = list(z = 15, x = -70))
@
\par\end{centering}

\caption{Plot of a multinomial PMF\label{fig:multinom-pmf2}}

\end{figure}



\section{Bivariate Transformations of Random Variables\label{sec:Transformations-Multivariate}}

We studied in Section BLANK how to find the PDF of $Y=g(X)$ given
the PDF of $X$. But now we have two random variables $X$ and Y,
with joint PDF $f_{X,Y}$, and we would like to consider the joint
PDF of two new random variables\[
U=g(X,Y)\quad\mbox{and}\quad V=h(X,Y),\]
where $g$ and $h$ are two given functions, typically {}``nice''
in the sense of Appendix BLANK. 

Suppose that the transformation $(x,y)\longmapsto(u,v)$ is one-to-one.
Then an inverse transformation $x=x(u,v)$ and $y=y(u,v)$ exists,
so let $\partial(x,y)/\partial(u,v)$ denote the Jacobian of the inverse
transformation. Then the joint PDF of $(U,V)$ is given by

\begin{equation}
f_{U,V}(u,v)=f_{X,Y}\left[x(u,v),\, y(u,v)\right]\left|\frac{\partial(x,y)}{\partial(u,v)}\right|,\end{equation}
or we can rewrite more shortly as\begin{equation}
f_{U,V}(u,v)=f_{X,Y}(x,y)\left|\frac{\partial(x,y)}{\partial(u,v)}\right|.\end{equation}
Take a moment and compare Equation BLANK to Equation BLANK. Do you
see the connection?
\begin{rem}
It is sometimes easier to \emph{postpone} solving for the inverse
transformation $x=x(u,v)$ and $y=y(u,v)$. Instead, leave the transformation
in the form $u=u(x,y)$ and $v=v(x,y)$ and calculate the Jacobian
of the \emph{original} transformation\begin{equation}
\frac{\partial(u,v)}{\partial(x,y)}=\left|\begin{array}{cc}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}\\
\frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}\end{array}\right|=\frac{\partial u}{\partial x}\frac{\partial v}{\partial y}-\frac{\partial u}{\partial y}\frac{\partial v}{\partial x}.\end{equation}
Once this is known, we can get the PDF of $(U,V)$ by\begin{equation}
f_{U,V}(u,v)=f_{X,Y}(x,y)\left|\frac{1}{\frac{\partial(u,v)}{\partial(x,y)}}\right|.\end{equation}
In some cases there will be a cancellation and the work will be a
lot shorter. Of course, it is not always true that\begin{equation}
\frac{\partial(x,y)}{\partial(u,v)}=\frac{1}{\frac{\partial(u,v)}{\partial(x,y)}},\end{equation}
but for the well-behaved examples that we will see in this book it
works just fine\ldots{} do you see the connection between Equations
BLANK and BLANK?\end{rem}
\begin{example}
Let $(X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0}_{2\times1},\,\mathtt{sigma}=\mathbf{I}_{2\times2})$
and consider the transformation\begin{align*}
U= & \ 3X+4Y,\\
V= & \ 5X+6Y.\end{align*}
We can solve the system of equations to find the inverse transformations;
they are\begin{align*}
X= & -3U+2V,\\
Y= & \ \frac{5}{2}U-\frac{3}{2}V,\end{align*}
in which case the Jacobian of the inverse transformation is\[
\begin{vmatrix}-3 & 2\\
\frac{5}{2} & -\frac{3}{2}\end{vmatrix}=3\left(-\frac{3}{2}\right)-2\left(\frac{5}{2}\right)=-\frac{1}{2}.\]
As $(x,y)$ traverses $\R^{2}$, so too does $(u,v)$. Since the joint
PDF of $(X,Y)$ is\[
f_{X,Y}(x,y)=\frac{1}{2\pi}\exp\left\{ -\frac{1}{2}\left(x^{2}+y^{2}\right)\right\} ,\quad(x,y)\in\R^{2},\]
we get that the joint PDF of $(U,V)$ is

\begin{equation}
f_{U,V}(u,v)=\frac{1}{2\pi}\exp\left\{ -\frac{1}{2}\left[\left(-3u+2v\right)^{2}+\left(\frac{5u-3v}{2}\right)^{2}\right]\right\} \cdot\frac{1}{2},\quad(u,v)\in\R^{2}.\end{equation}
\end{example}
\begin{rem}
It may not be obvious, but Equation BLANK is the PDF of a $\mathsf{mvnorm}$
distribution. For a more general result see Proposition BLANK.
\end{rem}

\subsection{How to do it with \textsf{R}}

It is possible to do the computations above in \textsf{R} with the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Ryacas!\inputencoding{utf8}
package. The package is an interface to the open-source computer algebra
system, {}``Yacas''. The user installs Yacas, then uses \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Ryacas!\inputencoding{utf8}
to submit commands to Yacas, after which the output is displayed in
the \textsf{R} console.

We did not want to require users of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR!\inputencoding{utf8}
package to install Yacas, so examples of its use is omitted. But there
are many online materials to help get the interested reader: see BLANK
to get started.


\section{Remarks for the Multivariate Case}

There is nothing spooky about $n\geq3$ random variables. We just
have a whole bunch of them: $X_{1}$, $X_{2}$,\ldots{}, $X_{n}$,
which we can shorten to $\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})^{\mathrm{T}}$
to make the formulas prettier (now may be a good time to check out
Appendix BLANK). For $\mathbf{X}$ supported on the set $S_{\mathbf{X}}$,
the joint PDF $f_{\mathbf{X}}$ (if it exists) satisfies\begin{equation}
f_{\mathbf{X}}(\mathbf{x})\geq0,\quad\mbox{for }\mathbf{x}\in S_{\mathbf{X}},\end{equation}
and\begin{equation}
\int\!\!\!\int\cdots\int f_{\mathbf{X}}(\mathbf{x})\,\diff x_{1}\diff x_{2}\cdots\diff x_{n}=1,\end{equation}
or even shorter: $\int f_{\mathbf{X}}(\mathbf{x})\,\diff\mathbf{x}=1$.
The joint CDF $F_{\mathbf{X}}$ is defined by \begin{equation}
F_{\mathbf{X}}(\mathbf{x})=\P(X_{1}\leq x_{1},\, X_{2}\leq x_{2},\ldots,\, X_{n}\leq x_{n}),\end{equation}
for $\mathbf{x}\in\R^{n}$. The expectation of a function $g(\mathbf{X})$
is defined just as we would imagine:\begin{equation}
\E g(\mathbf{X})=\int g(\mathbf{x})\, f_{\mathbf{X}}(\mathbf{x})\,\diff\mathbf{x}.\end{equation}
provided the integral exists and is finite. And the moment generating
function in the multivariate case is defined by\begin{eqnarray}
M_{\mathbf{X}}(\mathbf{t}) & = & \E\exp\left\{ \mathbf{t}^{\mathrm{T}}\mathbf{X}\right\} ,\end{eqnarray}
whenever the integral exists and is finite for all $\mathbf{t}$ in
a neighborhood of $\mathbf{0}_{\mathrm{n}\times1}$ (note that $\mathbf{t}^{\mathrm{T}}\mathbf{X}$
is shorthand for $t_{1}X_{1}+t_{2}X_{2}+\cdots+t_{n}X_{n}$). The
only difference in any of the above for the discrete case is that
integrals are replaced by sums. 

Marginal distributions are obtained by integrating out remaining variables
from the joint distribution. And even if we are given all of the univariate
marginals it is not enough to determine the joint distribution uniquely.

We say that $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ are \emph{mutually
independent} if their joint PDF factors into the product of the marginals\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=f_{X_{1}}(x_{1})\, f_{X_{2}}(x_{2})\,\cdots\, f_{X_{n}}(x_{n}),\end{equation}
for every $\mathbf{x}$ in their joint support $S_{\mathbf{X}}$,
and we say that $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ are \emph{exchangeable}
if their joint PDF (or CDF) is a symmetric function of its $n$ arguments,
that is, if \begin{equation}
f_{\mathbf{X}}(\mathbf{x^{\ast}})=f_{\mathbf{X}}(\mathbf{x}),\end{equation}
for any reordering $\mathbf{x^{\ast}}$ of the elements of $\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})$
in the joint support.
\begin{thm}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be independent with respective
population means $\mu_{1}$, $\mu_{2}$, \ldots{}, $\mu_{n}$ and
standard deviations $\sigma_{1}$, $\sigma_{2}$, \ldots{}, $\sigma_{n}$.
For given constants $a_{1}$, $a_{2}$, \ldots{},$a_{n}$ define
$Y=\sum_{i=1}^{n}a_{i}X_{i}$. Then the mean and standard deviation
of $Y$ are given by the formulas\begin{equation}
\mu_{Y}=\sum_{i=1}^{n}a_{i}\mu_{i},\quad\sigma_{Y}=\left(\sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}\right)^{1/2}.\end{equation}

\end{thm}
\begin{proof}
The mean is easy:\[
\E Y=\E\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\E X_{i}=\sum_{i=1}^{n}a_{i}\mu_{i}.\]
The variance is not too difficult to compute either. As an intermediate
step, we calculate $\E Y^{2}$. \[
\E Y^{2}=\E\left(\sum_{i=1}^{n}a_{i}X_{i}\right)^{2}=\E\left(\sum_{i=1}^{n}a_{i}^{2}X_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}X_{i}X_{j}\right).\]
Using linearity of expectation the $\E$ distributes through the sums.
Now $\E X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}$ and $\E X_{i}X_{j}=\E X_{i}\E X_{j}=\mu_{i}\mu_{j}$
when $i\neq j$ because of independence. Thus\begin{eqnarray*}
\E Y^{2} & = & \sum_{i=1}^{n}a_{i}^{2}(\sigma_{i}^{2}+\mu_{i}^{2})+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\\
 & = & \sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}+\left(\sum_{i=1}^{n}a_{i}^{2}\mu_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\right)\end{eqnarray*}
To complete the proof, note that the expression in the parentheses
is exactly $\left(\E Y\right)^{2}$, and recall the identity $\sigma_{Y}^{2}=\E Y^{2}-\left(\E Y\right)^{2}$. 
\end{proof}




There is a corresponding statement of Fact BLANK for the multivariate
case. The proof is also omitted here.
\begin{fact}
If $\mathbf{X}$ and $\mathbf{Y}$ are mutually independent random
vectors, then $u(\mathbf{X})$ and $v(\mathbf{Y})$ are independent
for any functions $u$ and $v$.
\end{fact}


Multiple exchangeable random variables; deFinetti's Theorem.

The multivariate normal distribution immediately generalizes from
the bivariate case. If the matrix $\Sigma$ is nonsingular then the
joint PDF of $\mathbf{X}\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)$
is\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=\frac{1}{(2\pi)^{n/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}\left(\mathbf{x}-\upmu\right)^{\top}\Sigma^{-1}\left(\mathbf{x}-\upmu\right)\right\} ,\end{equation}
and the MGF is\begin{equation}
M_{\mathbf{X}}(\mathbf{t})=\exp\left\{ \upmu^{\top}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\top}\Sigma\mathbf{t}\right\} .\end{equation}


We will need the following in Chapter BLANK.
\begin{thm}
If $\mathbf{X}\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)$
and $\mathbf{A}$ is any matrix, then the random vector $\mathbf{Y}=\mathbf{AX}$
is distributed\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{A}\upmu,\,\mathtt{sigma}=\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}).\end{equation}
\end{thm}
\begin{proof}
Look at the MGF of $\mathbf{Y}$:\begin{eqnarray*}
M_{\mathbf{Y}}(\mathbf{t}) & = & \E\,\exp\left\{ \mathbf{t}^{\mathrm{T}}(\mathbf{AX})\right\} ,\\
 & = & \E\,\exp\left\{ (\mathbf{A}^{\mathrm{T}}\mathbf{t})^{\mathrm{T}}\mathbf{X}\right\} ,\\
 & = & \exp\left\{ \upmu^{\mathrm{T}}(\mathbf{A}^{\top}\mathbf{t})+\frac{1}{2}(\mathbf{A}^{\mathrm{T}}\mathbf{t})^{\mathrm{T}}\Sigma(\mathbf{A}^{\mathrm{T}}\mathbf{t})\right\} ,\\
 & = & \exp\left\{ \left(\mathbf{A}\upmu\right)^{\mathrm{T}}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\mathrm{T}}\left(\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}\right)\mathbf{t}\right\} ,\end{eqnarray*}
and the last expression is the MGF of an $\mathsf{mvnorm}(\mathtt{mean}=\mathbf{A}\upmu,\,\mathtt{sigma}=\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}})$
distribution.
\end{proof}



\section{Chapter Exercises}

\setcounter{thm}{0}


\begin{xca}
Prove that $\mbox{Cov}(X,Y)=\E(XY)-(\E X)(\E Y).$ 

type here
\end{xca}





\chapter{Sampling Distributions\label{cha:Sampling-Distributions}}

This is an important chapter; it is the bridge from probability and
descriptive statistics that we studied in Chapters BLANK, BLANK and
BLANK to inferential statistics which forms the latter part of this
book.

Here is the link: we are presented with a \emph{population} about
which we would like to learn. And while it would be desirable to examine
every single member of the population, we find that it is either impossible
or infeasible to for us to do so, thus, we resort to collecting a
\emph{sample} instead. We do not lose heart. Our method will suffice,
provided the sample is \emph{representative} of the population. A
good way to achieve this is to sample \emph{randomly} from the population(s).

Supposing for the sake of argument that we have collected a random
sample, the next task is to make some \emph{sense} out of the data
because the complete list of sample information is usually cumbersome,
unwieldly. We summarize the data set with a descriptive \emph{statistic},
a quantity calculated from the data (we saw many examples of these
in Chapter BLANK). But our sample was random\ldots{} therefore, it
stands to reason that our statistic will be random, too. How is the
statistic distributed?

The probability distribution associated with the population (from
which we sample) is called the \emph{population distribution}, and
the probability distribution associated with our statistic is called
its \emph{sampling distribution}; clearly, the two are interrelated.
To learn about the population distribution, it is imperative to know
everything we can about the sampling distribution. Such is the goal
of this chapter.

We begin by introducing the notion of simple random samples and cataloguing
some of their more convenient mathematical properties. Next we focus
on what happens in the special case of sampling from the normal distribution
(which, again, has several convenient mathematical properties), and
in particular, we meet the sampling distribution of $\Xbar$ and $S^{2}$.
Then we explore what happens to $\Xbar$'s sampling distribution when
the population is not normal and prove one of the most remarkable
theorems in statistics, the Central Limit Theorem (CLT).

With the CLT in hand, we then investigate the sampling distributions
of several other popular statistics, taking full advantage of those
with a tractable form. We finish the chapter with an exploration of
statistics whose sampling distributions are not quite so tractable,
and to accomplish this goal we will use simulation methods that are
grounded in all of our work in the previous four chapters.

Sampling Distributions of one-sample statistics,

sampling distributions of two sample statistics.

simulated sampling distributions

What do I want them to know?
\begin{itemize}
\item what a srs(n) is
\item the sampling distributions of popular statistics

\begin{itemize}
\item of xbar, s\textasciicircum{}2, and phat
\end{itemize}
\item the sampling distributions of more complicated statistics (and how
to generate them)

\begin{itemize}
\item the IQR, median, and mad
\end{itemize}
\item prove the CLT
\item maybe mention the concepts of bias and variance of sampling distributions.
\end{itemize}

\section{Simple Random Samples\label{sec:Simple-Random-Samples}}


\subsection{Simple Random Samples\label{sub:Simple-Random-Samples}}
\begin{defn}
If $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ are independent with $X_{i}\sim f$
for $i=1,2,\ldots,n$, then we say that $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$ are \emph{independent and identically distributed} (i.i.d.)
from the population $f$ or altenatively we say that $X_{1}$, $X_{2}$,
\ldots{}, $X_{n}$ are a \emph{simple random sample of size} $n$,
denoted $SRS(n)$, from the population $f$. \end{defn}
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be a $SRS(n)$ from a
population distribution with mean $\mu$ and finite standard deviation
$\sigma$. Then the mean and standard deviation of $\Xbar$ are given
by the formulas $\mu_{\Xbar}=\mu$ and $\sigma_{\Xbar}=\sigma/\sqrt{n}$.
\end{prop}
\begin{proof}
Plug in $a_{1}=a_{2}=\cdots=a_{n}=1/n$ in Proposition BLANK. 
\end{proof}


The next fact will be useful to us when it comes time to prove the
Central Limit Theorem in Section BLANK.
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be a $SRS(n)$ from a
population distribution with MGF $M(t)$. Then the MGF of $\Xbar$
is given by\begin{equation}
M_{\Xbar}(t)=\left[M\left(\frac{t}{n}\right)\right]^{n}.\end{equation}

\end{prop}
\begin{proof}
Go from the definition:\begin{eqnarray*}
M_{\Xbar}(t) & = & \E\,\me^{t\Xbar}\\
 & = & \E\,\me^{t(X_{1}+\cdots+X_{n})/n}\\
 & = & \E\,\me^{tX_{1}/n}\me^{tX_{2}/n}\cdots\me^{tX_{n}/n}\end{eqnarray*}
And because $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ are independent,
Proposition BLANK allows us to distribute the expectation among each
term in the product, which is\[
\E\,\me^{tX_{1}/n}\,\E\me^{tX_{2}/n}\cdots\E\me^{tX_{n}/n}.\]
The last step is to recognize that each term in last product above
is exactly $M(t/n)$.
\end{proof}



\section{Sampling from a Normal Distribution\label{sec:Sampling-from-Normal}}


\subsection{The Distribution of the Sample Mean\label{sub:Samp-Mean-Dist}}
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be a $SRS(n)$ from a
$\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
Then the sample mean $\Xbar$ has a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})$
sampling distribution.
\end{prop}
\begin{proof}
The mean and standard deviation of $\Xbar$ follow directly from Proposition
BLANK. To address the shape, first remember from Chapter BLANK that
the $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ MGF is
of the form\[
M(t)=\exp\left\{ \mu t+\sigma^{2}t^{2}/2\right\} .\]
Now use Proposition BLANK to find\begin{eqnarray*}
M_{\Xbar}(t) & = & \left[M\left(\frac{t}{n}\right)\right]^{n}\\
 & = & \left[\exp\left\{ \mu(t/n)+\sigma^{2}(t/n)^{2}/2\right\} \right]^{n}\\
 & = & \exp\left\{ \, n\cdot\left[\mu(t/n)+\sigma^{2}(t/n)^{2}/2\right]\right\} \\
 & = & \exp\left\{ \mu t+(\sigma/\sqrt{n})^{2}t^{2}/2\right\} ,\end{eqnarray*}
and we recognize this last quantity as the MGF of a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})$
distribution.
\end{proof}



\subsection{The Distribution of the Sample Variance\label{sub:Samp-Var-Dist}}
\begin{thm}
\label{thm:Xbar-andS}Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$
be a $SRS(n)$ from a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$
distribution, and let\[
\Xbar=\sum_{i=1}^{n}X_{i}\quad\mbox{and}\quad S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\Xbar)^{2}.\]
Then
\begin{enumerate}
\item $\Xbar$ and $S^{2}$ are independent, and
\item The scaled sample variance \[
\frac{(n-1)}{\sigma^{2}}S^{2}=\frac{\sum_{i=1}^{n}(X_{i}-\Xbar)^{2}}{\sigma^{2}}\]
has \textup{a $\mathsf{chisq}(\mathtt{df}=n-1)$ sampling distribution.}
\end{enumerate}
\end{thm}
\begin{proof}
The proof is beyond the scope of the present book, but the theorem
is simply too important to be omitted. The interested reader could
consult Casella and Berger, or any other sophisticated text on Mathematical
Statistics. 
\end{proof}



\subsection{The Distribution of Student's $T$ Statistic\label{sub:Student's-t-Distribution}}
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be a $SRS(n)$ from a
$\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
Then the quantity \begin{equation}
T=\frac{\Xbar-\mu}{S/\sqrt{n}}\end{equation}
 has a $\mathsf{t}(\mathtt{df}=n-1)$ sampling distribution.
\end{prop}
\begin{proof}
Divide the numerator and denominator by $\sigma$ and rewrite\[
T=\frac{\frac{\Xbar-\mu}{\sigma/\sqrt{n}}}{S/\sigma}=\frac{\frac{\Xbar-\mu}{\sigma/\sqrt{n}}}{\sqrt{\left.\frac{(n-1)S^{2}}{\sigma^{2}}\right\slash (n-1)}}.\]
Now let \[
Z=\frac{\Xbar-\mu}{\sigma/\sqrt{n}}\quad\mbox{and}\quad V=\frac{(n-1)S^{2}}{\sigma^{2}},\]
so that\[
T=\frac{Z}{\sqrt{V/r}},\]
where $r=n-1$.

We know from Section \ref{sub:Samp-Mean-Dist} that $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
and we know from Section \ref{sub:Samp-Var-Dist} that $V\sim\mathsf{chisq}(\mathtt{df}=n-1)$.
Further, since we are sampling from a normal distribution, Theorem
\ref{thm:Xbar-andS} gives that $\Xbar$ and $S^{2}$ are independent
and by Fact BLANK so are $Z$ and $V$. In summary, the distribution
of $T$ is the same as the distribution of the quantity $Z/\sqrt{V/r}$,
where $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ and $V\sim\mathsf{chisq}(\mathtt{df}=r)$
are independent. This is in fact the definition of Student's $t$
distribution.
\end{proof}


This distribution was first published by W.~S.~Gosset (1900) under
the pseudonym Student, and the distribution has consequently come
to be known as Student's $t$ distribution. The PDF of $T$ can be
derived explicitly using the techniques of Section BLANK; it takes
the form 

\begin{equation}
f_{X}(x)=\frac{\Gamma[(r+1)/2]}{\sqrt{r\pi}\ \Gamma(r/2)}\left(1+\frac{x^{2}}{r}\right)^{-(r+1)/2},\quad\-\infty<x<\infty\end{equation}


Any random variable $T$ with the preceding PDF is said to have Student's
$t$ distribution with $r$ \emph{degrees of freedom} ($\mathtt{df}$),
and we write $T\sim\mathsf{t}(\mathtt{df}=r)$. The shape of the PDF
is similar to the normal, but the tails are considerably heavier.
See Figure BLANK. As with the Normal distribution, there are four
functions in \textsf{R} associated with the $t$ distribution, namely
\texttt{dt()}, \texttt{pt()}, \texttt{qt()}, and \texttt{rt()}, which
compute the p.d.f., c.d.f., quantiles, and generate random variates,
respectively.

Similar to that done for the normal we may define $\mathsf{t}_{\alpha}(\mathtt{df}=n-1)$
as the number on the $x$-axis such that there is exactly $\alpha$
area under the $\mathsf{t}(\mathtt{df}=n-1)$ curve to its right.
\begin{example}
Find $t_{[0.01]}^{(23)}$ with the quantile function:
\end{example}
\texttt{\textcolor{red}{> qt(0.01, df=23, lower.tail=FALSE)}}\texttt{}~\\
\texttt{\textcolor{blue}{{[}1{]} 2.499867}}

Notice the \texttt{df} parameter.
\begin{rem}
There are a few things to note about the $\mathtt{t}(\mathtt{df}=r)$
distribution.
\begin{enumerate}
\item It looks a lot like a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
distribution, except with heavier tails.
\item When $r=1$, the $\mathtt{t}(\mathtt{df}=r)$ distribution is the
same as the $\mathtt{cauchy}(\mathtt{location}=0,\,\mathtt{scale}=1)$
distribution.
\item The standard deviation -- if it exists -- is always bigger than one,
but decreases to one as $r\to\infty$.
\item As $r\to\infty$, the $\mathtt{t}(\mathtt{df}=r)$ distribution approaches
the $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ distribution. 
\end{enumerate}
\end{rem}

\section{The Central Limit Theorem\label{sec:The-Central-Limit}}

In this section we study the distribution of the sample mean when
the underlying distribution is \emph{not} normal. We saw in Section
\ref{sec:Sampling-from-Normal} that when $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$ is a $SRS(n)$ from a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$
distribution then $\Xbar\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})$.
In other words, we may say (owing to Proposition BLANK) when the underlying
population is normal that the sampling distribution of $Z$ defined
by \[
Z=\frac{\Xbar-\mu}{\sigma/\sqrt{n}}\]
is $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$. 

However, there are many populations that are \emph{not} normal\ldots{}
and the statistician often finds herself sampling from such populations.
What can be said in this case? The surprising answer is contained
in the following theorem.
\begin{thm}
\textbf{The Central Limit Theorem.} Let $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$ be a $SRS(n)$ from a population distribution with mean $\mu$
and finite standard deviation $\sigma$. Then the sampling distribution
of \[
Z=\frac{\Xbar-\mu}{\sigma/\sqrt{n}}\]
approaches a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ distribution
as $n\to\infty$. \end{thm}
\begin{rem}
Since we suppose that $X_{1},X_{2},...,X_{n}$ are iid, we already
know from Section \ref{sub:Simple-Random-Samples} that $\Xbar$ has
mean $\mu$ and standard deviation $\sigma/\sqrt{n}$, so that $Z$
has mean 0 and standard deviation 1. The beauty of the CLT is that
it addresses the \emph{shape} of $Z$'s distribution when the sample
size is large.
\end{rem}

\begin{rem}
Notice that the shape of the underlying population's distribution
is not mentioned in Theorem BLANK; indeed, the result is true for
any population that is well-behaved enough to have a finite standard
deviation. In particular, if the population is normally distributed
then we know from Section BLANK that the distribution of $\Xbar$
(and $Z$ by extension) is \emph{exactly} normal, for every $n$.
\end{rem}

\begin{rem}
How large is {}``sufficiently large''? It is at this point that
the shape of the underlying population distribution plays a role.
For populations with distributions that are approximately symmetric
and mound-shaped, the samples may need to be only of size four or
five, while for highly skewed or heavy-tailed populations the samples
may need to be much larger for the distribution of the sample means
to begin to show a bell-shape. Regardless, for a given population
the approximation tends to be better for larger sample sizes. 
\end{rem}

\subsection{How to do it with \textsf{R}}

I am thinking about some demonstrations, such as in TeachingDemos
or distr


\section{Sampling Distributions of Two-Sample Statistics\label{sec:Samp-Dist-Two-Samp}}

There are often two populations under consideration, and it sometimes
of interest to compare properties between groups. To do so we take
independent samples from each population and calculate respective
sample statistics for comparison. In some simple cases the sampling
distribution of the comparison is known and easy to derive; such cases
are the subject of the present section.


\subsection{Difference of Independent Sample Means}
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$ be an $SRS(n_{1})$
from a $\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})$
distribution and let $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{n_{2}}$ be
an $SRS(n_{2})$ from a $\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})$
distribution. Suppose that $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$
and $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{n_{2}}$ are independent samples.
Then the quantity \[
\frac{\Xbar-\Ybar-(\mu_{X}-\mu_{Y})}{\sqrt{\left.\sigma_{X}^{2}\right\slash n_{1}+\left.\sigma_{Y}^{2}\right\slash n_{2}}}\]
has a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ sampling distribution.
Equivalently, $\Xbar-\Ybar$ has a $\mathsf{norm}(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\left.\sigma_{X}^{2}\right\slash n_{1}+\left.\sigma_{Y}^{2}\right\slash n_{2}})$
sampling distribution. \end{prop}
\begin{proof}
We know that $\Xbar$ is $\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X}/\sqrt{n_{1}})$
and we also know that $\Ybar$ is $\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}/\sqrt{n_{2}})$.
And since the samples $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$
and $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{n_{2}}$ are independent, so
too are $\Xbar$ and $\Ybar$. The distribution of their difference
is thus normal as well, and the mean and standard deviation are given
by Proposition BLANK.\end{proof}
\begin{rem}
Even if the distribution of the samples is not
\end{rem}


In the special case that $\mu_{X}=\mu_{Y}$, we have shown that \[
\frac{\Xbar-\Ybar}{\sqrt{\sigma_{X}^{2}/n_{1}+\sigma_{Y}^{2}/n_{2}}}\]
has a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ sampling distribution,
or in other words, $\Xbar-\Ybar$ has a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sqrt{\sigma_{X}^{2}/n_{1}+\sigma_{Y}^{2}/n_{2}})$
sampling distribution. This will play a role when it comes time to
do hypothesis tests; see Section BLANK.


\subsection{Difference of Independent Sample Proportions}
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$ be an $SRS(n_{1})$
from a $\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{1})$ distribution
and let $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{n_{2}}$ be an $SRS(n_{2})$
from a $\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{2})$ distribution.
Suppose that $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$ and $Y_{1}$,
$Y_{2}$, \ldots{}, $Y_{n_{2}}$ are independent samples. Define
\[
\hat{p}_{1}=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}X_{i}\quad\mbox{and}\quad\hat{p}_{2}=\frac{1}{n_{2}}\sum_{j=1}^{n_{2}}Y_{j}.\]
Then the sampling distribution of\[
\frac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}}\]
approaches a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ distribution
as both $n_{1},\, n_{2}\to\infty$. In other words, the sampling distibution
of $\hat{p}_{1}-\hat{p}_{2}$ is approximately\[
\mathsf{norm}\left(\mathtt{mean}=p_{1}-p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}\right),\]
provided both $n_{1}$ and $n_{2}$ are sufficiently large.
\end{prop}
\begin{proof}
We know that $\hat{p}_{1}$ is approximately normal for $n_{1}$ sufficiently
large, by the Central Limit Theorem. And we know that $\hat{p}_{2}$
is approximately normal for $n_{2}$ sufficiently large, also by the
Central Limit Theorem. Further, $\hat{p}_{1}$ and $\hat{p}_{2}$
are independent since they are derived from independent samples. But
a difference of independent (approximately) normal distributions is
(approximately) normal, by Proposition BLANK%
\footnote{\begin{rem}
This does not explicitly follow, because of our cavalier use of {}``approximately''
in too many places. To be more thorough, however, would require more
concepts than we can afford at the moment. The interested reader may
consult a more advanced text, specifically the topic of weak convergence,
that is, convergence in distribution.
\end{rem}
%
}. The expressions for the mean and standard deviation follow immediately
from Proposition BLANK combined with the formulas for the $\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)$
distribution from Chapter BLANK.
\end{proof}



\subsection{Ratio of Independent Sample Variances}
\begin{prop}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$ be an $SRS(n_{1})$
from a $\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})$
distribution and let $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{n_{2}}$ be
an $SRS(n_{2})$ from a $\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})$
distribution. Suppose that $X_{1}$, $X_{2}$, \ldots{}, $X_{n_{1}}$
and $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{n_{2}}$ are independent samples.
Then the ratio \[
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}\]
has an $\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)$
sampling distribution.
\end{prop}
\begin{proof}
We know from Theorem BLANK that $(n_{1}-1)S_{X}^{2}/\sigma_{X}^{2}$
is distributed $\mathsf{chisq}(\mathtt{df}=n_{1}-1)$ and $(n_{2}-1)S_{Y}^{2}/\sigma_{Y}^{2}$
is distributed $\mathsf{chisq}(\mathtt{df}=n_{2}-1)$. Now write \[
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}=\frac{\left.(n_{1}-1)S_{Y}^{2}\right\slash (n_{1}-1)}{\left.(n_{2}-1)S_{Y}^{2}\right\slash (n_{2}-1)}\cdot\frac{\left.1\right\slash \sigma_{X}^{2}}{\left.1\right\slash \sigma_{Y}^{2}},\]
by multiplying and dividing the numerator with $n_{1}-1$ and doing
likewise for the denominator with $n_{2}-1$. Now we may regroup the
terms into \[
F=\frac{\left.\frac{(n_{1}-1)S_{X}^{2}}{\sigma_{X}^{2}}\right\slash (n_{1}-1)}{\left.\frac{(n_{2}-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\right\slash (n_{2}-1)},\]
and we recognize $F$ to be the ratio of independent $\mathsf{chisq}()$
distributions, each divided by its respective numerator $\mathtt{df}=n_{1}-1$
and denominator $\mathtt{df}=n_{1}-1$. degrees of freedom. This is,
indeed, the definition of Snedecor's $F$ distribution. 
\end{proof}

\begin{rem}
In the special case that $\sigma_{X}=\sigma_{Y}$, we have shown that
\[
F=\frac{S_{X}^{2}}{S_{Y}^{2}}\]
has an $\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)$
sampling distribution. This will be important in Chapter BLANK.
\end{rem}

\section{Simulated Sampling Distributions\label{sec:Simulated-Sampling-Distributions}}

Some comparisons are meaningful, but their sampling distribution is
not quite so tidy to describe analytically. What do we do then?

As it turns out, we do not need to know the exact analytical form
of the sampling distribution; sometimes it is enough to approximate
it with a simulated distribution. In this section, we will show you
how. Note that \textsf{R} is particularly well suited to compute simulated
sampling distributions, much more so than SPSS or SAS, say.


\subsection{The Interquartile Range}

<<>>=
iqrs <- replicate(100, IQR(rnorm(100)))
@

We can look at the mean of the simulated values

<<>>=
mean(iqrs)
@

and we can see the standard deviation

<<>>=
sd(iqrs)
@

Now let's take a look at a plot of the simulated values

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
hist(iqrs)
@
\par\end{centering}

\caption{Plot of simulated IQRs}

\end{figure}



\subsection{The Median Absolute Deviation}

<<>>=
mads <- replicate(100, mad(rnorm(100)))
@

We can look at the mean of the simulated values

<<>>=
mean(mads)
@

and we can see the standard deviation

<<>>=
sd(mads)
@

Now let's take a look at a plot of the simulated values

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
hist(mads)
@
\par\end{centering}

\caption{Plot of simulated MADs}

\end{figure}



\section{Chapter Exercises}

\setcounter{thm}{0}

<<echo = FALSE, results = hide>>=
k = 1
n = sample(10:30, size=10, replace = TRUE)
mu = round(rnorm(10, mean = 20))
@
\begin{xca}
Suppose that we observe a random sample $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$ of size $SRS(n=$\Sexpr{n[k]}) from a $\mathsf{norm}(\mathtt{mean}=$\Sexpr{mu[k]})
distribution. \end{xca}
\begin{enumerate}
\item What is the mean of $\Xbar$?
\item What is the standard deviation of $\Xbar$?
\item What is the distribution of $\Xbar$? (approximately)
\item Find $\P(a<\Xbar\leq b)$
\item Find $\P(\Xbar>c)$.\end{enumerate}
\begin{xca}
In this exercise we would like to investigate how the shape of the
population distribution affects the time until the distribution of
$\Xbar$ is acceptably normal.
\end{xca}
Using the programs and the commands you have learned in class, answer
the following questions. You will need to make plots and histograms
in the assignment. See Appendix BLANK for instructions about writing
reports with R. For these problems, the discussion/interpretation
parts are the most important, so be sure to ANSWER THE WHOLE QUESTION.
\vspace{0.02in}



\subsection*{The Central Limit Theorem}

For Questions 1-3, we assume that we have observed random variables
$X_{1}$, $X_{2}$, \ldots{},$X_{n}$ that are an $SRS(n)$ from
a given population (depending on the problem) and we want to investigate
the distribution of $\Xbar$ as the sample size $n$ increases. 
\begin{enumerate}
\item The population of interest in this problem has a Student's $t$ distribution
with $r=3$ degrees of freedom. We begin our investigation with a
sample size of $n=2$. Download \texttt{CLT 1.R} from the website
and open it with \texttt{Tinn-R}. Copy and paste the entire program
into \textsf{R}. 

\begin{enumerate}
\item What is the population mean $\mu$ and the population variance $\sigma^{2}$?
(Read these from the first graph.)
\item The second graph shows (after a few seconds) a relative frequency
histogram which closely approximates the distribution of $\Xbar$.
Record the values of \texttt{mean(xbar)} and \texttt{var(xbar)}. Use
the answers from part (a) to calculate what these estimates \emph{should}
be. How well do your answers to parts (a) and (b) agree?
\item Click on the histogram to superimpose a red Normal curve, which is
the theoretical limit of the distribution of $\Xbar$ as $n\to\infty$.
How well do the histogram and the Normal curve match? Describe the
differences between the two distributions. When judging between the
two, do not worry so much about the scale (the graphs are being rescaled
automatically, anyway). Rather, look at the peak: does the histogram
poke through the top of the normal curve? How about on the sides:
are there patches of white space between the histogram and line on
either side (or both)? How do the curvature of the histogram and the
line compare? Check down by the tails: does the red line drop off
visibly below the level of the histogram, or do they taper off at
the same height? 
\item Go back to \texttt{CLT 1.R} and increase the \texttt{sample.size}
from 2 to 11. Next, copy-and-paste the modified program and answer
parts (a) and (b) for this new sample size.
\item Go back to \texttt{CLT 1.R} and increase the \texttt{sample.size}
from 11 to 31. Next, copy-and-paste the modified program and answer
parts (a) and (b) for this new sample size.
\item Comment on whether it appears that the histogram and the red curve
are {}``noticeably different'' or whether they are {}``essentially
the same''. If they are still {}``noticeably different'', how large
does $n$ need to be until they are {}``essentially the same''?
(Experiment with different values of $n$).
\end{enumerate}
\item Repeat Question 1 for the program \texttt{CLT 2.R}. In this problem,
the population of interest has a $\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=10)$
distribution.
\item Repeat Question 1 for the program \texttt{CLT 3.R}. In this problem,
the population of interest has a $\mathsf{gamma}(\mathtt{shape}=1.21,\,\mathtt{rate}=1/2.37)$
distribution.
\item Summarize what you have learned. In your own words, what is the general
trend that is being displayed in these histograms, as the sample size
$n$ increases from 2 to 11, on to 31 and onward?
\item How would you describe the relationship between the \textbf{\emph{shape}}
of the population distribution and the \textbf{\emph{speed}} at which
$\Xbar$'s distribution converges to normal? In particular, consider
a population which is highly \textbf{skewed}. Will we need a relatively
LARGER sample size or a relatively SMALLER sample size in order for
$\Xbar$'s distribution to be approximately bell shaped?
\end{enumerate}

\begin{xca}
Let $X_{1}$,\ldots{}, $X_{25}$ be a random sample from a $\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45)$
distribution. Find the following probabilities. Let $\Xbar$ be the
sample mean of these $n=25$ observations.
\begin{enumerate}
\item How is $\Xbar$ distributed? 


$\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45/\sqrt{25})$ 

\item Find $\P(\Xbar>43.1)$.


<<>>=
pnorm(43.1, mean = 37, sd = 9, lower.tail = FALSE)
@

\end{enumerate}
\end{xca}



\chapter{Estimation}

There are two branches of estimation procedures: point estimation
and interval estimation. We briefly discuss point estimation first
and then spend the rest of the chapter on interval estimation.

We find an estimator using the first section. Then we take the estimator
and combine what we know from Chapter BLANK about sampling distributions
to study how the estimator will perform. Once we have estimators,
we add sampling distributions to get confidence intervals. Once we
have confidence intervals we can do inference in the form of hypothesis
tests in the next chapter.

What would I like them to know?
\begin{itemize}
\item How to estimate a parameter.
\item About maximum likelihood. SWBAT

\begin{itemize}
\item eyball a likelihood and get a maximum
\item use Calculus to find an MLE for one-parameter families
\end{itemize}
\item Talk about properties of estimators:

\begin{itemize}
\item bias
\item minimum variance
\item MSE?
\item asymptotics?
\end{itemize}
\item Find confidence intervals for all of the basic experimental designs.
\item Interpret confidence intervals a la PANIC
\item Introduce the concept of margin of error and its relationship to sample
size
\end{itemize}

\section{Point Estimation}

The following example was how I was introduced to maximum likelihood.
It is a 
\begin{example}
Suppose we have a small pond in our backyard, and in the pond there
live some fish. We would like to know how many fish live in the pond.
How can we estimate this? One procedure developed by researchers is
the capture-recapture method. Here is how it works.

We will fish from the pond and suppose that we capture $M=7$ fish.
On each caught fish we attach an unobtrusive tag to the fish's tail,
and release it back into the water. 

Next, we wait a few days for the fish to remix and become accustomed
to their new tag. Then we go fishing again. On the second trip suppose
that we catch $K=4$ fish and we find that 3 of them are tagged. Some
of the fish we catch may be tagged; some may not be. Let $X$ denote
the number of caught fish which are tagged%
\footnote{It is theoretically possible that we could catch the same tagged fish
more than once, which would inflate our count of tagged fish. To avoid
this difficulty, suppose that on the second trip we use a tank on
the boat to hold the caught fish until data collection is completed.%
}. 

Now let $F$ denote the (unknown) total number of fish in the pond.
We know that $F\geq7$, because we tagged that many on the first trip.
In fact, if we let $N$ denote the number of untagged fish in the
pond, then $F=M+N$. We have sampled $K=4$ times, without replacement,
from an urn which has $M=7$ white balls and $N=F-M$ black balls,
and we have observed $x=3$ of them which are white. What is the probability
of this?

Looking back to Section BLANK, we see that the random variable $X$
has a $\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=F-M,\,\mathtt{k}=K)$
distribution. Therefore, for an observed value $X=x$ the probability
would be\[
\P(X=x)=\frac{{M \choose x}{F-M \choose K-x}}{{F \choose K}}.\]


First we notice that $F$ must be at least 7. Could $F$ be equal
to seven? If $F=7$ then all of the fish would have been tagged tagged
on the first run, and there would be no untagged fish in the pond,
thus, $\P(\mbox{3 successes in 4 trials})=0$.

What about $F=8$; what would be the probabillity of observing $X=3$
tagged fish?\[
\P(\mbox{3 successes in 4 trials})=\frac{{7 \choose 3}{1 \choose 1}}{{8 \choose 4}}=\frac{35}{70}=0.5.\]


Similarly, if $F=9$ then the probabillity of observing $X=3$ tagged
fish would be\[
\P(\mbox{3 successes in 4 trials})=\frac{{7 \choose 3}{2 \choose 1}}{{9 \choose 4}}=\frac{70}{126}\approx0.556.\]
We can see already that the observed data $X=3$ is more likely when
$F=9$ than it is when $F=8$. And here is the genius of Sir Ronald
Aylmer Fisher: he asks, {}``What is the value of $F$ which has the
highest likelihood?'' In other words, for all of the different possible
values of $F$, which one makes the above probability the biggest?
We can answer this question with a plot of $\P(X=x)$ versus $F$.
See Figure BLANK.
\end{example}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
heights = rep(0, 16)
for (j in 7:15) heights[j] <- dhyper(3, m = 7, n = j - 7, k = 4)
plot(6:15, heights[6:15], pch = 16, cex = 1.5, xlab = "number of fish in pond", ylab = "Likelihood")
abline(h = 0)
lines(6:15, heights[6:15], type = "h", lwd = 2, lty = 3)
text(9, heights[9]/6, bquote(hat(F)==.(9)), cex = 2, pos = 4)
lines(9, heights[9], type = "h", lwd = 2)
points(9, 0, pch = 4, lwd = 3, cex = 2) 
@
\par\end{centering}

\caption{Capture-recapture experiment\label{fig:capture-recapture}}

\end{figure}



\begin{example}
In the last example we were only concerned with how many fish were
in the pond, but now, we will ask a different question. Suppose it
is known that there are only two species of fish in the pond: smallmouth
bass (\emph{Micropterus dolomieu}) and bluegill (\emph{Lepomis macrochirus});
perhaps we built the pond several years ago and stocked it with only
these two species. We would like to estimate the proportion of fish
in the pond which are bass.

Let $p=\mbox{the proportion of bass}$. Without any other information,
it is conceivable for $p$ to be any value in the interval $[0,1]$,
but for the sake of argument we will suppose that $p$ falls strictly
between 0 and 1. How can we learn about the true value of $p$? Go
fishing! As before, we will use catch-and-release, but unlike before,
we will not tag the fish. We will simply note the species of any caught
fish before returning it to the pond.

Suppose we catch $n$ fish. Let\[
X_{i}=\begin{cases}
1, & \mbox{if the \mbox{i}\mbox{th} fish is a bass,}\\
0, & \mbox{if the \mbox{i}\mbox{th} fish is a bluegill.}\end{cases}\]


Since we are returning the fish to the pond once caught, we may think
of this as a sampling scheme with replacement where the proportion
of bass $p$ does not change. Given that we allow the fish sufficient
time to {}``mix'' once returned, it is not completely unreasonable
to model our fishing experiment as a sequence of Bernoulli trials,
so that the $X_{i}$'s would be i.i.d.~$\mathsf{binom(\mathtt{size}}=1,\,\mathtt{prob}=p)$.
Under those assumptions we would have \begin{eqnarray*}
\P(X_{1}=x_{1},\, X_{2}=x_{2},\,\ldots,\, X_{n}=x_{n}) & = & \P(X_{1}=x_{1})\,\P(X_{2}=x_{2})\,\cdots\P(X_{n}=x_{n}),\\
 & = & p^{x_{1}}(1-p)^{x_{1}}\, p^{x_{2}}(1-p)^{x_{2}}\cdots\, p^{x_{n}}(1-p)^{x_{n}},\\
 & = & p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.\end{eqnarray*}
That is, \[
\P(X_{1}=x_{1},\, X_{2}=x_{2},\,\ldots,\, X_{n}=x_{n})=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.\]
This last quantity is a function of $p$, called the likelihood function
$L(p)$:\[
L(p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.\]
A graph of $L$ for selected values of $\sum x_{i}$ when $n=7$ is
shown in Figure BLANK. 

curve(x\textasciicircum{}2{*}(1-x)\textasciicircum{}4, 0, 1) 

curve(x\textasciicircum{}4{*}(1-x)\textasciicircum{}3, 0, 1, add =
TRUE) 

curve(x\textasciicircum{}5{*}(1-x)\textasciicircum{}2, 0, 1, add =
TRUE) 

What we want is to find the value of $p$ which has the highest likelihood,
that is, we again wish to maximize the likelihood. From Calculus (see
Appendix BLANK), we may differentiate $L$ and set $L'=0$ to find
a maximum.\[
L'(p)=\left(\sum x_{i}\right)p^{\sum x_{i}-1}(1-p)^{n-\sum x_{i}}+p^{\sum x_{i}}\left(n-\sum x_{i}\right)(1-p)^{n-\sum x_{i}-1}(-1).\]
 The derivative vanishes $L'=0$ when\begin{eqnarray*}
\left(\sum x_{i}\right)p^{\sum x_{i}-1}(1-p)^{n-\sum x_{i}} & = & p^{\sum x_{i}}\left(n-\sum x_{i}\right)(1-p)^{n-\sum x_{i}-1},\\
\sum x_{i}(1-p) & = & \left(n-\sum x_{i}\right)p,\\
\sum x_{i}-p\sum x_{i} & = & np-p\sum x_{i},\\
\frac{1}{n}\sum_{i=1}^{n}x_{i} & = & p.\end{eqnarray*}
The {}``best'' $p$, the one which maximizes the likelihood, is
called the maximum likelihood estimator (MLE) of $p$, and is denoted
$\hat{p}$. That is, \[
\hat{p}=\frac{\sum_{i=1}^{n}x_{i}}{n}=\xbar.\]

\begin{rem}
Properly speaking we have only shown that the derivative equals zero
at $\hat{p}$, and thus it is theoretically possible that the critical
value $\hat{p}=\xbar$ is located at a minimum instead of a maximum!
We should be thorough and check that $L'>0$ when $p<\xbar$ and $L'<0$
when $p>\xbar$. Then by the First Derivative Test (see BLANK) we
could be certain that $\hat{p}=\xbar$ is indeed a maximum likelihood
estimator, and not a minimum likelihood estimator.
\end{rem}
The result is shown in Figure BLANK.
\end{example}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
dat <- rbinom(27, size = 1, prob = 0.3)
like <- function(x){
r <- 1
for (k in 1:27){ r <- r*dbinom(dat[k], size = 1, prob = x)}
return(r)
}
curve(like, from = 0, to = 1, xlab = "parameter space", ylab = "Likelihood", lwd = 3, col = "blue")
abline(h = 0, lwd = 1, lty = 3, col = "grey")
mle <- mean(dat)
mleobj <- like(mle)
lines(mle, mleobj, type = "h", lwd = 2, lty = 3, col = "red")
points(mle, 0, pch = 4, lwd = 2, cex = 2, col = "red")
text(mle, mleobj/6, substitute(hat(theta)==a, list(a=round(mle, 4))), cex = 2, pos = 4)
@
\par\end{centering}

\caption{Species maximum likelihood\label{fig:species-mle}}

\end{figure}


In general, we have a family of PDFs $f(x|\theta)$ indexed by a parameter
$\theta$ in some parameter space $\Theta$. We want to learn about
$\theta$. We take a $SRS(n)$:\[
X_{1},\, X_{2},\,\ldots,X_{n}\mbox{ which are i.i.d.\,\mbox{f(x|\ensuremath{\theta})}.}\]

\begin{defn}
Given the observed data $x_{1}$, $x_{2}$, \ldots{}, $x_{n}$, the
\emph{likelihood function} $L$ is defined by \[
L(\theta)=\prod_{i=1}^{n}f(x_{i}|\theta),\quad\theta\in\Theta.\]
We next maximize $L$:\end{defn}
\begin{itemize}
\item How: for us, we will find the derivative $L'$, and solve the equation
$L'(\theta)=0$. Call a solution $\hat{\theta}$. We check that $L$
is maximized at $\hat{\theta}$ using the First Derivative Test or
the Second Derivative Test $\left(L''(\hat{\theta})<0\right)$.\end{itemize}
\begin{defn}
A value $\theta$ that maximizes $L$ is called a \emph{maximum likelihood
estimator} (MLE) and is denoted $\hat{\theta}$. Note that $\hat{\theta}$
is a function of the sample, $\hat{\theta}=\hat{\theta}\left(X_{1},\, X_{2},\,\ldots,X_{n}\right)$,
and is an example of a \emph{point estimator} of $\theta$.\end{defn}
\begin{rem}
Some comments about maximum likelihood estimators:
\begin{itemize}
\item Often it is easier to maximize the \emph{log-likelihood} $l(\theta)=\ln L(\theta)$
instead of the likelihood $L$. Since the logarithmic function $y=\ln x$
is a monotone transformation, the solutions to both problems are the
same.
\item MLEs do not always exist (for instance, sometimes the likelihood has
a vertical asymptote), and even when they do exist, they are not always
unique (imagine a function with a bunch of humps of equal height).
For any given problem, there could be zero, one, or any number of
values of $\theta$ for which $L(\theta)$ is a maximum.
\item The problems we will encounter are all very nice with likelihood functions
that have closed form representations and which are optimized by some
Calculus acrobatics. In practice, however, likelihood functions can
be quite nasty in which case we are obliged to use numerical methods
to find maxima (if there are any).
\item MLEs are just one of \underbar{many} possible estimators. One of the
more popular alternatives are the Method of Moments estimators; see
BLANK.
\end{itemize}
\end{rem}
Notice, in Example BLANK we had $X_{i}$ i.i.d.~$\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)$,
and we saw that the MLE was $\hat{p}=\Xbar$. But further\begin{eqnarray*}
\E\Xbar & = & \E\frac{X_{1}+X_{2}+\cdots+X_{n}}{n}\\
 & = & \frac{1}{n}\left(\E X_{1}+\E X_{2}+\cdots+\E X_{n}\right)\\
 & = & \frac{1}{n}\left(np\right)\\
 & = & p,\end{eqnarray*}
which is exactly the same as the parameter which we estimated. More
concisely, $\E\hat{p}=p$, that is, on the average, the estimator
is exactly right.
\begin{defn}
Let $s(X_{1},X_{2},\ldots,X_{n})$ be a statistic which estimates
$\theta$. If \[
\E s(X_{1},X_{2},\ldots,X_{n})=\theta,\]
then the statistic $s(X_{1},X_{2},\ldots,X_{n})$ is said to be an
\emph{unbiased estimator} of $\theta$. Otherwise, it is \emph{biased}.\end{defn}
\begin{example}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be an $SRS(n)$ from a
$\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
It can be shown (see Exercise \ref{xca:norm-mu-sig-MLE}) that if
we let $\mbox{\ensuremath{\theta}}=(\mu,\sigma^{2})$, then the MLE
of $\theta$ is\[
\hat{\theta}=(\hat{\mu},\hat{\sigma}^{2}),\]
 where $\hat{\mu}=\Xbar$ and \[
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\Xbar\right)^{2}=\frac{n-1}{n}S^{2}.\]
We of course know from BLANK that $\hat{\mu}$ is unbiased. What about
$\hat{\sigma^{2}}$? Let us check: \begin{eqnarray*}
\E\,\hat{\sigma^{2}} & = & \E\,\frac{n-1}{n}S^{2}\\
 & = & \E\left(\frac{\sigma^{2}}{n}\frac{(n-1)S^{2}}{\sigma^{2}}\right)\\
 & = & \frac{\sigma^{2}}{n}\E\ \mathsf{chisq}(\mathtt{df}=n-1)\\
 & = & \frac{\sigma^{2}}{n}(n-1),\end{eqnarray*}
from which we may conclude two things:
\begin{enumerate}
\item $\hat{\sigma^{2}}$ is a biased estimator of $\sigma^{2}$, and 
\item $S^{2}=n\hat{\sigma^{2}}/(n-1)$ is an unbiased estimator of $\sigma^{2}$.
\end{enumerate}
\end{example}
One of the most common questions in an introductory statistics class
is, {}``Why do we divide by $n-1$ when we compute the sample variance?
Why do we not divide by $n$?'' One answer is that division by $n$
amounts to the use of $\hat{\sigma^{2}}$ as an estimator for $\sigma^{2}$,
which we have just shown to be biased. That is, if we divided by $n$
then on the average we would \emph{underestimate} the true value of
$\sigma^{2}$. We use $n-1$ so that, on the average, our estimator
of $\sigma^{2}$ will be exactly right. 


\subsection{How to do it with \textsf{R}}

R can be used to find maximum likelihood estimators in a lot of diverse
settings. We will discuss only the most basic here and will leave
the rest to more sophisticated texts.

For one parameter estimation problems we may use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!optimize!\inputencoding{utf8}
function to find MLEs. The arguments are the function to be maximized
(the likelihood function), the range over which the optimization is
to take place, and optionally any other arguments to be passed to
the likelihood if needed.

Let us see how to do Example BLANK. Recall that our likelihood function
was given by\[
L(p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.\]
Notice that the likelihood is just a product of $\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)$
PMFs. We first give some sample data (in the vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!datavals!\inputencoding{utf8}),
next we define the likelihood function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!L!\inputencoding{utf8},
and finally we \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!optimize!\inputencoding{utf8}
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!L!\inputencoding{utf8}
over the range \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!c(0,1)!\inputencoding{utf8}.

<<>>=
x <- mtcars$am
L <- function(p,x) prod(dbinom(x, size = 1, prob = p))
optimize(L, interval = c(0,1), x = x, maximum = TRUE)
@

<<echo = FALSE>>=
A <- optimize(L, interval = c(0,1), x = x, maximum = TRUE)
@

Note that the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!optimize!\inputencoding{utf8}
function by default minimizes the function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!L!\inputencoding{utf8},
so we have to set \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!maximum = TRUE!\inputencoding{utf8}
to get an MLE. The returned value of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!$maximum!\inputencoding{utf8}
gives an approximate value of the MLE to be \Sexpr{round(A$maximum, 3)}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!$objective!\inputencoding{utf8}
gives \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!L!\inputencoding{utf8}
evaluated at the MLE which is approximately \Sexpr{round(A$objective, 3)}.

We previously remarked that it is usually more numerically convenient
to maximize the log-likelihood (or minimize the negative log-likelihood),
and we can just as easily do this with \textsf{R}. We just need to
calculate the log-likelihood beforehand which (for this example) is\[
-l(p)=-\sum x_{i}\ln\, p-\left(n-\sum x_{i}\right)\ln(1-p).\]
It is done in \textsf{R} with

<<>>=
minuslogL <- function(p,x) -sum(dbinom(x, size = 1, prob = p, log = TRUE))
optimize(minuslogL, interval = c(0,1), x = x)
@

Note that we did not need \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!maximum = TRUE!\inputencoding{utf8}
because we minimized the negative log-likelihood. The answer for the
MLE is essentially the same as before, but the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!$objective!\inputencoding{utf8}
value was different, of course.

For multiparameter problems we may use a similar approach by way of
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!mle!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!stats4!\inputencoding{utf8}
package. 
\begin{example}
\textbf{Plant Growth.} We will investigate the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!weight!\inputencoding{utf8}
variable of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!PlantGrowth!\inputencoding{utf8}
data. We will suppose that the weights constitute a random observations
$X_{1}$, $X_{2}$,\ldots{}, $X_{n}$ that are i.i.d.~$\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$
which is not unreasonable based on a histogram and other exploratory
measures. We will find the MLE of $\theta=(\mu,\sigma^{2})$. We claimed
in Example BLANK that $\hat{\theta}=(\hat{\mu},\hat{\sigma}^{2})$
had the form given above. Let us check whether this is plausible numerically.
The negative log-likelihood function is

<<keep.source = TRUE>>=
minuslogL <- function(mu, sigma2){
  -sum(dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE))
}
@

Note that we omitted the data as an argument to the log-likelihood
function; the only arguments were the parameters over which the maximization
is to take place. Now we will simulate some data and find the MLE.
The optimization algorithm requires starting values (intelligent guesses)
for the parameters. We choose values close to the sample mean and
variance (which turn out to be approximately 5 and 0.5, respectively)
to illustrate the procedure.

<<>>=
x <- PlantGrowth$weight
library(stats4)
MaxLikeEst <- mle(minuslogL, start = list(mu = 5, sigma2 = 0.5))
summary(MaxLikeEst)
@

The outputted MLEs are shown above, and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!mle!\inputencoding{utf8}
even gives us estimates for the standard errors of $\hat{\mu}$ and
$\hat{\sigma}^{2}$ (which were obtained by inverting the numerical
Hessian matrix at the optima; see Appendix BLANK). Let us check how
close the numerical MLEs came to the theoretical MLEs:

<<>>=
mean(x)
var(x)*29/30
sd(x)/sqrt(30)
@

The numerical MLEs were very close to the theoretical MLEs. We already
knew that the standard error of $\hat{\mu}=\Xbar$ is $\sigma/\sqrt{n}$,
and the numerical estimate of this was very close too.

\end{example}
There is functionality in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!distrTest!\inputencoding{utf8}
package to calculate theoretical MLEs; we will skip examples of these
for the time being.


\section{Confidence Intervals for Means}

Given $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ an $SRS(n)$ from a
$\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution
where $\mu$ is unknown. We know that we may estimate $\mu$ with
$\Xbar$, and we have seen that this estimator is the MLE. But how
good is our estimate? We know that \begin{equation}
\frac{\Xbar-\mu}{\sigma/\sqrt{n}}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).\end{equation}
For a big probability $1-\alpha$, for instance, 95\%, we can calculate
the quantile $z_{\alpha/2}$. Then \begin{equation}
\P\left(-z_{\alpha/2}\leq\frac{\Xbar-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2}\right)=1-\alpha.\end{equation}
But now consider the following string of equivalent inequalities:\[
-z_{\alpha/2}\leq\frac{\Xbar-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2},\]
\[
-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq\Xbar-\mu\leq z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right),\]
\[
-\Xbar-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq-\mu\leq-\Xbar+z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right),\]
\[
\Xbar-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq\mu\leq\Xbar+z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right).\]
That is, \begin{equation}
\P\left(\Xbar-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\mu\leq\Xbar+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)=1-\alpha.\end{equation}



\begin{defn}
The interval\begin{equation}
\left[\Xbar-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\ \Xbar+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right]\end{equation}
 is a $100(1-\alpha)\%$ \emph{confidence interval for} $\mu$. The
quantity $1-\alpha$ is called the \emph{confidence coefficient}.\end{defn}
\begin{rem}
The interval is also sometimes written more compactly as\begin{equation}
\Xbar\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}.\label{eq:z-interval}\end{equation}

\end{rem}
The interpretation of confidence intervals is tricky and often mistaken
by novices. When I am teaching the concept {}``live'' during class,
I usually ask the students to imagine that my piece of chalk represents
the {}``unknown'' parameter, and I lay it down on the desk in front
of me. Once the chalk has been lain, it is \emph{fixed}; it does not
move. Our goal is to estimate the parameter. For the estimator I pick
up a sheet of loose paper lying nearby. The estimation procedure is
to randomly drop the piece of paper from above, and observe where
it lands. If the piece of paper covers the piece of chalk, then we
are successful -- our estimator covers the parameter. If it falls
off to one side or the other, then we are unsuccessful; our interval
fails to cover the parameter.

Then I ask them: suppose we were to repeat this procedure hundreds,
thousands, millions of times. Suppose we kept track of how many times
we covered and how many times we did not. What percentage of the time
would we be successful?

In the demonstration, the parameter corresponds to the chalk, the
sheet of paper corresponds to the confidence interval, and the random
experiment corresponds to dropping the sheet of paper. The percentage
of the time that we are successful \emph{exactly} corresponds to the
\emph{confidence coefficient}. That is, if we use a 95\% confidence
interval, then we can say that, in the long run, approximately 95\%
of our intervals will cover the true parameter (which is fixed, but
unknown). 

See Figure \ref{fig:ci-examp}, which is a graphical display of these
ideas.%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 6, width = 6>>=
library(TeachingDemos)
ci.examp()
@
\par\end{centering}

\caption{Simulation experiment for confidence intervals, using \texttt{ci.examp()}
from the \texttt{TeachingDemos} package. Fifty (50) samples of size
twenty five (25) were generated from a $\mathsf{norm}(\mathtt{mean}=100,\,\mathtt{sd}=10)$
distribution, and each sample was used to find a 95\% confidence interval
for the population mean using Equation \ref{eq:z-interval}. The 50
confidence intervals are represented above by horizontal lines, and
the respective sample means are denoted by vertical slashes. Confidence
intervals that {}``cover'' the true mean value of 100 are plotted
in black; those that fail to cover are plotted in a lighter color.
In the plot we see that two (2) simulated intervals out of the 50
failed to cover $\mu=100$, which is a success rate of 96\%. As the
number of generated samples increased from 50 to 500 to 50000, \ldots{},
we would expect our success rate to approach the exact value of 95\%.\label{fig:ci-examp}}

\end{figure}


Under the above framework, we can reason that an {}``interval''
with a \emph{larger} confidence coefficient corresponds to a \emph{wider}
sheet of paper. Furthermore, the width of the confidence interval
(sheet of paper) should be \emph{somehow} related to the amount of
information contained in the random sample, $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$. The following remarks makes these notions precise. 
\begin{rem}
For a fixed confidence coefficient $1-\alpha$,\begin{equation}
\mbox{if }n\mbox{ increases, then the confidence interval gets \emph{SHORTER}.}\end{equation}

\end{rem}

\begin{rem}
For a fixed sample size $n$,\begin{equation}
\mbox{if }1-\alpha\mbox{ increases, then the confidence interval gets \emph{WIDER}.}\end{equation}
\end{rem}
\begin{example}
Give some data with $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ an $SRS(n)$
from a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
Maybe small sample?\end{example}
\begin{enumerate}
\item What is the parameter of interest? in the context of the problem.
Give a point estimate for $\mu$.
\item What are the assumptions being made in the problem? Do they meet the
conditions of the interval?
\item Calculate the interval.
\item Draw the conclusion.
\end{enumerate}
Draw a picture here.

What if $\sigma$ is unknown? We will instead use the interval\[
\Xbar\pm z_{\alpha/2}\frac{S}{\sqrt{n}},\]
where $S$ is the sample standard deviation.
\begin{itemize}
\item If $n$ is large, then $\Xbar$ will have an approximately normal
distribution regardless of the underlying population (by the CLT)
and $S$ will be very close to the parameter $\sigma$ (by the SLLN);
thus the above interval will have approximately $100(1-\alpha)\%$
confidence of covering $\mu$.
\item If $n$ is small, then 

\begin{itemize}
\item if the underlying population is normal then we may replace $z_{\alpha/2}$
with $t_{\alpha/2}(\mathtt{df}=n-1)$. The resulting $100(1-\alpha)\%$
confidence interval is\begin{equation}
\Xbar\pm t_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}}\label{eq:one-samp-t-int}\end{equation}

\item if the underlying population is not normal, but approximately normal,
then we may use the $t$ interval, Equation \ref{eq:one-samp-t-int}.
The interval will have approximately $100(1-\alpha)\%$ confidence
of covering $\mu$. However, if the population is highly skewed or
the data have outliers, then we should ask a professional statistician
for advice.
\end{itemize}
\end{itemize}
In general, with confidence interval problems it is useful to follow
a similar procedure. An acronym to summarize the procedure is PANIC:
\emph{P}arameter, \emph{A}ssumptions, \emph{N}ame, \emph{I}nterval,
and \emph{C}onclusion.
\begin{description}
\item [{Parameter:}] identify the parameter of interest with the proper
symbols. Write down what the parameter means in the context of the
problem. 
\item [{Assumptions:}] list any assumptions made in the experiment. If
there are any other assumptions needed or that were not checked, state
what they are and why they are important.
\item [{Name:}] choose a statistical procedure from your bag of tricks
based on the answers to the previous two parts. The assumptions of
the procedure you choose should match those of the problem; if they
do not match then either pick a different procedure or openly admit
that the results may not be reliable. Write down any underlying formulas
used.
\item [{Interval:}] calculate the interval from the sample data. This can
be done by hand but will more often be done with the aid of the computer.
Regardless of the method, all calculations or code should be shown
to make the entire process repeatable by a subsequent reader.
\item [{Conclusion:}] state the final results, using language in the context
of the problem. Include the appropriate interpretation of the interval,
making reference to the confidence coefficient.\end{description}
\begin{rem}
The intervals above are two-sided, but there are also one-sided intervals
for $\mu$. They look like \[
\left[\Xbar-z_{\alpha}\frac{\sigma}{\sqrt{n}},\ \infty\right)\quad\mbox{or}\quad\left(-\infty,\ \Xbar+z_{\alpha}\frac{\sigma}{\sqrt{n}}\right]\]
and satisfy\[
\P\left(\Xbar-z_{\alpha}\frac{\sigma}{\sqrt{n}}\leq\mu\right)=1-\alpha\quad\mbox{and}\quad\P\left(\Xbar+z_{\alpha}\frac{\sigma}{\sqrt{n}}\geq\mu\right)=1-\alpha.\]
\end{rem}
\begin{example}
Small sample, some data with $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$
an $SRS(n)$ from a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$
distribution.
\begin{enumerate}
\item PANIC
\end{enumerate}
\end{example}

\subsection{How to do it with \textsf{R}}

library(HH)

normal.and.t.dist(obs.mean = 56.8, std.dev = 2, n = 10, alpha.right
= 0.025, Use.alpha.left = TRUE, hypoth.or.conf = 'Conf', polygon.density
= 10 )

normal.and.t.dist(obs.mean = mean(c(37.4, 48.8, 46.9, 55, 44)), std.dev
= sd(c(37.4, 48.8, 46.9, 55, 44)), n = 5, alpha.right = 0.05, deg.freedom
= 4, Use.alpha.left = TRUE, hypoth.or.conf = 'Conf', polygon.density
= 10 ) 


\section{Confidence Intervals for Differences of Means}

Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be a $SRS(n)$ from a
$\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})$ distribution
and let $Y_{1}$, $Y_{2}$, \ldots{}, $Y_{m}$ be a $SRS(m)$ from
a $\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})$
distribution. Further, assume that the $X_{1}$, $X_{2}$, \ldots{},
$X_{n}$ sample is independent of the $Y_{1}$, $Y_{2}$, \ldots{},
$Y_{m}$ sample.

Suppose that $\sigma_{X}$ and $\sigma_{Y}$ are known. We would like
a confidence interval for $\mu_{X}-\mu_{Y}$.

We know that \begin{equation}
\Xbar-\Ybar\sim\mathsf{norm}\left(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}}\right).\end{equation}
Therefore, a $100(1-\alpha)$\% confidence interval for $\mu_{X}-\mu_{Y}$
is given by\begin{equation}
\left(\Xbar-\Ybar\right)\pm z_{\alpha/2}\sqrt{\frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}}.\end{equation}
Unfortunately, most of the time the values of $\sigma_{X}$ and $\sigma_{Y}$
are unknown. This leads us to the following:
\begin{itemize}
\item If both sample sizes are large, then we may appeal to the CLT/SLLN
(see BLANK) and substitute $S_{X}^{2}$ and $S_{Y}^{2}$ for $\sigma_{X}^{2}$
and $\sigma_{Y}^{2}$ in the interval BLANK. The resulting confidence
interval will have approximately $100(1-\alpha)\%$ confidence.
\item If one or more of the sample sizes is small then we are in trouble,
unless

\begin{itemize}
\item the underlying populations are both normal and $\sigma_{X}=\sigma_{Y}$.
In this case (setting $\sigma=\sigma_{X}=\sigma_{Y}$), \[
\Xbar-\Ybar\sim\mathsf{norm}\left(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}\right).\]
Now let \[
U=\frac{n-1}{\sigma^{2}}S_{X}^{2}+\frac{m-1}{\sigma^{2}}S_{Y}^{2}.\]
Then by BLANK we know that $U\sim\mathsf{chisq}(\mathtt{df}=n+m-2)$
and is not a large leap to believe that $U$ is independent of $\Xbar-\Ybar$;
thus \[
T=\frac{Z}{\sqrt{\left.U\right\slash (n+m-2)}}\sim\mathsf{t}(\mathtt{df}=n+m-2).\]
But\begin{align*}
T & =\frac{\frac{\Xbar-\Ybar-(\mu_{X}-\mu_{Y})}{\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}}}{\sqrt{\left.\frac{n-1}{\sigma^{2}}S_{X}^{2}+\frac{m-1}{\sigma^{2}}S_{Y}^{2}\right\slash (n+m-2)}},\\
 & =\frac{\Xbar-\Ybar-(\mu_{X}-\mu_{Y})}{\sqrt{\left(\frac{1}{n}+\frac{1}{m}\right)\left(\frac{(n-1)S_{X}^{2}+(m-1)S_{Y}^{2}}{n+m-2}\right)}},\\
 & \sim\mathsf{t}(\mathtt{df}=n+m-2).\end{align*}
Therefore a $100(1-\alpha)\%$ confidence interval for $\mu_{X}-\mu_{Y}$
is given by\[
\left(\Xbar-\Ybar\right)\pm t_{\alpha/2}(\mathtt{df}=n+m-2)\, S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}},\]
where \[
S_{p}=\sqrt{\frac{(n-1)S_{X}^{2}+(m-1)S_{Y}^{2}}{n+m-2}}\]
is called the {}``pooled'' estimator of $\sigma$.
\item if one of the samples is small, and both underlying populations are
normal, but $\sigma_{X}\neq\sigma_{Y}$, then we may use a procedure
attributed to Welch-Aspin (BLANK). The idea is to use an interval
of the form\begin{equation}
\left(\Xbar-\Ybar\right)\pm t_{\alpha/2}(\mathtt{df}=r)\,\sqrt{\frac{S_{X}^{2}}{n}+\frac{S_{Y}^{2}}{m}},\end{equation}
where the degrees of freedom $r$ is chosen so that the interval has
nice statistical properties. It turns out that a good choice for $r$
is given by\[
r=\frac{\left(S_{X}^{2}/n+S_{Y}^{2}/m\right)^{2}}{\frac{1}{n-1}\left(S_{X}^{2}/n\right)^{2}+\frac{1}{m-1}\left(S_{Y}^{2}/m\right)^{2}},\]
where we understand that $r$ is rounded down to the nearest integer.
The resulting interval has approximately $100(1-\alpha)\%$ confidence.
\end{itemize}
\end{itemize}

\subsection{How to do it with \textsf{R}}


\section{Confidence Intervals for Proportions}

We would like to know $p$ which is the {}``proportion of successes''.
For instance, $p$ could be:
\begin{itemize}
\item the proportion of U.S.~citizens that support Obama,
\item the proportion of smokers among adults age 18 or over,
\item the proportion of people worldwide infected by the H1N1 virus.
\end{itemize}
We are given an $SRS(n)$ $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$
distributed $\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)$.
Recall from Section BLANK that the mean of these random variables
is $\E X=p$ and the variance is $\E(X-p)^{2}=p(1-p)$. If we let
$Y=\sum X_{i}$, then from Section BLANK we know that $Y\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)$
and that \[
\Xbar=\frac{Y}{n}\mbox{ has }\E\Xbar=p\mbox{ and }\mathrm{Var}(\Xbar)=\frac{p(1-p)}{n}.\]
Thus if $n$ is large (here is the CLT) then an approximate $100(1-\alpha)\%$
confidence interval for $p$ would be given by\begin{equation}
\Xbar\pm z_{\alpha/2}\sqrt{\frac{p(1-p)}{n}}.\label{eq:ci-p-no-good}\end{equation}
OOPS\ldots{}! Equation \ref{eq:ci-p-no-good} is of no use to us
because the \underbar{unknown} parameter $p$ is in the formula! (If
we knew what $p$ was to plug in the formula then we would not need
a confidence interval in the first place.) There are two solutions
to this problem.
\begin{enumerate}
\item Replace $p$ with $\hat{p}=\Xbar$. Then an approximate $100(1-\alpha)\%$
confidence interval for $p$ is given by \begin{equation}
\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.\end{equation}
This approach is called the \emph{Wald interval} and is also known
as the \emph{asymptotic interval} because it appeals to the CLT for
large sample sizes.
\item Go back to first principles. Note that\[
-z_{\alpha/2}\leq\frac{Y/n-p}{\sqrt{p(1-p)/n}}\leq z_{\alpha/2}\]
exactly when the function $f$ defined by\[
f(p)=\left(Y/n-p\right)^{2}-z_{\alpha/2}^{2}\frac{p(1-p)}{n}\]
satisfies $f(p)\leq0$. But $f$ is quadratic in $p$ so its graph
is a parabola; it has two roots, and these roots form the limits of
the confidence interval. We can get an expression for the bounds by
means of the quadratic formula (see Exercise BLANK):\begin{equation}
\left.\left[\left(\hat{p}+\frac{z_{\alpha/2}^{2}}{2n}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{z_{\alpha/2}^{2}}{(2n)^{2}}}\right]\right\slash \left(1+\frac{z_{\alpha/2}^{2}}{n}\right)\end{equation}
 This approach is called the \emph{score interval} because it is based
on the inversion of the {}``Score test''. See Section BLANK. It
is also known as the \emph{Wilson interval}; see reference BLANK.
\end{enumerate}
For two proportions $p_{1}$ and $p_{2}$, we may collect independent
$\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)$ samples of size
$n_{1}$ and $n_{2}$, respectively. Let $Y_{1}$ and $Y_{2}$ denote
the number of successes in the respective samples. 

We know that \[
\frac{Y_{1}}{n_{1}}\approx\mathsf{norm}\left(\mathtt{mean}=p_{1},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}}\right)\]
and\[
\frac{Y_{2}}{n_{2}}\approx\mathsf{norm}\left(\mathtt{mean}=p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{2}(1-p_{2})}{n_{2}}}\right)\]
so it stands to reason that an approximate $100(1-\alpha)\%$ confidence
interval for $p_{1}-p_{2}$ is given by\[
\left(\hat{p}_{1}-\hat{p}_{2}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}}+\frac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}},\]
where $\hat{p}_{1}=Y_{1}/n_{1}$ and $\hat{p}_{2}=Y_{2}/n_{2}$.
\begin{rem}
When estimating a single proportion, one-sided intervals are sometimes
needed. They take the form\[
\left[0,\ \hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right]\]
or \[
\left[\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\ 1\right]\]
or in other words, we know in advance that the true proportion is
restricted to the interval $[0,1]$, so we can truncate our confidence
interval to those values on either side.
\end{rem}

\subsection{How to do it with \textsf{R}}

<<>>=
library(Hmisc)
binconf(x = 7, n = 25, method = "asymptotic")
binconf(x = 7, n = 25, method = "wilson")
@

The default value of the \inputencoding{latin9}\lstinline!method!\inputencoding{utf8}
argument is \inputencoding{latin9}\lstinline!wilson!\inputencoding{utf8}.

An alternate way is 

<<>>=
tab <- xtabs(~gender, data = RcmdrTestDrive)
prop.test(rbind(tab), conf.level = 0.95, correct = FALSE)
@

djlsd

<<>>=
A <- as.data.frame(Titanic)
library(reshape)
B <- with(A, untable(A, Freq))
@


\section{Confidence Intervals for Variances}

I am thinking one and two sample problems here.


\subsection{How to do it with \textsf{R}}

I am thinking about sigma.test() and var.test() here.


\section{Fitting Distributions}


\subsection{How to do it with R}

I am thinking about fitdistr() here.


\section{Sample Size and Margin of Error}

Sections BLANK through BLANK all began the same way: we were given
the sample size $n$ and the confidence coefficient $1-\alpha$, and
our task was to find a margin of error $E$ so that \[
\hat{\theta}\pm E\mbox{ is a }100(1-\alpha)\%\mbox{ confidence interval for }\theta.\]
Some examples we have seen are:
\begin{itemize}
\item $E=z_{\alpha/2}\sigma/\sqrt{n}$, in the one-sample $z$-interval,
\item $E=t_{\alpha/2}(\mathtt{df}=n+m-2)S_{p}\sqrt{n^{-1}+m^{-1}}$, in
the two-sample pooled $t$-interval. 
\end{itemize}
We already know (see Equation BLANK) that $E$ decreases as $n$ increases.
Now we would like to use this information to our advantage: suppose
that we have a fixed margin of error $E,$ say $E=3$, and we want
a $100(1-\alpha)\%$ confidence interval for $\mu$. The question
is: how big does $n$ have to be?

For the case of a population mean, the answer is easily obtained.
We simply set up an equation and solve for $n$.
\begin{example}
Given a situation, given $\sigma$, given $E$, we would like to know
how big $n$ has to be to ensure that $\Xbar\pm5$ is a 95\% confidence
interval for $\mu$. \end{example}
\begin{rem}
~
\begin{enumerate}
\item Always round up any decimal values of $n$, no matter how small the
decimal is.
\item Another name for $E$ is the {}``maximum error of the estimate''.
\end{enumerate}
\end{rem}
For proportions, recall that the asymptotic formula to estimate $p$
was\[
\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.\]
Reasoning as above we would want\begin{align}
E & =z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\mbox{ or}\\
n & =z_{\alpha/2}^{2}\frac{\hat{p}(1-\hat{p})}{E^{2}}.\end{align}
OOPS! (again.) Recall that $\hat{p}=Y/n$, which would put the variable
$n$ on both sides of Equation BLANK. Again, there are two solutions
to the problem.
\begin{enumerate}
\item If we have a good idea of what $p$ is, say $p^{\ast}$ then we can
plug it in to get\[
n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}.\]

\item Even if we have no idea what $p$ is, we do know from calculus that
$p(1-p)\leq1/4$ because the function $f(x)=x(1-x)$ is quadratic
(so its graph is a parabola which opens downward) with maximum value
attained at $x=1/2$. Therefore, regardless of our choice for $p^{\ast}$
the sample size must satisfy\[
n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}\leq\frac{z_{\alpha/2}^{2}}{4E^{2}}.\]
The quantity $z_{\alpha/2}^{2}/4E^{2}$ is large enough to guarantee
$100(1-\alpha)\%$ confidence.\end{enumerate}
\begin{example}
Proportion example\end{example}
\begin{rem}
For very small populations sometimes the value of $n$ obtained from
the formula is too big. In this case we should use the hypergeometric
distribution for a sampling model rather than the binomial model.
With this modification the formulas change to the following: if $N$
denotes the population size then let\[
m=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}\]
 and the sample size needed to ensure $100(1-\alpha)\%$ confidence
is achieved is\[
n=\frac{m}{1+\frac{m-1}{N}}.\]
If we do not have a good value for the estimate $p^{\ast}$ then we
may use $p^{\ast}=1/2$. 
\end{rem}

\subsection{How to do it with \textsf{R}}

I am thinking about 

power.t.test

power.prop.test

power.anova.test

also thinking about replicate


\section{Other Topics}

Mention mle from the stats4 package


\section{Chapter Exercises}
\begin{xca}
Let $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ be an $SRS(n)$ from a
$\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
Find a two-dimensional MLE for $\theta=(\mu,\sigma)$.\label{xca:norm-mu-sig-MLE}
\end{xca}

\chapter{Hypothesis Testing}

What do I want them to know:
\begin{itemize}
\item basic terminology and philosophy of the Neyman-Pearson paradigm
\item classical hypothesis tests for the standard one and two sample problems
with means and variances, and proportions.
\item introduce one-way anova, and in particular, the notion of between
versus within group variation.
\item Introduce the concept of statistical power and its relationship with
sample size
\end{itemize}

\section{Introduction}


\section{Tests for Proportions}
\begin{example}
We have a machine that makes widgets. \end{example}
\begin{itemize}
\item Under normal operation, about 0.10 of the widgets produced are defective.
\item Go out and purchase a torque converter.
\item Install the torque converter, and observe $n=100$ widgets from the
machine.
\item Let $Y=\mbox{number of defective widgets observed}$.
\end{itemize}
If
\begin{itemize}
\item $Y=0$, then the torque converter is great!
\item $Y=4$, then the torque converter seems to be helping.
\item $Y=9$, then there is not much evidence that the torque converter
helps.
\item $Y=17$, then throw away the torque converter.
\end{itemize}
We use statistics to decide. Let \[
p=\mbox{proportion of defectives produced by the machine.}\]
Before the torque converter, $p=0.10$. We installed the torque converter.
Did $p$ change? Did it go up or down? How do we decide?

One method is to observe data and construct a 95\% CI for $p$,\[
\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.\]
If the confidence interval is
\begin{itemize}
\item $[0.01,\,0.05]$, then we are 95\% confident that $0.01\leq p\leq0.05$,
and there is evidence that the torque converter is helping.
\item $[0.15,\,0.19]$, then we are 95\% confident that $0.15\leq p\leq0.19$,
and there is evidence that the torque converter is hurting.
\item $[0.07,\,0.11]$, then there is not enough evidence to conclude that
the torque converter is doing anything at all, positive or negative.
\end{itemize}

\subsection{Terminology}

The \emph{null hypothesis} $H_{0}$ is the hypothesis that nothing
has changed. For this example, the null hypothesis would be \[
H_{0}:p=0.10\]
The \emph{alternative hypothesis} $H_{1}$ is the hypothesis that
something has changed, in this case, $H_{1}:p\neq0.10$.

We wish to test the hypothesis $H_{0}:p=0.10$ versus the alternative
$H_{1}:p\neq0.10$.

How to do it:
\begin{enumerate}
\item Go out and collect some data, in particular, a simple random sample
of observations from the machine.
\item We assume that $H_{0}$ is true and construct a $100(1-\alpha)\%$
confidence interval for $p$.
\item If the confidence interval does not cover $p=0.10$, then we REJECT
$H_{0}$. Otherwise, we FAIL TO REJECT $H_{0}$.
\end{enumerate}
Remarks
\begin{itemize}
\item It is possible to be wrong. There are two types of mistakes:

\begin{itemize}
\item Type I Error: Reject $H_{0}$ when in fact, $H_{0}$ is true. This
would be akin to convicting an innocent person for a crime (s)he did
not convict.
\item Type II Error: Fail to reject $H_{0}$ when in fact, $H_{1}$ is true.
This is analogous to a guilty person going free.
\end{itemize}
\item Type I Errors are usually considered to be worse%
\footnote{There is no mathematical difference between the errors, however. The
bottom line is that we choose one type of error to control with an
iron fist, and we try to minimize the probability of making the other
type. This being said, null hypotheses are often by design to correspond
to the {}``simpler'' model, and it is easier to analyze (and thereby
control) the probabilities associated with Type I Errors.%
}, and we design our statistical procedures to control the probability
of making such a mistake. We define\[
\mbox{significance level of the test}=\P(\mbox{Type I Error})=\alpha.\]
We want $\alpha$ to be small.
\item The \emph{rejection region} for the test is the set of sample values
which would result in the rejection of $H_{0}$. This is also known
as the \emph{critical region} for the test.
\item The above example with $H_{1}:p\neq0.10$ is called a \emph{two-sided}
test. Many times we are interested in a \emph{one-sided} test, which
could look like $H_{1}:p<0.29$ or $H_{1}:p>0.34$.
\end{itemize}
We are ready for tests of hypotheses for one proportion

Table Here

Don't forget the assumptions.

PANIC
\begin{example}
Suppose $p=\mbox{proportion of BLANK who BLANK}$.

Find
\begin{enumerate}
\item The null and alternative hypotheses
\item Check your assumptions.
\item Define a critical region with an $\alpha=0.05$ significance level.
\item Calculate the value of the test statistic and state your conclusion.
\end{enumerate}
\end{example}

\begin{example}
Suppose $p=\mbox{proportion of BLANK who BLANK}$. I give you the
hypotheses up here.

What is the conclusion if the significance level is 
\begin{enumerate}
\item $\alpha=0.05$
\item $\alpha=0.01$
\end{enumerate}
\end{example}


Oops! We saw in the last example that our final conclusion changed
depending on our selection of the significance level. This is bad;
for a particular test, we would never know whether our conclusion
would have been different if we had chosen a different significance
level. Or would we?

Clearly, for some significance levels we reject, and for some significance
levels we do not. Where is the boundary? That is, what is the significance
level for which we would reject for any significance level bigger,
and we would fail to reject for any significance level smaller? This
boundary value has a special name: it is called the $p$\emph{-value}
of the test.
\begin{defn}
The $p$-value for a hypothesis test is the probability of obtaining
the observed value of $\hat{p}$, or more extreme values, when the
null hypothesis is true.\end{defn}
\begin{example}
Calculate the $p$-value for the test in Example BLANK.
\end{example}
Another way to phrase the test is, we will reject $H_{0}$ at the
$\alpha$-level of significance if the $p$-value is less than $\alpha$.
\begin{rem}
If we have two populations with proportions $p_{1}$ and $p_{2}$
then we can test the null hypothesis $H_{0}:p_{1}=p_{2}$.
\end{rem}
Table Here.
\begin{example}
Example.
\end{example}

\subsection{How to do it with \textsf{R}}

Here we find a confidence interval for $p$ and are testing hypotheses
such as\[
H_{0}:p=p_{0}\quad\mbox{versus}\quad H_{1}:p\neq p_{0}\]


<<>>=
# this is the example from the help file
nheads <- rbinom(1, size = 100, prob = 0.45)
prop.test(x = nheads, n = 100, p = 0.50, alternative = "two.sided", conf.level = 0.95, correct = TRUE)
prop.test(x = nheads, n = 100, p = 0.50, alternative = "two.sided", conf.level = 0.95, correct = FALSE)

@

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true>>=
library(HH)
plot(prop.test(x = nheads, n = 100, p = 0.50, alternative = "two.sided", conf.level = 0.95, correct = FALSE), 'Hypoth')
@
\par\end{centering}

\caption{Hypothesis test}

\end{figure}


Use Yates' continuity correction when the expected frequency of successes
is less than 10. You can use it all of the time, but you will have
a decrease in power. For large samples the correction does not matter.


\paragraph*{With the \textsf{R} Commander}

If you already know the number of successes and failures, then you
can use the menu \textsf{Statistics $\triangleright$ Proportions
$\triangleright$ IPSUR Enter table for single sample\ldots{}}

Otherwise, your data -- the raw successes and failures -- should be
in a column of the Active Data Set. Furthermore, the data must be
stored as a {}``factor'' internally. If the data are not a factor
but are numeric then you can use the menu \textsf{Data $\triangleright$
Manage variables in active data set $\triangleright$ Convert numeric
variables to factors\ldots{}} to convert the variable to a factor.
Or, you can always use the \inputencoding{latin9}\lstinline!factor!\inputencoding{utf8}
function.

Once your unsummarized data is a column, then you can use the menu
\textsf{Statistics $\triangleright$ Proportions $\triangleright$
Single-sample proportion test\ldots{}}


\section{One Sample Tests for Means and Variances}


\subsection{For Means}

Here, $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ are a $SRS(n)$ from
a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
We would like to test $H_{0}:\mu=\mu_{0}$.

Case A: Suppose $\sigma$ is known. Then under $H_{0}$,\[
Z=\frac{\Xbar-\mu_{0}}{\sigma/\sqrt{n}}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).\]


Table Here.

Case B: When $\sigma$ is unknown, under $H_{0}$\[
T=\frac{\Xbar-\mu_{0}}{S/\sqrt{n}}\sim\mathsf{t}(\mathtt{df}=n-1).\]
Table Here.

Remark: If $\sigma$ is unknown but $n$ is large then we can use
the $z$-test.
\begin{example}
Let $X$= BLANK.
\begin{enumerate}
\item Find the null and alternative hypotheses.
\item Choose a test and find the critical region.
\item Calculate the value of the test statistic and state the conclusion.
\item Find the $p$-value.
\end{enumerate}
\end{example}

\begin{rem}
Remarks
\begin{itemize}
\item $p$-values are also known as tail end probabilities. We reject $H_{0}$
when the $p$-value is small.
\item $\sigma/\sqrt{n}$ when $\sigma$ is known, is called the standard
error of the sample mean. In general, if we have an estimator $\hat{\theta}$
then $\sigma_{\hat{\theta}}$ is called the standard error of $\hat{\theta}$.
We usually need to estimate $\sigma_{\hat{\theta}}$ with $\hat{\sigma_{\hat{\theta}}}$.
\end{itemize}
\end{rem}

\subsection{Tests for a Variance}

Here, $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ are a $SRS(n)$ from
a $\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$ distribution.
We would like to test $H_{0}:\sigma^{2}=\sigma_{0}$. We know that
under $H_{0}$,\[
X^{2}=\frac{(n-1)S^{2}}{\sigma^{2}}\sim\mathsf{chisq}(\mathtt{df}=n-1).\]


Table here.
\begin{example}
Give some data and a hypothesis.
\begin{enumerate}
\item Give an $\alpha$-level and test the critical region way
\item Find the $p$-value for the test.
\end{enumerate}
\end{example}

\subsection{How to do it with \textsf{R}}

z.test() in TeachingDemos

t.test()

For the Mean when the Variance is Known

Here we find a confidence interval for $\mu$ and are testing the
hypotheses (such as)\[
H_{0}:\mu=\mu_{0}\quad\mbox{versus}\quad H_{1}:\mu\neq\mu_{0}\]


For these procedures, the standard deviation $\sigma$ should be known
in advance.

<<>>=
x <- rnorm(37, mean = 2, sd = 3)
library(TeachingDemos)
z.test(x, mu = 1, sd = 3, conf.level = 0.90)
@

library(HH)

normal.and.t.dist(mu.H0 = 3.4, obs.mean = 3.556, std.dev = 0.167,
n = 9, alpha.right = 0.05, deg.freedom = 8, Use.obs.mean = TRUE, polygon.density
= 10 )

old.omd <- par(omd=c(.05,.88, .05,1)) 

chisq.setup(df=12) 

chisq.curve(df=12, col='blue') 

chisq.observed(22, df=12) 

par(old.omd)

old.omd <- par(omd=c(.05,.88, .05,1)) 

chisq.setup(df=12) 

chisq.curve(df=12, col='blue', alpha=c(.05, .05)) 

par(old.omd) 


\paragraph*{How to do it with the \textsf{R} Commander }

Can't do it with the \textsf{R} Commander (yet).


\subsection{How to do it with \textsf{R}}

<<>>=
x <- rnorm(13, mean = 2, sd = 3)
t.test(x, mu = 0, conf.level = 0.90, alternative = "greater")
@


\paragraph*{With the \textsf{R} Commander}

Your data should be in a single numeric column (a variable) of the
Active Data Set. Use the menu \textsf{Statistics $\triangleright$
Means $\triangleright$ Single-sample t-test\ldots{}}


\section{Two-Sample Tests for Means and Variances}

The basic idea for this section is the following. We have $X\sim\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})$
and $Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})$.
distributed independently. We would like to know whether $X$ and
$Y$ come from the same population distribution, that is, we would
like to know:\[
\mbox{Does }X\overset{\mathrm{d}}{=}Y?\]
where the symbol $\overset{\mathrm{d}}{=}$ means equality of probability
distributions.

Since both $X$ and $Y$ are normal, we may rephrase the question:\[
\mbox{Does }\mu_{X}=\mu_{Y}\mbox{ and }\sigma_{X}=\sigma_{Y}?\]
Suppose first that we do not know the values of $\sigma_{X}$ and
$\sigma_{Y}$, but we know that they are equal, $\sigma_{X}=\sigma_{Y}$.
Our test would then simplify to $H_{0}:\mu_{X}=\mu_{Y}$. We collect
data $X_{1}$, $X_{2}$, \ldots{}, $X_{n}$ and $Y_{1}$, $Y_{2}$,
\ldots{}, $Y_{m}$, both simple random samples of size $n$ and $m$
from their respective normal distributions. Then under $H_{0}$ (that
is, assuming $H_{0}$ is true) we have $\mu_{X}=\mu_{Y}$ or rewriting,
$\mu_{X}-\mu_{Y}=0$, so \[
T=\frac{\Xbar-\Ybar}{S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}}}=\frac{\Xbar-\Ybar-(\mu_{X}-\mu_{Y})}{S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}}}\sim\mathsf{t}(\mathtt{df}=n+m-2).\]



\subsection{Independent Samples}
\begin{rem}
If the values of $\sigma_{X}$ and $\sigma_{Y}$ are known, then we
can plug them in to our statistic:\[
Z=\frac{\Xbar-\Ybar}{\sqrt{\sigma_{X}^{2}/n+\sigma_{Y}^{2}/m}};\]
the result will have a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
distribution when $H_{0}:\mu_{X}=\mu_{Y}$ is true.
\end{rem}

\begin{rem}
Even if the values of $\sigma_{X}$ and $\sigma_{Y}$ are not known,
if both $n$ and $m$ are large then we can plug in the sample estimates
and the result will have approximately a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
distribution when $H_{0}:\mu_{X}=\mu_{Y}$ is true.\[
Z=\frac{\Xbar-\Ybar}{\sqrt{S_{X}^{2}/n+S_{Y}^{2}/m}}.\]

\end{rem}

\begin{rem}
It is usually important to construct side-by-side boxplots and other
visual displays in concert with the hypothesis test. This gives a
visual comparison of the samples and helps to identify departures
from the test's assumptions -- such as outliers.
\end{rem}

\begin{rem}
WATCH YOUR ASSUMPTIONS.
\begin{itemize}
\item The normality assumption can be relaxed as long as the population
distributions are not highly skewed.
\item The equal variance assumption can be relaxed as long as both sample
sizes $n$ and $m$ are large. However, if one (or both) samples is
small, then the test does not perform well; we should instead use
the methods of Chapter BLANK. See Section BLANK.
\end{itemize}
\end{rem}
For a nonparametric alternative to the two-sample $F$ test see Section
BLANK.


\subsection{Paired Samples}

t.test(extra \textasciitilde{} group, data = sleep, paired = TRUE)


\subsection{How to do it with R}


\section{Analysis of Variance}

For example do lm(weight \textasciitilde{} feed, data = chickwts) 

with(chickwts, by(weight, feed, shapiro.test)

Plot for the intuition of between versus within

AND%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig = true>>=
y1 <- rnorm(300, mean = c(2,8,22))
plot(y1, xlim = c(-1,25), ylim = c(0,0.45) , type = "n")
f <- function(x){dnorm(x, mean = 2)}
curve(f, from = -1, to = 5, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 8)}
curve(f, from = 5, to = 11, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 22)}
curve(f, from = 19, to = 25, add = TRUE, lwd = 2)
rug(y1)
@
\par\end{centering}

\caption{Between versus within}

\end{figure}




Plots for the hypothesis tests:%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig = true>>=
y2 <- rnorm(300, mean = c(4,4.1,4.3))
hist(y2, 30, prob = TRUE)
f <- function(x){dnorm(x, mean = 4)/3}
curve(f, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 4.1)/3}
curve(f, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 4.3)/3}
curve(f, add = TRUE, lwd = 2)
@
\par\end{centering}

\caption{Between versus within}

\end{figure}
lkjljdflsjdljsdlsdfljsldsd%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig = true>>=
library(HH)
old.omd <- par(omd = c(.05,.88, .05,1))
F.setup(df1 = 5, df2 = 30)
F.curve(df1 = 5, df2 = 30, col='blue')
F.observed(3, df1 = 5, df2 = 30)
par(old.omd)
@
\par\end{centering}

\caption{djdfd}



\end{figure}


hdskfs

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig = true>>=
library(HH)
old.omd <- par(omd = c(.05,.88, .05,1))
F.setup(df1 = 5, df2 = 30)
F.curve(df1 = 5, df2 = 30, col = 'blue', alpha = c(.05, .05))
par(old.omd)
@
\par\end{centering}

\caption{Graph of a single sample test for variance}

\end{figure}



\section{Sample Size and Power}

We have seen and discussed a

The power function of a test for a parameter $\theta$ is\[
\beta(\theta)=\P_{\theta}(\mbox{Reject }H_{0}),\quad-\infty<\theta<\infty.\]
Here are some properties of power functions:
\begin{enumerate}
\item $\beta(\theta)\leq\alpha$ for any $\theta\in\Theta_{0}$, and $\beta(\theta_{0})=\alpha$.
We interpret this by saying that no matter what value $\theta$ takes
inside the null parameter space, there is never more than a chance
of $\alpha$ of rejecting the null hypothesis. We have controlled
the Type I error rate to be no greater than $\alpha$.
\item $\lim_{n\to\infty}\beta(\theta)=1$ for any fixed $\theta\in\Theta_{1}$.
In other words, as the sample size grows without bound we are able
to detect nonnull values of $\theta$ with increasing accuracy, no
matter how close it lies to the null parameter space. This may appear
to be a good thing at first glance, but it often turns out to be a
curse. For notice that another interpretation is that our Type II
error rate grows as the sample size increases. 
\end{enumerate}
The meaning of the 


\subsection{How to do it with R}

I am thinking about replicate() here.


\section{Chapter Exercises}


\chapter{Simple Linear Regression}

What do I want them to know?
\begin{itemize}
\item basic philosophy of SLR and the regression assumptions
\item point and interval estimation of the parameters of the linear model
\item point and interval estimation of future observations from the model
\item regression diagnostics including $R^{2}$ and residual analysis
\end{itemize}

\section{Basic Philosophy\label{sec:Basic-Philosophy}}

Here we have two variables $X$ and $Y$. For our purposes, $X$ is
not random (so we will write $x$), but $Y$ is random. We believe
that $Y$ depends in \emph{some} way on $x$. Some typical examples
of $(x,Y)$ pairs are
\begin{itemize}
\item $x=$ study time and $Y=$ score on a test.
\item $x=$ height and $Y=$ weight.
\item $x=$ smoking frequency and $Y=$ age of first heart attack.
\end{itemize}
Given information about the relationship between $x$ and $Y$, we
would like to \emph{predict} future values of $Y$ for particular
values of $x$. This turns out to be a difficult problem%
\footnote{Yogi Berra once said, {}``It is always difficult to make predictions,
especially about the future.''%
}, so instead we first tackle an easier problem: we estimate $\E Y$.
How can we accomplish this? Well, we know that $Y$ depends somehow
on $x,$ so it stands to reason that\begin{equation}
\E Y=\mu(x),\ \mbox{a function of }x.\end{equation}
But we should be able to say more than that. To focus our efforts
we impose some structure on the functional form of $\mu$. For instance,
\begin{itemize}
\item if $\mu(x)=\beta_{0}+\beta_{1}x$, we try to estimate $\beta_{0}$
and $\beta_{1}$.
\item if $\mu(x)=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}$ , we try to estimate
$\beta_{0}$, $\beta_{1}$, and $\beta_{2}$.
\item if $\mu(x)=\beta_{0}\me^{\beta_{1}x}$, we try to estimate $\beta_{0}$
and $\beta_{1}$.
\end{itemize}
This helps us in the sense that we concentrate on the estimation of
just a few parameters, $\beta_{0}$ and $\beta_{1}$, say, rather
than some nebulous function. Our \emph{modus operandi} is simply to
perform the random experiment $n$ times and observe the $n$ ordered
pairs of data $(x_{1},Y_{1}),\ (x_{2},Y_{2}),\ \ldots,(x_{n},Y_{n})$.
We use these $n$ data points to estimate the parameters.

More to the point, there are \emph{three simple linear regression
(SLR) assumptions} that will form the basis for the rest of this chapter:
\begin{assumption}
We assume that $\mu$ is a linear function of $x$, that is, \begin{equation}
\mu(x)=\beta_{0}+\beta_{1}x,\end{equation}
\textup{\emph{where $\beta_{0}$ and $\beta_{1}$ are unknown constants
to be estimated}}\textup{.}
\end{assumption}

\begin{assumption}
We further assume that $Y_{i}$ is $\mu(x_{i})$ -- the {}``signal''
-- plus some {}``error'' (represented by the symbol $\epsilon_{i}$):\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\quad i=1,2,\ldots,n.\end{equation}

\end{assumption}

\begin{assumption}
We lastly assume that the errors are i.i.d.~normal with mean 0 and
variance $\sigma^{2}$:\begin{equation}
\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{n}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma).\end{equation}
\end{assumption}
\begin{rem}
We assume both the normality of the errors $\epsilon$ and the linearity
of the mean function $\mu$. Recall from Proposition BLANK of Chapter
BLANK that if $(X,Y)\sim\mathsf{mvnorm}$ then the mean of $Y|x$
is a linear function of $x$. This is not a coincidence. In more advanced
classes we study the case that both $X$ and $Y$ are random, and
in particular, when they are jointly normally distributed.
\end{rem}

\subsection*{What does it all mean?}

See Figure \ref{fig:philosophy}. Shown in the figure is a solid line,
the regression line $\mu$, which in this display has slope $0.5$
and $y$-intercept 2.5, that is, $\mu(x)=2.5+0.5x$. The intuition
is that for each given value of $x$, we observe a random value of
$Y$ which is normally distributed with a mean equal to the height
of the regression line at that $x$ value. Normal densities are superimposed
on the plot to drive this point home; in principle, the densities
stand outside of the page, perpendicular to the plane of the paper.
The figure shows three such values of $x$, namely, $x=1$, $x=2.5$,
and $x=4$. Not only do we assume that the observations at the three
locations are independent, but we also assume that their distributions
have the same spread. In mathematical terms this means that the normal
densities all along the line have identical standard deviations --
there is no {}``fanning out'' or {}``scrunching in'' of the normal
densities as $x$ increases%
\footnote{In practical terms, this constant variance assumption is often violated,
in that we often observe scatterplots that fan out from the line as
$x$ gets large or small. We say under those circumstances that the
data show \emph{heteroscedasticity}. There are methods to address
it, but they fall outside the realm of SLR. %
}. 
\begin{quotation}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 5, width = 6>>=
 # open window
plot(c(0,5), c(0,6.5), type = "n", xlab="x", ylab="y")
## the x- and y-axes
abline(h=0, v=0, col = "gray60")
# regression line
abline(a = 2.5, b = 0.5, lwd = 2)
# normal curves
x <- 600:3000/600
y <- dnorm(x, mean = 3, sd = 0.5)
lines(y + 1.0, x)
lines(y + 2.5, x + 0.75)
lines(y + 4.0, x + 1.5)
# pretty it up
abline(v = c(1, 2.5, 4), lty = 2, col = "grey")
segments(1,3, 1+dnorm(0,0,0.5),3, lty = 2, col = "gray")
segments(2.5, 3.75, 2.5+dnorm(0,0,0.5), 3.75, lty = 2, col = "gray")
segments(4,4.5, 4+dnorm(0,0,0.5),4.5, lty = 2, col = "gray")
@
\par\end{centering}

\caption{Philosophical foundations\label{fig:philosophy}}

\end{figure}
\end{quotation}
\begin{example}
\label{exa:Speed-and-Stopping}Speed and Stopping Distance of Cars 

We will use the data frame \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!datasets!\inputencoding{utf8}
package. It has two variables: \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dist!\inputencoding{utf8}.
We can take a look at some of the values in the data frame: 

<<>>=
head(cars)
@

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}
represents how fast the car was going ($x$) in miles per hour and
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dist!\inputencoding{utf8}
($Y$) measures how far it took the car to stop, in feet. We can make
a simple scatterplot of the data with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!plot(dist ~ speed, data = cars)!\inputencoding{utf8}. 
\begin{quotation}
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(dist ~ speed, data = cars)
@
\par\end{centering}

\caption{Scatterplot of the cars data\label{fig:Scatter-cars}}

\end{figure}

\end{quotation}
You can see the output in Figure \ref{fig:Scatter-cars}. 

\end{example}

\section{Estimation\label{sec:Point-Estimation}}


\subsection{Point Estimates of the Parameters}

Where is $\mu(x)$? In essence, we would like to {}``fit'' a line
to the points. But how do we determine a {}``good'' line? Is there
a \emph{best} line? We will use maximum likelihood to find it. We
know:\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\quad i=1,\ldots,n,\end{equation}
where the $\epsilon_{i}$'s are i.i.d.~$\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)$.
Thus $Y_{i}\sim\mathsf{norm}(\mathtt{mean}=\beta_{0}+\beta_{1}x_{i},\,\mathtt{sd}=\sigma),\ i=1,\ldots,n$.
Furthermore, $Y_{1},\ldots,Y_{n}$ are independent -- but not identically
distributed. The likelihood function is:\begin{alignat}{1}
L(\beta_{0},\beta_{1},\sigma)= & \prod_{i=1}^{n}f_{Y_{i}}(y_{i}),\\
= & \prod_{i=1}^{n}(2\pi\sigma^{2})^{-1/2}\exp\left\{ \frac{-(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} ,\\
= & (2\pi\sigma^{2})^{-n/2}\exp\left\{ \frac{-\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} .\end{alignat}
We take the natural logarithm to get\begin{equation}
\ln L(\beta_{0},\beta_{1},\sigma)=-\frac{n}{2}\ln(2\pi\sigma^{2})-\frac{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}.\label{eq:regML-lnL}\end{equation}
 We would like to maximize this function of $\beta_{0}$ and $\beta_{1}$.
See Appendix BLANK, which tells us that we should find critical points
by means of the partial derivatives. Let us start by differentiating
with respect to $\beta_{0}$: \begin{equation}
\frac{\partial}{\partial\beta_{0}}\ln L=0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(y_{i}-\beta_{0}-\beta_{1}x_{i})(-1),\end{equation}
and the partial derivative equals zero when $\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})=0$,
that is, when\begin{equation}
n\beta_{0}+\beta_{1}\sum_{i=1}^{n}x_{i}=\sum_{i=1}^{n}y_{i}.\label{eq:regML-a}\end{equation}
Moving on, we next take the partial derivative of $\ln L$ (equation
\ref{eq:regML-lnL}) with respect to $\beta_{1}$ to get \begin{alignat}{1}
\frac{\partial}{\partial\beta_{1}}\ln L=\  & 0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(y_{i}-\beta_{0}-\beta_{1}x_{i})(-x_{i}),\\
= & \frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}y_{i}-\beta_{0}x_{i}-\beta_{1}x_{i}^{2}\right),\end{alignat}
and this equals zero when the last sum equals zero, that is, when
\begin{equation}
\beta_{0}\sum_{i=1}^{n}x_{i}+\beta_{1}\sum_{i=1}^{n}x_{i}^{2}=\sum_{i=1}^{n}x_{i}y_{i}.\label{eq:regML-b}\end{equation}
Solving the system of equations \ref{eq:regML-a} and \ref{eq:regML-b}
\begin{eqnarray}
n\beta_{0}+\beta_{1}\sum_{i=1}^{n}x_{i} & = & \sum_{i=1}^{n}y_{i}\\
\beta_{0}\sum_{i=1}^{n}x_{i}+\beta_{1}\sum_{i=1}^{n}x_{i}^{2} & = & \sum_{i=1}^{n}x_{i}y_{i}\end{eqnarray}
for $\beta_{0}$ and $\beta_{1}$ (in Exercise BLANK) gives \begin{equation}
\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}x_{i}y_{i}-\left.\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)\right\slash n}{\sum_{i=1}^{n}x_{i}^{2}-\left.\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right\slash n}\end{equation}
and\begin{equation}
\hat{\beta}_{0}=\ybar-\hat{\beta}_{1}\xbar.\end{equation}
The conclusion? To estimate the mean line \begin{equation}
\mu(x)=\beta_{0}+\beta_{1}x,\end{equation}
 we use the {}``line of best fit''\begin{equation}
\hat{\mu}(x)=\hat{\beta}_{0}+\hat{\beta}_{1}x,\end{equation}
where $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ are given as above.
For notation we will usually write $b_{0}=\hat{\beta_{0}}$ and $b_{1}=\hat{\beta_{1}}$
so that $\hat{\mu}(x)=b_{0}+b_{1}x$.
\begin{rem}
The formula for $b_{1}$ in Equation BLANK gets the job done, but
does not really make any sense. There are many equivalent formulas
for $b_{1}$ that are more intuitive, or at the least are easier to
remember. One of the author's favorites is\begin{equation}
b_{1}=r\frac{s_{y}}{s_{x}},\end{equation}
where $r$, $s_{y}$, and $s_{x}$ are the sample correlation coefficient
and the sample standard deviations of the $Y$ and $x$ data, respectively.
See Exercise BLANK. Also, notice the similarity between Equation BLANK
and Equation BLANK.
\end{rem}

\subsection*{How to do it with \textsf{R}}

Here we go. \textsf{R} will calculate the linear regression line with
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lm!\inputencoding{utf8}
function. We will store the result in an object which we will call
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}.
Here is how it works:

<<>>=
cars.lm <- lm(dist ~ speed, data = cars)
@

The first part of the input to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lm!\inputencoding{utf8}
function, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!dist~speed!\inputencoding{utf8},
is a \emph{model formula}, read as {}``\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dist!\inputencoding{utf8}
is described by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}''.
The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!data = cars!\inputencoding{utf8}
argument tells \textsf{R} where to look for the variables quoted in
the model formula. The output object \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}
contains a multitude of information. Let's first take a look at the
coefficients of the fitted regression line, which are extracted by
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!coef!\inputencoding{utf8}
function%
\footnote{Alternatively, we could just type \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}
to see the same thing.%
}:

<<>>=
coef(cars.lm)
@

The parameter estimates $b_{0}$ and $b_{1}$ for the intercept and
slope, respectively, are shown above. The regression line is thus
given by $\hat{\mu}(\mathtt{speed})=$ \Sexpr{round(coef(cars.lm)[1],2)}
$+$ \Sexpr{round(coef(cars.lm)[2],2)}$\mathtt{speed}$.

It is good practice to visually inspect the data with the regression
line added to the plot. To do this we first scatterplot the original
data and then follow with a call to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!abline!\inputencoding{utf8}
function. The inputs to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!abline!\inputencoding{utf8}
are the coefficients of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}
(see Figure \ref{fig:Scatter-cars-regline}): %
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(dist ~ speed, data = cars)
abline(coef(cars.lm))
@
\par\end{centering}

\caption{Scatterplot of the cars data with added regression line\label{fig:Scatter-cars-regline}}

\end{figure}


<<eval = FALSE>>=
plot(dist ~ speed, data = cars)
abline(coef(cars))
@

To calculate points on the regression line we may simply plug the
desired $x$ value(s) into $\hat{\mu}$, either by hand, or with the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!predict!\inputencoding{utf8}
function. The inputs to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!predict!\inputencoding{utf8}
are the fitted linear model object, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8},
and the desired $x$ value(s) represented by a data frame. See the
example below.
\begin{example}
Using the regression line for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
data:
\begin{enumerate}
\item What is the meaning of $\mu(60)=\beta_{0}+\beta_{1}(8$)? 


This represents the average stopping distance (in feet) for a car
going 8\,mph. 

\item Interpret the slope $\beta_{1}$. 


The true slope $\beta_{1}$ represents the increase in average stopping
distance for each mile per hour faster that the car drives. In this
case, we estimate the car to take approximately \Sexpr{round(coef(cars.lm)[2],2)}
additional feet to stop for each additional mph increase in speed.

\item Interpret the intercept $\beta_{0}$. 


This would represent the mean stopping distance for a car traveling
0\,mph (which our regression line estimates to be \Sexpr{round(coef(cars.lm)[1],2)}).
Of course, this interpretation does not make any sense for this example,
because a car travelling 0\,mph takes 0~ft to stop (it was not moving
in the first place)! What went wrong? Looking at the data, we notice
that the smallest speed for which we have measured data is 4\,mph.
Therefore, if we predict what would happen for slower speeds then
we would be \emph{extrapolating}, a dangerous practice which often
gives nonsensical results.

\end{enumerate}
\end{example}

\subsection{Point Estimates of the Regression Line}

We said at the beginning of the chapter that our goal was to estimate
$\mu=\E Y$, and the arguments Section BLANK showed how to obtain
an estimate $\hat{\mu}$ of $\mu$ when the regression assumptions
hold. Now we will reap the benefits of our work in more ways than
we previously disclosed. Given a particular value $x_{0}$, there
are two values we would like to estimate:
\begin{enumerate}
\item the mean value of $Y$ at $x_{0}$, and
\item a future value of $Y$ at $x_{0}$.
\end{enumerate}
The first is a number, $\mu(x_{0})$, and the second is a random variable,
$Y(x_{0})$, but our point estimate is the same for both: $\hat{\mu}(x_{0})$.
\begin{example}
We may use the regression line to obtain a point estimate of the mean
stopping distance for a car traveling 8\,mph: $\hat{\mu}(15)=b_{0}+8b_{1}\approx$
\Sexpr{round(coef(cars.lm)[1],2)} $+(8)$ (\Sexpr{round(coef(cars.lm)[2],2)})$\approx13.88$.
We would also use 13.88 as a point estimate for the stopping distance
of a future car traveling 8\,mph.
\end{example}
Note that we actually have observed data for a car traveling 8\,mph;
its stopping distance was 16\,ft as listed in the fifth row of the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!cars!\inputencoding{utf8}
data:

<<>>=
cars[5, ]
@

There is a special name for estimates $\hat{\mu}(x_{0})$ when $x_{0}$
matches an observed value $x_{i}$ from the data set. They are called
\emph{fitted values}, they are denoted by $\hat{Y}_{1}$, $\hat{Y}_{2}$,
\ldots{}, $\hat{Y}_{n}$ (ignoring repetition), and they play an
important role in the sections that follow. 

In an abuse of notation we will sometimes write $\hat{Y}$ or $\hat{Y}(x_{0})$
to denote a point on the regression line even when $x_{0}$ does not
belong to the original data if the context of the statement obviates
any danger of confusion.

We saw in Example BLANK that spooky things can happen when we are
cavalier about point estimation. While it is usually acceptable to
predict/estimate at values of $x_{0}$ that fall within the range
of the original $x$ data, it is reckless to use $\hat{\mu}$ for
point estimates at locations outside that range. Such estimates are
usually worthless. \emph{Do not extrapolate} unless there are compelling
external reasons, and even then, temper it with a good deal of caution.


\subsection*{How to do it with R}

The fitted values are automatically computed as a byproduct of the
model fitting procedure and are already stored as a component of the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!cars.lm!\inputencoding{utf8}
object. We may access them with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!fitted!\inputencoding{utf8}
function (we only show the first five entries):

<<>>=
fitted(cars.lm)[1:5]
@

Predictions at $x$ values that are not necessarily part of the original
data are done with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!predict!\inputencoding{utf8}
function. The first argument is the original \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!cars.lm!\inputencoding{utf8}
object and the second argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!newdata!\inputencoding{utf8}
accepts a dataframe (in the same form that was used to fit \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!cars.lm!\inputencoding{utf8})
that contains the locations at which we are seeking predictions.

Let us predict the average stopping distances of cars traveling 6\,mph,
8\,mph, and 21\,mph:

<<>>=
predict(cars.lm, newdata = data.frame(speed = c(6, 8, 21)))
@

Note that there were no observed cars that traveled 6\,mph or 21\,mph.
Also note that our estimate for a car traveling 8\,mph matches the
value we computed by hand in Example BLANK.


\subsection{Mean Square Error and Standard Error}

To find the MLE of $\sigma^{2}$ we consider the partial derivative\begin{equation}
\frac{\partial}{\partial\sigma^{2}}\ln L=\frac{n}{2\sigma^{2}}-\frac{1}{2(\sigma^{2})^{2}}\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2},\end{equation}
and after plugging in $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ and
setting equal to zero we get\begin{equation}
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i})^{2}=\frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{\mu}(x_{i})]^{2}.\end{equation}
We write $\hat{Yi}=\hat{\mu}(x_{i})$, and we let $E_{i}=Y_{i}-\hat{Y_{i}}$
be the $i^{\mathrm{th}}$ \emph{residual}. We see \begin{equation}
n\hat{\sigma^{2}}=\sum_{i=1}^{n}E_{i}^{2}=SSE=\mbox{ the sum of squared errors.}\end{equation}
For a point estimate of $\sigma^{2}$ we use the \emph{mean square
error} $S^{2}$ defined by \begin{equation}
S^{2}=\frac{SSE}{n-2},\end{equation}
and we estimate $\sigma$ with the \emph{standard error} $S=\sqrt{S^{2}}$.
%
\footnote{Be careful not to confuse the mean square error $S^{2}$ with the
sample variance $S^{2}$ in Chapter BLANK. Other notation the reader
may encounter is the lowercase $s^{2}$ or the bulky $MSE$.%
}


\subsection*{How to do it with \textsf{R}}

The residuals for the model may be obtained with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!residuals!\inputencoding{utf8}
function; we only show the first few entries in the interest of space:

<<>>=
residuals(cars.lm)[1:5]
@

In the last section, we calculated the fitted value for $x=8$ and
found it to be approximately $\hat{\mu}(8)\approx$\Sexpr{round(predict(cars.lm, newdata = data.frame(speed = 8)), 2)}.
Now, it turns out that there was only one recorded observation at
$x=8$, and we have seen this value in the output of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!head(cars)!\inputencoding{utf8}
in Example \ref{exa:Speed-and-Stopping}; it was $\mathtt{dist}=16$~ft
for a car with $\mathtt{speed}=8$~mph. Therefore, the residual should
be $E=Y-\hat{Y}$ which is $E\approx16-$\Sexpr{round(predict(cars.lm, newdata = data.frame(speed = 8)), 2)}.
Now take a look at the last entry of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!residuals(cars.lm)!\inputencoding{utf8},
above. It is not a coincidence.

The estimate $S$ for $\sigma$ is called the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Residual standard error!\inputencoding{utf8}
and for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
data is shown a few lines up on the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(cars.lm)!\inputencoding{utf8}
output (see How to do it with \textsf{R} in Section BLANK). We may
read it from there to be $S\approx$\Sexpr{round(summary(cars.lm)$sigma, 2)},
or we can access it directly from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
object.

<<>>=
carsumry <- summary(cars.lm)
carsumry$sigma
@


\subsection{Interval Estimates of the Parameters}

We discussed general interval estimation in Chapter BLANK. There we
found that we could use what we know about the sampling distribution
of certain statistics to construct confidence intervals for the parameter
being estimated. We will continue in that vein, and to get started
we will determine the sampling distributions of the parameter estimates,
$b_{1}$ and $b_{0}$.

To that end, we can see from Equation BLANK (and it is made clear
in Chapter BLANK) that $b_{1}$ is just a linear combination of normally
distributed random variables, so $b_{1}$ is normally distributed
too. Further, it can be shown that\begin{equation}
b_{1}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{1},\,\mathtt{sd}=\sigma_{b_{1}}\right)\end{equation}
 where \begin{equation}
\sigma_{b_{1}}=\frac{\sigma}{\sqrt{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}\end{equation}
is called \emph{the standard error of} $b_{1}$ which unfortunately
depends on the unknown value of $\sigma$. We do not lose heart, though,
because we can estimate $\sigma$ with the standard error $S$ from
the last section. This gives us an estimate $S_{b_{1}}$ for $\sigma_{b_{1}}$
defined by\begin{equation}
S_{b_{1}}=\frac{S}{\sqrt{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}.\end{equation}
Now, it turns out that $b_{0}$, $b_{1}$, and $S$ are mutually independent
(see the footnote in Section BLANK). Therefore, the quantity\begin{equation}
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}\end{equation}
has a $\mathsf{t}(\mathtt{df}=n-2)$ distribution. Therefore, a $100(1-\alpha)\%$
confidence interval for $\beta_{1}$ is given by \begin{equation}
b_{1}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{1}}\end{equation}


It is also sometimes of interest to construct a confidence interval
for $\beta_{0}$ in which case we will need the sampling distribution
of $b_{0}$. It is shown in Chapter BLANK that\begin{equation}
b_{0}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{0},\,\mathtt{sd}=\sigma_{b_{0}}\right),\end{equation}
where $\sigma_{b_{0}}$ is given by\begin{equation}
\sigma_{b_{0}}=\sigma\sqrt{\frac{1}{n}+\frac{\xbar^{2}}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}},\end{equation}
and which we estimate with the $S_{b_{0}}$ defined by\begin{equation}
S_{b_{0}}=S\sqrt{\frac{1}{n}+\frac{\xbar^{2}}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}.\end{equation}
Thus the quantity\begin{equation}
T=\frac{b_{0}-\beta_{0}}{S_{b_{0}}}\end{equation}
has a $\mathsf{t}(\mathtt{df}=n-2)$ distribution and a $100(1-\alpha)\%$
confidence interval for $\beta_{0}$ is given by\begin{equation}
b_{0}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{0}}\end{equation}



\subsection*{How to do it with \textsf{R}}

Let us take a look at the output from \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(cars.lm)!\inputencoding{utf8}:

<<>>=
summary(cars.lm)
@
<<echo = FALSE>>=
A <- round(summary(cars.lm)$coef, 3)
B <- round(confint(cars.lm), 3)
@

In the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Coefficients!\inputencoding{utf8}
section we find the parameter estimates and their respective standard
errors in the second and third columns; the other columns are discussed
in Section BLANK. If we wanted, say, a 95\% confidence interval for
$\beta_{1}$ we could use $b_{1}=\ $\Sexpr{A[2,1]} and $S_{b_{1}}=\ $\Sexpr{A[2,2]}
together with a $\mathsf{t}_{0.025}(\mathtt{df}=23)$ critical value
to calculate $b_{1}\pm\mathsf{t}_{0.025}(\mathtt{df}=23)S_{b_{1}}$.

Or, we could use the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!confint!\inputencoding{utf8}
function.

<<>>=
confint(cars.lm)
@

With 95\% confidence, the random interval {[}\Sexpr{B[2,1]}, \Sexpr{B[2,2]}{]}
covers the parameter $\beta_{1}$.


\subsection{Interval Estimates of the Regression Line}

We have seen how to estimate the coefficients of regression line with
both point estimates and confidence intervals. We even saw how to
estimate a value $\hat{\mu}(x)$ on the regression line for a given
value of $x$, such as $x=15$. 

But how good is our estimate $\hat{\mu}(15)$? How much confidence
do we have in \emph{this} estimate? Furthermore, suppose we were going
to observe another value of $Y$ at $x=15$. What could we say?

Intuitively, it should be easier to get bounds on the mean (average)
value of $Y$ at $x_{0}$ (called a \emph{confidence interval for
the mean value of $Y$ at $x_{0}$}) than it is to get bounds on a
future observation of $Y$ (called a \emph{prediction interval for
$Y$ at $x_{0}$}). As we shall see, the intuition serves us well
and confidence intervals are shorter for the mean value, longer for
the individual value.

Our point estimate of $\mu(x_{0})$ is of course $\hat{Y}=\hat{Y}(x_{0})$,
so for a confidence interval we will need to know $\hat{Y}$'s sampling
distribution. It turns out (see Section BLANK) that $\hat{Y}=\hat{\mu}(x_{0})$
is distributed\begin{equation}
\hat{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu(x_{0}),\:\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{(x_{0}-\xbar)^{2}}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}\right).\end{equation}
Since $\sigma$ is unknown we estimate it with $S$ (we should expect
the appearance of a $\mathsf{t}(\mathtt{df}=n-2)$ distribution in
the near future). 

A $100(1-\alpha)\%$ \emph{confidence interval (CI) for} $\mu(x_{0})$
is given by\begin{equation}
\hat{Y}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-2)\, S\sqrt{\frac{1}{n}+\frac{(x_{0}-\xbar^{2})}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}.\end{equation}


It is time for prediction intervals, which are slightly different.
In order to find confidence bounds for a new observation of $Y$ (we
will denote it $Y_{\mbox{new}}$) we use the fact that\begin{equation}
Y_{\mbox{new}}\sim\mathtt{norm}\left(\mathtt{mean}=\mu(x_{0}),\,\mathtt{sd}=\sigma\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\xbar)^{2}}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}\right).\end{equation}
Of course $\sigma$ is unknown and we estimate it with $S$. Thus,
a $100(1-\alpha)\%$ prediction interval (PI) for a future value of
$Y$ at $x_{0}$ is given by \begin{equation}
\hat{Y}(x_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\: S\,\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\xbar)^{2}}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}.\end{equation}
We notice that the CI in Equation BLANK is wider than the PI in Equation
BLANK, just as we expected at the beginning of the section.


\subsection*{How to do it with \textsf{R}}

Confidence and prediction intervals are calculated in \textsf{R} with
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!predict!\inputencoding{utf8}
function, which we encountered in Section BLANK. There we neglected
to take advantage of its additional \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!interval!\inputencoding{utf8}
argument. The general syntax follows.
\begin{example}
We will find confidence and prediction intervals for the stopping
distance of a car travelling 5, 6, and 21\,mph (note from the graph
that there are no collected data for these speeds). We have computed
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}
earlier, and we will use this for input to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!predict!\inputencoding{utf8}
function. Also, we need to tell \textsf{R} the values of $x_{0}$
at which we want the predictions made, and store the $x_{0}$ values
in a data frame whose variable is labeled with the correct name. \emph{This
is important}. 

<<>>=
new <- data.frame(speed = c(5, 6, 21))
@

Next we instruct \textsf{R} to calculate the intervals. Confidence
intervals are given by

<<>>=
predict(cars.lm, newdata = new, interval = "confidence")
@

<<echo = FALSE, results = hide>>=
carsCI <- round(predict(cars.lm, newdata = new, interval = "confidence"), 2)
@

Prediction intervals are given by

<<>>=
predict(cars.lm, newdata = new, interval = "prediction")
@

<<echo = FALSE, results = hide>>=
carsPI <- round(predict(cars.lm, newdata = new, interval = "prediction"), 2)
@

\end{example}
The type of interval is dictated by the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!interval!\inputencoding{utf8}
argument (which is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!none!\inputencoding{utf8}
by default), and the default confidence level is 95\% (which can be
changed with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!level!\inputencoding{utf8}
argument). 
\begin{example}
Using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
data,
\begin{enumerate}
\item Report a point estimate of and a 95\% confidence interval for the
mean stopping distance for a car travelling 5\,mph.


The fitted value for $x=5$ is \Sexpr{carsCI[1, 1]}, so a point estimate
would be \Sexpr{carsCI[1, 1]}\,ft. The 95\% CI is given by {[}\Sexpr{carsCI[1, 2]},
\Sexpr{carsCI[1, 3]}{]}, so with 95\% confidence the mean stopping
distance lies somewhere between \Sexpr{carsCI[1, 2]}\,ft and \Sexpr{carsCI[1, 3]}\,ft.

\item Report a point prediction for and a 95\% prediction interval for the
stopping distance of a hypothetical car travelling 21\,mph.


The fitted value for $x=21$ is \Sexpr{carsPI[3, 1]}, so a point
prediction for the stopping distance is \Sexpr{carsPI[3, 1]}\,ft.
The 95\% PI is given by {[}\Sexpr{carsPI[3, 2]}, \Sexpr{carsPI[3, 3]}{]},
so with 95\% confidence we may assert that the hypothetical stopping
distance for a car travelling 21\,mph would lie somewhere between
\Sexpr{carsPI[3, 2]}\,ft and \Sexpr{carsPI[3, 3]}\,ft.

\end{enumerate}
\end{example}

\subsection*{Graphing the Confidence and Prediction Bands}

We earlier guessed that a bound on the value of a single new observation
would be inherently less certain than a bound for an average (mean)
value; therefore, we expect the CIs for the mean to be tighter than
the PIs for a new observation. A close look at the standard deviations
in Equations BLANK and BLANK confirms our guess, but we would like
to see a picture to drive the point home.

We may plot the confidence and prediction intervals with one fell
swoop using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ci.plot!\inputencoding{utf8}
function from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!HH!\inputencoding{utf8}
package. The graph is displayed in Figure \ref{fig:Scatter-cars-CIPI}.
%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
library(HH)
print(ci.plot(cars.lm))
@
\par\end{centering}

\caption{Scatterplot of the cars data with added regression line and confidence/prediction
bands\label{fig:Scatter-cars-CIPI}}

\end{figure}


<<eval = FALSE>>=
library(HH)
ci.plot(cars.lm)
@

Notice that the bands curve outward away from the regression line
as the $x$ values move away from the center. This is expected once
we notice the $(x_{0}-\xbar)^{2}$ term in the standard deviation
formulas in Equations BLANK and BLANK.


\section{Model Utility and Inference}


\subsection{Hypothesis Tests for the Parameters}

Much of the attention of SLR is directed toward $\beta_{1}$ because
when $\beta_{1}\neq0$ the mean value of $Y$ increases (or decreases)
as $x$ increases. Further, if $\beta_{1}=0$ then the mean value
of $Y$ remains the same, regardless of the value of $x$ (when the
regression assumptions hold, of course). It is thus very important
to decide whether or not $\beta_{1}=0$. We address the question with
a statistical test of the null hypothesis $H_{0}:\beta_{1}=0$ versus
the alternative hypothesis $H_{1}:\beta_{1}\neq0$, and to do that
we need to know the sampling distribution of $b_{1}$ when the null
hypothesis is true.

To this end we already know from Section BLANK that the quantity\begin{equation}
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}\end{equation}
has a $t(\mathtt{df}=n-2)$ distribution; therefore, when $\beta_{1}=0$
the quantity $b_{1}/S_{b_{1}}$ has a $t(\mathtt{df}=n-2)$ distribution
and we can compute a $p$-value by comparing the observed value of
$b_{1}/S{}_{b_{1}}$ with values under a $\mathsf{t}(\mathtt{df}=n-2)$
curve. 

Similarly, we may test the hypothesis $H_{0}:\beta_{0}=0$ versus
the alternative $H_{1}:\beta_{0}\neq0$ with the statistic $T=b_{0}/S_{b_{0}}$,
where $S_{b_{0}}$ is given in Section BLANK. The test is conducted
the same way as for $\beta_{1}$. 


\subsection*{How to do it with \textsf{R}}

Let us take another look at the output from \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(cars.lm)!\inputencoding{utf8}:

<<>>=
summary(cars.lm)
@
<<echo = FALSE>>=
A <- round(summary(cars.lm)$coef, 3)
B <- round(confint(cars.lm), 3)
@

In the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Coefficients!\inputencoding{utf8}
section we find the $t$ statistics and the $p$-values associated
with the tests that the respective parameters are zero in the fourth
and fifth columns. Since the $p$-values are (much) less than 0.05,
we conclude that there is strong evidence that the parameters $\beta_{1}\neq0$
and $\beta_{0}\neq0$, and as such, we say that there is a statistically
significant linear relationship between \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dist!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}. 


\subsection{Simple Coefficient of Determination}

It would be nice to have a single number that indicates how well our
linear regression model is doing, and the \emph{simple coefficient
of determination} is designed for that purpose. In what follows, we
observe the values $Y_{1}$, $Y_{2}$, \ldots{},$Y_{n}$, and the
goal is to estimate $\mu(x_{0})$, the mean value of $Y$ at the location
$x_{0}$. 

If we disregard the dependence of $Y$ and $x$ and base our estimate
only on the $Y$ values then a reasonable choice for an estimator
is just the MLE of $\mu$, which is $\Ybar$. Then the errors incurred
by the estimate are just $Y_{i}-\Ybar$ and the variation about the
estimate as measured by the sample variance is proportional to \begin{equation}
SSTO=\sum_{i=1}^{n}(Y_{i}-\Ybar)^{2}.\end{equation}
Here, $SSTO$ is an acronym for the \emph{total sum of squares}.

But we do have additional information, namely, we have values $x_{i}$
associated with each value of $Y_{i}$. We have seen that this information
leads us to the estimate $\hat{Y_{i}}$ and the errors incurred are
just the residuals, $E_{i}=Y_{i}-\hat{Y_{i}}$. The variation associated
with these errors can be measured with \begin{equation}
SSE=\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}.\end{equation}
We have seen the $SSE$ before, which stands for the \emph{sum of
squared errors} or \emph{error sum of squares}. Of course, we would
expect the error to be less in the latter case, since we have used
more information. The improvement in our estimation as a result of
the linear regression model can be measured with the difference\[
(Y_{i}-\Ybar)-(Y_{i}-\hat{Y_{i}})=\hat{Y_{i}}-\Ybar,\]
and we measure the variation in these errors with\begin{equation}
SSR=\sum_{i=1}^{n}(\hat{Y_{i}}-\Ybar)^{2},\end{equation}
also known as the \emph{regression sum of squares}. It is not obvious,
but some algebra proved a famous result known as the \textbf{ANOVA
Equality}:\begin{equation}
\sum_{i=1}^{n}(Y_{i}-\Ybar)^{2}=\sum_{i=1}^{n}(\hat{Y_{i}}-\Ybar)^{2}+\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}\label{eq:anovaeq}\end{equation}
or in other words,\begin{equation}
SSTO=SSR+SSE.\end{equation}
This equality has a nice interpretation. Consider $SSTO$ to be the
\emph{total variation} of the errors. Think of a decomposition of
the total variation into pieces: one piece measuring the reduction
of error from using the linear regression model, or \emph{explained
variation} ($SSR$), while the other represents what is left over,
that is, the errors that the linear regression model doesn't explain,
or \emph{unexplained variation} ($SSE$). In this way we see that
the ANOVA equality merely partitions the variation into \[
\mbox{total variation}=\mbox{explained variation}+\mbox{unexplained variation}.\]
For a single number to summarize how well our model is doing we use
the simple coefficient of determination $r^{2}$, defined by\begin{equation}
r^{2}=1-\frac{SSE}{SSTO}.\end{equation}
We interpret $r^{2}$ as the proportion of total variation that is
explained by the simple linear regression model. When $r^{2}$ is
large, the model is doing a good job; when $r^{2}$ is small, the
model is not doing a good job.

Related to the simple coefficient of determination is the sample correlation
coefficient, $r$. As you can guess, the way we get $r$ is by the
formula $|r|=\sqrt{r^{2}}$. But how do we get the sign? It is equal
the sign of the slope estimate $b_{1}$. That is, if the regression
line $\hat{\mu}(x)$ has positive slope, then $r=\sqrt{r^{2}}$. Likewise,
if the slope of $\hat{\mu}(x)$ is negative, then $r=-\sqrt{r^{2}}$.


\subsection*{How to do it with R}

The primary method to display partitioned sums of squared errors is
with an \emph{ANOVA table}. The command in \textsf{R} to produce such
a table is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!anova!\inputencoding{utf8}.
The input to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!anova!\inputencoding{utf8}
is the result of an \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lm!\inputencoding{utf8}
call which for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
data is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}.

<<>>=
anova(cars.lm)
@

The output gives\[
r^{2}=1-\frac{SSE}{SSR+SSE}=1-\frac{11353.5}{21185.5+11353.5}\approx0.65.\]
 The interpretation should be: {}``The linear regression line accounts
for approximately 65\% of the variation of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dist!\inputencoding{utf8}
as explained by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}''.

The value of $r^{2}$ is stored in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!r.squared!\inputencoding{utf8}
component of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(cars.lm)!\inputencoding{utf8},
which we called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!carsumry!\inputencoding{utf8}.

<<>>=
carsumry$r.squared
@

We already knew this. We saw it in the next to the last line of the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(cars.lm)!\inputencoding{utf8}
output where it was called {}``\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Multiple R-squared!\inputencoding{utf8}''.
Listed right beside it is the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Adjusted R-squared!\inputencoding{utf8}
which we will discuss in Chapter BLANK.

For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
data, we find $r$ to be

<<>>=
sqrt(carsumry$r.squared)
@

We choose the principal square root because the slope of the regression
line is positive.


\subsection{Overall \emph{F} statistic}

There is another way to test the significance of the linear regression
model. In SLR, the new way also tests the hypothesis $H_{0}:\beta_{1}=0$
versus $H_{1}:\beta_{1}\neq0$, but it is done with a new test statistic
called the \emph{overall} $F$ \emph{statistic}. It is defined by
\begin{equation}
F=\frac{SSR}{SSE/(n-2)}.\end{equation}
Under the regression assumptions and when $H_{0}$ is true, the $F$
statistic has an $\mathtt{f}(\mathtt{df1}=1,\,\mathtt{df2}=n-2)$
distribution. We reject $H_{0}$ when $F$ is large -- that is, when
the explained variation is large relative to the unexplained variation.

All this being said, we have not yet gained much from the overall
$F$ statistic because we already knew from Section BLANK how to test
$H_{0}:\beta_{1}=0$\ldots{} we use the Student's $t$ statistic.
What is worse is that (in the simple linear regression model) it can
be proved that the $F$ in Equation BLANK is exactly the Student's
$t$ statistic for $\beta_{1}$ squared,\begin{equation}
F=\left(\frac{b_{1}}{S_{b_{1}}}\right)^{2}.\end{equation}
So why bother to define the $F$ statistic? Why not just square the
$t$ statistic and be done with it? The answer is that the $F$ statistic
has a more complicated interpretation and plays a more important role
in the multiple linear regression model which we will study in Chapter
BLANK. See Section BLANK for details.


\subsection{How to do it with \textsf{R}}

The overall $F$ statistic and $p$-value are displayed in the bottom
line of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary(cars.lm)!\inputencoding{utf8}
output. It is also shown in the final columns of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!anova(cars.lm)!\inputencoding{utf8}:

<<>>=
anova(cars.lm)
@

Here we see that the $F$ statistisic is \Sexpr{round(carsumry$fstatistic, 2)}
with a $p$-value very close to zero. The conclusion: there is very
strong evidence that $H_{0}:\beta_{1}=0$ is false, that is, there
is strong evidence that $\beta_{1}\neq0$. Moreover, we conclude that
the regression relationship between \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dist!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}
is significant.

Note that the value of the $F$ statistic is the same as the Student's
$t$ statistic for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}
squared.


\section{Residual Analysis}

We know from our model that $Y=\mu(x)+\epsilon$, or in other words,
$\epsilon=Y-\mu(x)$. Further, we know that $\epsilon\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)$.
We may estimate $\epsilon_{i}$ with the \emph{residual} $E_{i}=Y_{i}-\hat{Y_{i}}$,
where $\hat{Y_{i}}=\hat{\mu}(x_{i})$. If the regression assumptions
hold, then $ $the residuals should be normally distributed. We check
this in Section \ref{sub:Normality-Assumption}. Further, the residuals
should have mean zero with constant variance $\sigma^{2}$, and we
check this in Section \ref{sub:Constant-Variance-Assumption}. Last,
the residuals should be independent, and we check this in Section
\ref{sub:Independence-Assumption}.

In every case, we will begin by looking at residual plots -- that
is, scatterplots of the residuals $E_{i}$ versus index or predicted
values $\hat{Y_{i}}$ -- and follow up with hypothesis tests.


\subsection{Normality Assumption\label{sub:Normality-Assumption}}

We can assess the normality of the residuals with graphical methods
and hypothesis tests. To check graphically whether the residuals are
normally distributed we may look at histograms or \emph{q}-\emph{q}
plots. We first examine a histogram in Figure BLANK. There we see
that the distribution of the residuals appears to be mound shaped,
for the most part. We can plot the order statistics of the sample
versus quantiles from a $\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$
distribution with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plot(cars.lm, which = 2)!\inputencoding{utf8},
and the results are in Figure BLANK. If the assumption of normality
were true, then we would expect points randomly scattered about the
dotted straight line displayed in the figure. In this case, we see
a slight departure from normality in that the dots show systematic
clustering on one side or the other of the line. The points on the
upper end of the plot also appear begin to stray from the line. We
would say there is some evidence that the residuals are not perfectly
normal.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(cars.lm, which = 2)
@
\par\end{centering}

\caption{Normal q-q plot of the residuals, used for checking the normality
assumption. Look out for any curvature or substantial departures from
the straight line; hopefully the dots hug the line closely.}

\end{figure}



\subsubsection*{Testing the Normality Assumption}

Even though we may be concerned about the plots, we can use tests
to determine if the evidence present is statistically significant,
or if it could have happened merely by chance. There are many statistical
tests of normality. We will use the Shapiro-Wilk test, since it is
known to be a good test and to be quite powerful. However, there are
many other fine tests of normality including the Anderson-Darling
test and the Lillefors test, just to mention two of them. 

The Shapiro-Wilk test is based on the statistic\begin{equation}
W=\frac{\left(\sum_{i=1}^{n}a_{i}E_{(i)}\right)^{2}}{\sum_{j=1}^{n}E_{j}^{2}},\end{equation}
where the $E_{(i)}$ are the ordered residuals and the $a_{i}$ are
constants derived from the order statistics of a sample of size $n$
from a normal distribution. See Section BLANK.

We perform the Shapiro-Wilk test below, using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!shapiro.test!\inputencoding{utf8}
function from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!stats!\inputencoding{utf8}
package. The hypotheses are\[
H_{0}:\mbox{ the residuals are normally distributed }\]
versus\[
H_{1}:\mbox{ the residuals are not normally distributed.}\]
The results from \textsf{R} are

<<>>=
shapiro.test(residuals(cars.lm))
@

For these data we would reject the assumption of normality of the
residuals at the $\alpha=0.05$ significance level, but do not lose
heart, because the regression model is reasonably robust to departures
from the normality assumption. As long as the residual distribution
is not highly skewed, then the regression estimators will perform
reasonably well. Moreover, departures from constant variance and independence
will sometimes affect the quantile plots and histograms, therefore
it is wise to delay final decisions regarding normality until all
diagnostic measures have been investigated.


\subsection{Constant Variance Assumption\label{sub:Constant-Variance-Assumption}}

We will again go to residual plots to try and determine if the spread
of the residuals is changing over time (or index). However, it is
unfortunately not that easy because the residuals do not have constant
variance! In fact, it can be shown that the variance of the residual
$E_{i}$ is \begin{equation}
\mbox{Var\ensuremath{(E_{i})}}=\sigma^{2}(1-h_{ii}),\quad i=1,2,\ldots,n,\end{equation}
where $h_{ii}$ is a quantity called the \emph{leverage} which is
defined below. Consequently, in order to check the constant variance
assumption we must standardize the residuals before plotting. We estimate
the standard error of $E_{i}$ with $s_{E_{i}}=s\sqrt{(1-h_{ii})}$
and define the \emph{standardized residuals} $R_{i}$, $i=1,2,\ldots,n$,
by \begin{equation}
R_{i}=\frac{E_{i}}{s\,\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.\end{equation}


For the constant variance assumption we do not need the sign of the
residual so we will plot $\sqrt{|R_{i}|}$ versus the fitted values.
As we look at a scatterplot of $\sqrt{|R_{i}|}$ versus $\hat{Y}_{i}$
we would expect under the regression assumptions to see a constant
band of observations, indicating no change in the magnitude of the
observed distance from the line. We want to watch out for a fanning-out
of the residuals, or a less common funneling-in of the residuals.
Both patterns indicate a change in the residual variance and a consequent
departure from the regression assumptions, the first an increase,
the second a decrease.

In this case, we plot the standardized residuals versus the fitted
values. The graph may be seen in Figure BLANK. For these data there
does appear to be somewhat of a slight fanning-out of the residuals.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(cars.lm, which = 3)
@
\par\end{centering}

\caption{Plot of standardized residuals against the fitted values, used for
checking the constant variance assumption. Watch out for any fanning
out (or in) of the dots; hopefully they fall in a constant band.}

\end{figure}



\subsubsection*{Testing the Constant Variance Assumption}

We will use the Breusch-Pagan test to decide whether the variance
of the residuals is nonconstant. The null hypothesis is that the variance
is the same for all observations, and the alternative hypothesis is
that the variance is not the same for all observations. The test statistic
is found by fitting a linear model to the centered squared residuals\begin{equation}
W_{i}=E_{i}^{2}-\frac{SSE}{n},\quad i=1,2,\ldots,n.\end{equation}
By default the same explanatory variables are used in the new model
which produces fitted values $\hat{W}_{i}$, $i=1,2,\ldots,n$. The
Breusch-Pagan test statistic in \textsf{R} is then calculated with
\begin{equation}
BP=n\sum_{i=1}^{n}\hat{W}_{i}^{2}\div\sum_{i=1}^{n}W_{i}^{2}.\end{equation}
We reject the null hypothesis if $BP$ is too large, which happens
when the explained variation in the new model is large relative to
the unexplained variation in the original model.

We do it in \textsf{R} with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!bptest!\inputencoding{utf8}
function from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lmtest!\inputencoding{utf8}
package. 

<<>>=
library(lmtest)
bptest(cars.lm)
@

For these data we would not reject the null hypothesis at the $\alpha=0.05$
level. There is relatively weak evidence against the assumption of
constant variance. 


\subsection{Independence Assumption\label{sub:Independence-Assumption}}

One of the strongest of the regression assumptions is the one regarding
independence. Departures from the independence assumption are often
exhibited by correlation (or autocorrelation, literally, self-correlation)
present in the residuals. There can be positive or negative correlation.

Positive correlation is displayed by positive residuals followed by
positive residuals, and negative residuals followed by negative residuals.
Looking from left to right, this is exhibited by a cyclical feature
in the residual plots, with long sequences of positive residuals being
followed by long sequences of negative ones.

On the other hand, negative correlation implies positive residuals
followed by negative residuals, which are then followed by positive
residuals, \emph{etc}. Consequently, negatively correlated residuals
are often associated with an alternating pattern in the residual plots.
We examine the residual plot in Figure BLANK. There is no obvious
cyclical wave pattern or structure to the residual plot. 

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(cars.lm, which = 1)
@
\par\end{centering}

\caption{Plot of the residuals versus the fitted values, used for checking
the independence assumption. Watch out for any patterns or structure;
hopefully the points are randomly scattered in the plot.}

\end{figure}



\subsubsection*{Testing the Independence Assumption}

We may statistically test whether there is evidence of autocorrelation
in the residuals with the Durbin-Watson test. The test is based on
the statistic\begin{equation}
D=\frac{\sum_{i=2}^{n}(E_{i}-E_{i-1})^{2}}{\sum_{j=1}^{n}E_{j}^{2}}.\end{equation}
Exact critical values are difficult to obtain, but \textsf{R} will
calculate the \emph{p-}value to great accuracy. It is performed with
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dwtest!\inputencoding{utf8}
function from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lmtest!\inputencoding{utf8}
package. We will conduct a two sided test that the correlation is
not zero, which is not the default (the default is to test that the
autocorrelation is positive).

<<>>=
library(lmtest)
dwtest(cars.lm, alternative = "two.sided")
@

In this case we do not reject the null hypothesis at the $\alpha=0.05$
significance level; there is very little evidence of nonzero autocorrelation
in the residuals.


\subsection{Remedial Measures}

We often find problems with our model that suggest that at least one
of the three regression assumptions is violated. What do we do then?
There are many measures at the statistician's disposal, and we mention
specific steps one can take to improve the model under certain types
of violation.
\begin{description}
\item [{Mean~response~is~not~linear.}] We can directly modify the model
to better approximate the mean response. In particular, perhaps a
polynomial regression function of the form \[
\mu(x)=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}\]
would be appropriate. Alternatively, we could have a function of the
form\[
\mu(x)=\beta_{0}\me^{\beta_{1}x}.\]
Models like these are studied in nonlinear regression courses.
\item [{Error~variance~is~not~constant.}] Sometimes a transformation
of the dependent variable will take care of the problem. There is
a large class of them called \emph{Box-Cox transformations}. They
take the form \begin{equation}
Y^{\ast}=Y^{\lambda},\end{equation}
 where $\lambda$ is a constant. (The method proposed by Box and Cox
will determine a suitable value of $\lambda$ automatically by maximum
likelihood). The class contains the transformations \begin{alignat*}{1}
\lambda=2,\quad & Y^{\ast}=Y^{2}\\
\lambda=0.5,\quad & Y^{\ast}=\sqrt{Y}\\
\lambda=0,\quad & Y^{\ast}=\ln\: Y\\
\lambda=-1,\quad & Y^{\ast}=1/Y\end{alignat*}
Alternatively, we can use the method of \emph{weighted least squares}.
This is studied in more detail in later classes. 
\item [{Error~distribution~is~not~normal.}] The same transformations
for stabilizing the variance are equally appropriate for smoothing
the residuals to a more Gaussian form. In fact, often we will kill
two birds with one stone.
\item [{Errors~are~not~independent.}] There are a large class of autoregressive
models to be used in this situation which occupy the latter part of
Chapter BLANK.
\end{description}

\section{Other Diagnostic Tools}

There are two types of observations with which we must be especially
careful:
\begin{description}
\item [{\emph{Influential~observations}}] are those that have a substantial
effect on our estimates, predictions, or inferences. A small change
in an influential observation is followed by a large change in the
parameter estimates or inferences. 
\item [{\emph{Outlying~observations}}] are those that fall fall far from
the rest of the data. They may be indicating a lack of fit for our
regression model, or they may just be a mistake or typographical error
that should be corrected. Regardless, special attention should be
given to these observations. An outlying observation may or may not
be influential.
\end{description}
We will discuss outliers first because the notation builds sequentially
in that order.


\subsection{Outliers}

There are three ways that an observation $(x_{i},y_{i})$ may be an
outlier: it can have an $x_{i}$ value which falls far from the other
$x$ values, it can have a $y_{i}$ value which falls far from the
other $y$ values, or it can have both $x_{i}$ and $y_{i}$ values
to fall far from the other $x$ and $y$ values.


\subsection*{Leverage}

Leverage statistics are designed to identify observations which have
$x$ values that are far away from the rest of the data. In the simple
linear regression model the leverage of $x_{i}$ is denoted by $h_{ii}$
and defined by \begin{equation}
h_{ii}=\frac{1}{n}+\frac{(x_{i}-\xbar)^{2}}{\sum_{k=1}^{n}(x_{k}-\xbar)^{2}},\quad i=1,2,\ldots,n.\end{equation}
The formula has a nice interpretation in the SLR model: if the distance
from $x_{i}$ to $\xbar$ is large relative to the other $x$'s then
$h_{ii}$ will be close to 1. 

Leverages have nice mathematical properties; for example, they satisfy\begin{equation}
0\leq h_{ii}\leq1,\end{equation}
and their sum is \begin{eqnarray}
\sum_{i=1}^{n}h_{ii} & = & \sum_{i=1}^{n}\left[\frac{1}{n}+\frac{(x_{i}-\xbar)^{2}}{\sum_{k=1}^{n}(x_{k}-\xbar)^{2}}\right],\\
 & = & \frac{n}{n}+\frac{\sum_{i}(x_{i}-\xbar)^{2}}{\sum_{k}(x_{k}-\xbar)^{2}},\\
 & = & 2.\end{eqnarray}
A rule of thumb is to consider leverage values to be large if they
are more than double their average size (which is $2/n$ according
to Equation BLANK). So leverages larger than $4/n$ are suspect. Another
rule of thumb is to say that values bigger than 0.5 indicate high
leverage, while values between 0.3 and 0.5 indicate moderate leverage.


\subsection*{Standardized and Studentized Deleted Residuals }

We have already encountered the \emph{standardized residuals} $r_{i}$
in Section BLANK; they are merely residuals that have been divided
by their respective standard deviations: \begin{equation}
R_{i}=\frac{E_{i}}{S\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.\end{equation}
Values of $|R_{i}|>2$ are extreme and suggest that the observation
has an outlying $y$-value. 

Now delete the $i$$^{\textrm{th}}$ case and fit the regression function
to the remaining $n-1$ cases, producing a fitted value $\hat{Y}_{(i)}$
with \emph{deleted residual} $D_{i}=Y_{i}-\hat{Y}_{(i)}$. It is shown
in later classes that \begin{equation}
\mbox{Var\ensuremath{(D_{i})}}=\frac{S_{(i)}^{2}}{1-h_{ii}},\quad i=1,2,\ldots,n,\end{equation}
so that the \emph{studentized deleted residuals} $t_{i}$ defined
by\begin{equation}
t_{i}=\frac{D_{i}}{S_{(i)}/(1-h_{ii})},\quad i=1,2,\ldots,n,\end{equation}
have a $\mathsf{t}(\mathtt{df}=n-3)$ distribution and we compare
observed values of $t_{i}$ to this distribution to decide whether
or not an observation is extreme. 

The folklore in regression classes is that a test based on the statistic
in Equation BLANK can be too liberal. A rule of thumb is if we suspect
an observation to be an outlier \emph{before} seeing the data then
we say it is signicantly outlying if its two-tailed $p$-value is
less than $\alpha$, but if we suspect an observation to be an outlier
\emph{after} seeing the data, then we should only say it is significantly
outlying if its two-tailed $p$-value is less than $\alpha/n$. The
latter rule of thumb is called the Bonferroni approach and can be
overly conservative for large data sets. The statistician must look
at the data and use his/her best judgement, in every case.


\subsection{How to do it with \textsf{R}}

We can calculate the standardized residuals with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rstandard!\inputencoding{utf8}
function. The input is the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lm!\inputencoding{utf8}
object, which is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars.lm!\inputencoding{utf8}.

<<>>=
sres <- rstandard(cars.lm)
sres[1:5]
@

We can find out which observations have studentized residuals larger
than two with the command

<<>>=
sres[which(abs(sres) > 2)]
@

In this case, we see that observations 23, 35, and 49 are potential
outliers with respect to their $y$-value.

We can compute the studentized deleted residuals with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rstudent!\inputencoding{utf8}:

<<>>=
sdelres <- rstudent(cars.lm)
sdelres[1:5]
@

We should compare these values with critical values from a $\mathsf{t}(\mathtt{df}=n-3)$
distribution, which in this case is $\mathsf{t}(\mathtt{df}=50-3=47)$.
We can calculate a 0.005 quantile and check with

<<>>=
t0.005 <- qt(0.005, df = 47, lower.tail = FALSE)
sdelres[which(abs(sdelres) > t0.005)]
@

This means that observations 23 and 49 have a large studentized deleted
residual. The leverages can be found with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!hatvalues!\inputencoding{utf8}
function:

<<>>=
leverage <- hatvalues(cars.lm)
leverage[1:5]
leverage[which(leverage > 4/50)]
@

Here we see that observations 1, 2, and 50 have leverages bigger than
double their mean value. These observations would be considered outlying
with respect to their $x$ value (although they may or may not be
influential).


\subsection{Influential Observations}


\subsection*{$DFBETAS$ and $DFFITS$}

Anytime we do a statistical analysis, we are confronted with the variability
of data. It is always a concern when an observation plays too large
a role in our regression model, and we would not like or procedures
to be overly influenced by the value of a single observation. Hence,
it becomes desirable to check to see how much our estimates and predictions
would change if one of the observations were not included in the analysis.
If an observation changes the estimates/predictions a large amount,
then the observation is influential and should be subjected to a higher
level of scrutiny.

We measure the change in the parameter estimates as a result of deleting
an observation with $DFBETAS$. The $DFBETAS$ for the intercept $b_{0}$
are given by\begin{equation}
(DFBETAS)_{0(i)}=\frac{b_{0}-b_{0(i)}}{S_{(i)}\sqrt{\frac{1}{n}+\frac{\xbar^{2}}{\sum_{i=1}^{n}(x_{i}-\xbar)^{2}}}},\quad i=1,2,\ldots,n.\end{equation}
and the $DFBETAS$ for the slope $b_{1}$ are given by\begin{equation}
(DFBETAS)_{1(i)}=\frac{b_{1}-b_{1(i)}}{S_{(i)}\left[\sum_{i=1}^{n}(x_{i}-\xbar)^{2}\right]^{-1/2}},\quad i=1,2,\ldots,n.\end{equation}
See Section BLANK for a better way to write these. The signs of the
$DFBETAS$ indicate whether the coefficients would increase or decrease
as a result of including the observation. If the $DFBETAS$ are large,
then the observation has a large impact on those regression coefficients.
We label observations as suspicious if their $DFBETAS$ have magnitude
greater 1 for small data or $2/\sqrt{n}$ for large data sets.

We can calculate the $DFBETAS$ with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dfbetas!\inputencoding{utf8}
function (some output has been omitted):

<<>>=
dfb <- dfbetas(cars.lm)
head(dfb)
@

We see that the inclusion of the first observation slightly increases
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Intercept!\inputencoding{utf8}
and slightly decreases the coefficient on \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!speed!\inputencoding{utf8}.

We can measure the influence that an observation has on its fitted
value with $DFFITS$. These are calculated by deleting an observation,
refitting the model, recalculating the fit, then standardizing. The
formula is 

\begin{equation}
(DFFITS)_{i}=\frac{\hat{Y_{i}}-\hat{Y}_{(i)}}{S_{(i)}\sqrt{h_{ii}}},\quad i=1,2,\ldots,n.\end{equation}
The value represents the number of standard deviations of $\hat{Y_{i}}$
that the fitted value $\hat{Y_{i}}$ increases or decreases with the
inclusion of the $i$$^{\textrm{th}}$ observation. We can compute
them with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!dffits!\inputencoding{utf8}
function.

<<>>=
dff <- dffits(cars.lm)
dff[1:5]
@

A rule of thumb is to flag observations whose $DFFIT$ exceeds one
in absolute value, but there are none of those in this data set.


\subsection*{Cook's Distance}

The $DFFITS$ are good for measuring the influence on a single fitted
value, but we may want to measure the influence an observation has
on all of the fitted values simultaneously. The statistics used for
measuring this are Cook's distances which may be calculated%
\footnote{Cook's distances are actually defined by a different formula than
the one shown. The formula in Equation BLANK is algebraically equivalent
to the defining formula and is, in the author's opinion, more transparent.%
} by the formula

\begin{equation}
D_{i}=\frac{E_{i}^{2}}{(p+1)S^{2}}\cdot\frac{h_{ii}}{(1-h_{ii})^{2}},\quad i=1,2,\ldots,n.\end{equation}
It shows that Cook's distance depends both on the residual $E_{i}$
and the leverage $h_{ii}$ and in this way $D_{i}$ contains information
about outlying $x$ and $y$ values. 

To assess the significance of $D$, we compare to quantiles of an
$\mathsf{f}(\mathtt{df1}=2,\,\mathtt{df2}=n-2)$ distribution. A rule
of thumb is to classify observations falling higher than the 50$^{\textrm{th}}$
percentile as being extreme. 


\subsection{How to do it with \textsf{R}}

We can calculate the Cook's Distances with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cooks.distance!\inputencoding{utf8}
function.

<<>>=
cooksD <- cooks.distance(cars.lm)
cooksD[1:5]
@

We can look at a plot of the Cook's distances with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!plot(cars.lm, which = 4)!\inputencoding{utf8}.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(cars.lm, which = 4)
@
\par\end{centering}

\caption{Cook's distances for the cars data}

\end{figure}


Observations with the largest Cook's D values are labeled, hence we
see that observations 23, 39, and 49 are suspicious. However, we need
to compare to the quantiles of an $\mathsf{f}(\mathtt{df1}=2,\,\mathtt{df2}=48)$
distribution:

<<>>=
F0.50 <- qf(0.5, df1 = 2, df2 = 48)
cooksD[which(cooksD > F0.50)]
@

We see that with this data set there are no observations with extreme
Cook's distance, after all.


\subsection{All Influence Measures Simultaneously}

We can display the result of diagnostic checking all at once in one
table, with potentially influential points displayed. We do it with
the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!influence.measures(cars.lm)!\inputencoding{utf8}:

<<eval = FALSE>>=
influence.measures(cars.lm)
@

The output is a huge matrix display, which we have omitted in the
interest of brevity. A point is identified if it is classified to
be influential with respect to any of the diagnostic measures. Here
we see that observations 2, 11, 15, and 18 merit further investigation. 

We can also look at all diagnostic plots at once with the commands

<<eval = FALSE>>=
par(mfrow = c(2,2))
plot(cars.lm)
par(mfrow = c(1,1))
@

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!par!\inputencoding{utf8}
command is used so that $2\times2=4$ plots will be shown on the same
display. The diagnostic plots for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!cars!\inputencoding{utf8}
data are shown in Figure BLANK:

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true>>=
par(mfrow = c(2,2))
plot(cars.lm)
par(mfrow = c(1,1))
@
\par\end{centering}

\caption{Diagnostic Plots for the cars data}

\end{figure}


We have discussed all of the plots except the last, which is possibly
the most interesting. It shows Residuals vs. Leverage, which will
identify outlying $y$ values versus outlying $x$ values. Here we
see that observation 23 has a high residual, but low leverage, and
it turns out that observations 1 and 2 have relatively high leverage
but low/moderate leverage (they are on the right side of the plot,
just above the horizontal line). Observation 49 has a large residual
with a comparatively large leverage. 

We can identify the observations with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!identify!\inputencoding{utf8}
command; it allows us to display the observation number of dots on
the plot. First, we plot the graph, then we call \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!identify!\inputencoding{utf8}:

<<eval = FALSE, keep.source = TRUE>>=
plot(cars.lm, which = 5)   # std'd resids vs lev plot
identify(leverage, sres, n = 4)   # identify 4 points
@

The graph with the identified points is omitted (but the plain plot
is shown in the bottom right corner of Figure BLANK). Observations
1 and 2 fall on the far right side of the plot, near the horizontal
axis.


\section{Chapter Exercises}


\subsection*{Section \ref{sec:Basic-Philosophy}}
\begin{xca}
Prove the ANOVA equality, Equation \ref{eq:anovaeq}. \emph{Hint}:
show that\[
\sum\]

\end{xca}



\chapter{Multiple Linear Regression}

We know a lot about simple linear regression models, and a next step
is to study multiple regression models that have more than one independent
(explanatory) variable. In the discussion that follows we will assume
that we have $p$ explanatory variables, where $p>1$.

The language is phrased in matrix terms -- for two reasons. First,
it is quicker to write and (arguably) more pleasant to read. Second,
the matrix approach will be required for later study of the subject;
the reader might as well be introduced to it now.

Most of the results are stated without proof or with only a cursory
justification. Those yearning for more should consult an advanced
text in linear regression for details, such as Applied Linear Regression
Models or C. R. Rao.


\section{The Multiple Linear Regression Model}

The first thing to do is get some better notation. We will write \begin{equation}
\mathbf{Y}_{\mathrm{n}\times1}=\begin{bmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{n}\end{bmatrix},\quad\mbox{and}\quad\mathbf{X}_{\mathrm{n}\times(\mathrm{p}+1)}=\begin{bmatrix}1 & x_{11} & x_{21} & \cdots & x_{p1}\\
1 & x_{12} & x_{22} & \cdots & x_{p2}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_{1n} & x_{2n} & \cdots & x_{pn}\end{bmatrix}.\end{equation}
The vector $\mathbf{Y}$ is called the \emph{response vector} and
the matrix $\mathbf{X}$ is called the \emph{model matrix}. As in
Chapter BLANK, the most general assumption that relates $\mathbf{Y}$
to $\mathbf{X}$ is\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\upepsilon,\end{equation}
where $\mu$ is some function (the \emph{signal}) and $\upepsilon$
is the \emph{noise} (everything else). We usually impose some structure
on $\mu$ and $\upepsilon$. In particular, the standard multiple
linear regression model assumes \begin{equation}
\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon,\end{equation}
where the parameter vector $\upbeta$ looks like \begin{equation}
\upbeta_{(\mathrm{p}+1)\times1}=\begin{bmatrix}\beta_{0} & \beta_{1} & \cdots & \beta_{p}\end{bmatrix}^{\mathrm{T}},\end{equation}
and the random vector $\upepsilon_{\mathrm{n}\times1}=\begin{bmatrix}\epsilon_{1} & \epsilon_{2} & \cdots & \epsilon_{n}\end{bmatrix}^{\mathrm{T}}$
is assumed to be distributed\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right).\end{equation}
The assumption on $\upepsilon$ is equivalent to the assumption that
$\epsilon_{1}$, $\epsilon_{2}$, \ldots{}, $\epsilon_{n}$ are i.i.d.~$\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)$.
It is a linear model because the quantity $\mu(\mathbf{X})=\mathbf{X}\upbeta$
is linear in the parameters $\beta_{0}$, $\beta_{1}$,\ldots{},
$\beta_{p}$. It may be helpful to see the model in expanded form;
the above matrix formulation is equivalent to the more lengthy\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+\cdots+\beta_{p}x_{pi}+\epsilon_{i},\quad i=1,2,\ldots,n.\end{equation}

\begin{example}
\textbf{Girth, Height, and Volume for Black Cherry trees.} Measurements
were made of the girth, height, and volume of timber in 31 felled
black cherry trees. Note that girth is the diameter of the tree (in
inches) measured at 4\,ft 6\,in above the ground. The variables
are
\begin{enumerate}
\item \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}:
tree diameter in inches (denoted $x_{1}$)
\item \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}:
tree height in feet ($x_{2}$).
\item \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}:
volume of the tree in cubic feet. ($y$)
\end{enumerate}
The data are in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!datasets!\inputencoding{utf8}
package and are already on the search path; they can be viewed with

<<>>=
head(trees)
@

Let us take a look at a visual display of the data. For multiple variables,
instead of a simple scatterplot we use a scatterplot matrix which
is made with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!splom!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lattice!\inputencoding{utf8}
package as shown below. The plot is shown in Figure BLANK.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
library(lattice)
print(splom(trees))
@
\par\end{centering}

\caption{Scatterplot matrix of trees data}

\end{figure}


<<eval = FALSE>>=
library(lattice)
splom(trees)
@

The dependent (response) variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
is listed in the first row of the scatterplot matrix. Moving from
left to right, we see an approximately linear relationship between
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
and the independent (explanatory) variables \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}.
A first guess at a model for these data might be\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon,\end{equation}
in which case the quantity $\mu(x_{1},x_{2})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$
would represent the mean value of $Y$ at the point $(x_{1},x_{2})$.

\end{example}

\subsection*{What does it mean?}

The interpretation is simple. The intercept $\beta_{0}$ represents
the mean \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
when all other independent variables are zero. The parameter $\beta_{i}$
represents the change in mean \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
when there is a unit increase in $x_{i}$, while the other independent
variable is held constant. For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data, $\beta_{1}$ represents the change in average \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
as \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
increases by one unit when the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
is held constant, and $\beta_{2}$ represents the change in average
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
as \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
increases by one unit when the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
is held constant.

In simple linear regression, we had one independent variable and our
linear regression surface was 1D, simply a line. In multiple regression
there are many independent variables and so our linear regression
surface will be many-D\ldots{} in general, a hyperplane. But when
there are only two explanatory variables the hyperplane is just an
ordinary plane and we can look at it with a 3D scatterplot. 

One way to do this is with the \textsf{R} Commander in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
package. It has a 3D scatterplot option under the \textsf{Graphs}
menu. It is especially great because the resulting graph is dynamic;
it can be moved around with the mouse, zoomed, \emph{etc}. But that
particular display does not translate well to a printed book.

Another way to do it is with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scatterplot3d!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scatterplot3d!\inputencoding{utf8}
package. The syntax follows, and the result is shown in Figure BLANK.

<<eval = FALSE>>=
library(scatterplot3d)
s3d <- with(trees, scatterplot3d(Girth, Height, Volume, pch = 16, highlight.3d = TRUE, angle = 60))
fit <- lm(Volume ~ Girth + Height, data = trees)
s3d$plane3d(fit)
@

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
library(scatterplot3d)
s3d <- with(trees, scatterplot3d(Girth, Height, Volume, pch = 16, highlight.3d = TRUE, angle = 60))
fit <- lm(Volume ~ Girth + Height, data = trees)
s3d$plane3d(fit)
@
\par\end{centering}

\caption{3D Scatterplot with Regression Plane}

\end{figure}


Looking at the graph we see that the data points fall close to a plane
in three dimensional space. (The plot looks remarkably good. In the
author's experience it is rare to see points fit so well to the plane
without some additional work.)


\section{Estimation and Prediction}


\subsection{Parameter estimates}

We will proceed exactly like we did in Section BLANK. We know \begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right),\end{equation}
which means that $\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon$ has an
$\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right)$
distribution. Therefore, the likelihood function is\begin{equation}
L(\upbeta,\sigma)=\frac{1}{2\pi^{n/2}\sigma}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\right\} .\end{equation}
To \emph{maximize} the likelihood in $\upbeta$, we need to \emph{minimize}
the quantity $g(\upbeta)=\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)$.
We do this by differentiating $g$ with respect to $\upbeta$. (It
may be a good idea to brush up on the material in Appendix BLANK.)
First we will rewrite $g$:\begin{equation}
g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-\mathbf{Y}^{\mathrm{T}}\mathbf{X}\upbeta-\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,\end{equation}
which can be further simplified to $g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-2\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta$
since $\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}$ is
$1\times1$ and thus equal to its transpose. Now we differentiate
to get \begin{equation}
\frac{\partial g}{\partial\upbeta}=\mathbf{0}-2\mathbf{X}^{\mathrm{T}}\mathbf{Y}+2\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,\end{equation}
since $\mathbf{X}^{\mathrm{T}}\mathbf{X}$ is symmetric. Setting the
derivative equal to the zero vector yields the so called {}``normal
equations''\begin{equation}
\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta=\mathbf{X}^{\mathrm{T}}\mathbf{Y}.\end{equation}
In the case that $\mathbf{X}^{\mathrm{T}}\mathbf{X}$ is invertible%
\footnote{We can find solutions of the normal equations even when $\mathbf{X}^{\mathrm{T}}\mathbf{X}$
is not of full rank, but the topic falls outside the scope of this
book. The interested reader can consult an advanced text such as BLANK
(CR.Rao)%
}, we may solve the equation for $\upbeta$ to get the maximum likelihood
estimator of $\upbeta$ which we denote by $\mathbf{b}$:\begin{equation}
\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}.\end{equation}

\begin{rem}
The formula in Equation BLANK is convenient for mathematical study
but is inconvenient for numerical computation. Researchers have devised
much more efficient algorithms for the actual calculation of the parameter
estimates, and we do not explore them here.
\end{rem}

\begin{rem}
We have only found a critical value, and have not actually shown that
the critical value is a minimum. We omit the details and refer the
interested reader to BLANK.
\end{rem}

\subsection*{How to do it with R}

We do all of the above just as we would in simple linear regression.
The powerhouse is the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!lm!\inputencoding{utf8}
function. Everything else is based on it. We separate explanatory
variables in the model formula by a plus sign.

<<>>=
trees.lm <- lm(Volume ~ Girth + Height, data = trees)
trees.lm
@

We see from the output that for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data our parameter estimates are $\mathbf{b}=\begin{bmatrix}-58.0 & 4.7 & 0.3\end{bmatrix}$,
and consequently our estimate of the mean response is $\hat{\mu}$
given by \begin{alignat}{1}
\hat{\mu}(x_{1},x_{2})= & \ b_{0}+b_{1}x_{1}+b_{2}x_{2},\\
\approx & -58.0+4.7x_{1}+0.3x_{2}.\end{alignat}
We could see the entire model matrix $\mathbf{X}$ with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!model.matrix!\inputencoding{utf8}
function, but in the interest of brevity we only show the first few
rows.

<<>>=
head(model.matrix(trees.lm))
@


\subsection{Point Estimates of the Regression Surface}

The parameter estimates $\mathbf{b}$ make it easy to find the fitted
values, $\hat{\mathbf{Y}}$. We write them individually as $\hat{Y}_{i}$,
$i=1,2,\ldots,n$, and recall that they are defined by\begin{eqnarray}
\hat{Y}_{i} & = & \hat{\mu}(x_{1i},x_{2i}),\\
 & = & b_{0}+b_{1}x_{1i}+b_{2}x_{2i},\quad i=1,2,\ldots,n.\end{eqnarray}
They are expressed more compactly by the matrix equation\begin{equation}
\hat{\mathbf{Y}}=\mathbf{X}\mathbf{b}.\end{equation}
From Equation BLANK we know that $\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}$,
so we can rewrite\begin{eqnarray}
\hat{\mathbf{Y}} & = & \mathbf{X}\left[\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\right],\\
 & = & \mathbf{H}\mathbf{Y},\end{eqnarray}
where $\mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}$
is appropriately named \emph{the hat matrix} because it {}``puts
the hat on $\mathbf{Y}$''. The hat matrix is very important in later
courses. Some facts about $\mathbf{H}$ are
\begin{itemize}
\item $\mathbf{H}$ is a symmetric square matrix, of dimension $\mathrm{n}\times\mathrm{n}$.
\item The diagonal entries $h_{ii}$ satisfy $0\leq h_{ii}\leq1$ (compare
to Equation BLANK).
\item The trace is $\mathrm{tr}(\mathbf{H})=p$.
\item $\mathbf{H}$ is \emph{idempotent} (also known as a \emph{projection
matrix}) which means that $\mathbf{H}^{2}=\mathbf{H}$. The same is
true of $\mathbf{I}-\mathbf{H}$.
\end{itemize}
Now let us write a column vector $\mathbf{x}_{0}=(x_{10},x_{20})^{\mathrm{T}}$
to denote given values of the explanatory variables \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Girth =!\inputencoding{utf8}
$x_{10}$ and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Height =!\inputencoding{utf8}
$x_{20}$. These values may match those of the collected data, or
they may be completely new values not observed in the original data
set. We may use the parameter estimates to find $\hat{Y}(\mathbf{x}_{0})$,
which will give us
\begin{enumerate}
\item an estimate of $\mu(\mathbf{x}_{0})$, the mean value of a future
observation at $\mathbf{x}_{0}$, and
\item a prediction for $Y(\mathbf{x}_{0})$, the actual value of a future
observation at $\mathbf{x}_{0}$.
\end{enumerate}
We can represent $\hat{Y}(\mathbf{x}_{0})$ by the matrix equation\begin{equation}
\hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b},\end{equation}
which is just a fancy way to write\begin{equation}
\hat{Y}(x_{10},x_{20})=b_{0}+b_{1}x_{10}+b_{2}x_{20}.\end{equation}
 
\begin{example}
If we wanted to predict the average volume of black cherry trees that
have \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Girth = 15!\inputencoding{utf8}\,in
and are \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height = 77!\inputencoding{utf8}\,ft
tall then we would use the estimate \begin{alignat*}{1}
\hat{\mu}(15,\,77)= & -58+4.7(15)+0.3(77),\\
\approx & 35.6\mbox{\,\ ft}^{3}.\end{alignat*}
We would use the same estimate $\hat{Y}=35.6$ to predict the measured
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
of another black cherry tree -- yet to be observed -- that has \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Girth = 15!\inputencoding{utf8}\,in
and is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height = 77!\inputencoding{utf8}\,ft
tall.
\end{example}

\subsection*{How to do it with R}

The fitted values are stored inside \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees.lm!\inputencoding{utf8}
and may be accessed with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!fitted!\inputencoding{utf8}
function. We only show the first five fitted values.

<<>>=
fitted(trees.lm)[1:5]
@

The syntax for general prediction does not change much from simple
linear regression. The computations are done with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!predict!\inputencoding{utf8}
function as described below.

The only difference from SLR is in the way we tell \textsf{R} the
values of the explanatory variables for which we want predictions.
In SLR we had only one independent variable but in MLR we have many
(for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data we have two). We will store values for the independent variables
in the data frame \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!new!\inputencoding{utf8},
which has two columns (one for each independent variable) and three
rows (we shall make predictions at three different locations).

<<>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
@

We can view the locations at which we will predict:

<<>>=
new
@

We continue just like we would have done in SLR.

<<>>=
predict(trees.lm, newdata = new)
@

<<echo = FALSE>>=
treesFIT <- round(predict(trees.lm, newdata = new), 1)
@
\begin{example}
Using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data,
\begin{enumerate}
\item Report a point estimate of the mean \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
of a tree of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
9.1\,in and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
69\,ft.


The fitted value for $x_{1}=9.1$ and $x_{2}=69$ is \Sexpr{treesFIT[1]},
so a point estimate would be \Sexpr{treesFIT[1]} cubic feet. 

\item Report a point prediction for and a 95\% prediction interval for the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
of a hypothetical tree of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
12.5\,in and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
87\,ft.


The fitted value for $x_{1}=12.5$ and $x_{2}=87$ is \Sexpr{treesFIT[3]},
so a point prediction for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
is \Sexpr{treesFIT[3]} cubic feet. 

\end{enumerate}
\end{example}

\subsection{Mean Square Error and Standard Error}

The residuals are given by\begin{equation}
\mathbf{E}=\mathbf{Y}-\hat{\mathbf{Y}}=\mathbf{Y}-\mathbf{H}\mathbf{Y}=(\mathbf{I}-\mathbf{H})\mathbf{Y}.\end{equation}
Now we can use Proposition BLANK to see that the residuals are distributed\begin{equation}
\mathbf{E}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\sigma^{2}(\mathbf{I}-\mathbf{H})),\end{equation}
since $(\mathbf{I}-\mathbf{H})\mathbf{X}\upbeta=\mathbf{X}\upbeta-\mathbf{X}\upbeta=\mathbf{0}$
and $(\mathbf{I}-\mathbf{H})\,(\sigma^{2}\mathbf{I})\,(\mathbf{I}-\mathbf{H})^{\mathrm{T}}=\sigma^{2}(\mathbf{I}-\mathbf{H})^{2}=\sigma^{2}(\mathbf{I}-\mathbf{H})$.
The sum of squared errors $SSE$ is just\begin{equation}
SSE=\mathbf{E}^{\mathrm{T}}\mathbf{E}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})(\mathbf{I}-\mathbf{H})\mathbf{Y}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.\end{equation}
Recall that in SLR we had two parameters ($\beta_{0}$ and $\beta_{1}$)
in our regression model and we estimated $\sigma^{2}$ with $s^{2}=SSE/(n-2)$.
In MLR, we have $p+1$ parameters in our regression model and we might
guess that to estimate $\sigma^{2}$ we would use the \emph{mean square
error} $S^{2}$ defined by \begin{equation}
S^{2}=\frac{SSE}{n-(p+1)}.\end{equation}
That would be a good guess. The \emph{residual standard error} is
$S=\sqrt{S^{2}}$.


\subsection*{How to do it with R}

The residuals are also stored with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees.lm!\inputencoding{utf8}
and may be accessed with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!residuals!\inputencoding{utf8}
function. We only show the first five residuals.

<<>>=
residuals(trees.lm)[1:5]
@

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!summary!\inputencoding{utf8}
function output (shown later) lists the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Residual Standard Error!\inputencoding{utf8}
which is just $S=\sqrt{S^{2}}$. It is stored in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!sigma!\inputencoding{utf8}
component of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
object.

<<>>=
treesumry <- summary(trees.lm)
treesumry$sigma
@

For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data we find $s\approx$ \Sexpr{round(treesumry$sigma, 3)}.


\subsection{Interval Estimates of the Parameters}

We showed in Section BLANK that $\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}$,
which is really just a big matrix -- namely $\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}$
-- multiplied by $\mathbf{Y}$. It stands to reason that the sampling
distribution of $\mathbf{b}$ would be intimately related to the distribution
of $\mathbf{Y}$, which we assumed to be

\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}\right).\end{equation}
Now recall Proposition BLANK that we said we were going to need eventually
(the time is now). That proposition guarantees that

\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right),\end{equation}
since\begin{equation}
\E\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\mathbf{X}\upbeta)=\upbeta,\end{equation}
and\begin{equation}
\mbox{Var}(\mathbf{b})=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\sigma^{2}\mathbf{I})\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1},\end{equation}
the first equality following because the matrix $\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}$
is symmetric.

There is a lot that we can glean from Equation BLANK. First, it follows
that the estimator $\mathbf{b}$ is unbiased (see Section BLANK).
Second, the variances of $b_{0}$, $b_{1}$, \ldots{}, $b_{n}$ are
exactly the diagonal elements of $\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}$,
which is completely known except for that pesky parameter $\sigma^{2}$.
Third, we can estimate the standard error of $b_{i}$ (denoted $S_{b_{i}}$)
with the mean square error $S$ (defined in the previous section)
multiplied by the corresponding diagonal element of $\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}$.
Finally, given estimates of the standard errors we may construct confidence
intervals for $\beta_{i}$ with an interval that looks like\begin{equation}
b_{i}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)S_{b_{i}}.\end{equation}
The degrees of freedom for the Student's $t$ distribution%
\footnote{We are taking great leaps over the mathematical details. In particular,
we have yet to show that $s^{2}$ has a chi-square distribution and
we have not even come close to showing that $b_{i}$ and $s_{b_{i}}$
are independent. But these are entirely outside the scope of the present
book and the reader may rest assured that the proofs await in later
classes. See C.R. Rao for more.%
} are the same as the denominator of $S^{2}$. 


\subsection*{How to do it with \textsf{R}}

To get confidence intervals for the parameters we need only use \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!confint!\inputencoding{utf8}:

<<>>=
confint(trees.lm)
@

<<echo = FALSE, results = hide>>=
treesPAR <- round(confint(trees.lm), 1)
@

For example, using the calculations above we say that for the regression
model \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume ~ Girth + Height!\inputencoding{utf8}
we are 95\% confident that the parameter $\beta_{1}$ lies somewhere
in the interval {[}\Sexpr{treesPAR[2, 1]}, \Sexpr{treesPAR[2, 2]}{]}.


\subsection{Confidence and Prediction Intervals}

We saw in Section BLANK how to make point estimates of the mean value
of additional observations and predict values of future observations,
but how good are our estimates? We need confidence and prediction
intervals to gauge their accuracy, and lucky for us the formulas look
similar to the ones we saw in SLR.

In Equation BLANK we wrote $\hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b},$
and in Equation BLANK we saw that \begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right),\end{equation}
The following is therefore immediate from Proposition BLANK:\begin{equation}
\hat{Y}(\mathbf{x}_{0})\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{x}_{0}^{\mathrm{T}}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\right).\end{equation}
It should be no surprise that confidence intervals for the mean value
of a future observation at the location $\mathbf{x}_{0}=\begin{bmatrix}x_{10} & x_{20} & \ldots & x_{p0}\end{bmatrix}^{\mathrm{T}}$
are given by\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.\end{equation}
Intuitively, $\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}$
measures the distance of $\mathbf{x}_{0}$ from the center of the
data. The degrees of freedom in the Student's $t$ critical value
are $n-(p+1)$ because we need to estimate $p+1$ parameters.

Prediction intervals for a new observation at $\mathbf{x}_{0}$ are
given by\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{1+\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.\end{equation}
The prediction intervals are wider than the confidence intervals,
just as in Section BLANK.


\subsection*{How to do it with \textsf{R}}

The syntax is identical to that used in SLR, with the proviso that
we need to specify values of the independent variables in the data
frame \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!new!\inputencoding{utf8}
as we did in Section BLANK (which we repeat here for illustration).

<<>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
@

Confidence intervals are given by

<<>>=
predict(trees.lm, newdata = new, interval = "confidence")
@

<<echo = FALSE, results = hide>>=
treesCI <- round(predict(trees.lm, newdata = new, interval = "confidence"), 1)
@

Prediction intervals are given by

<<>>=
predict(trees.lm, newdata = new, interval = "prediction")
@

<<echo = FALSE, results = hide>>=
treesPI <- round(predict(trees.lm, newdata = new, interval = "prediction"), 1)
@

As before, the interval type is decided by the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!interval!\inputencoding{utf8}
argument and the default confidence level is 95\% (which can be changed
with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!level!\inputencoding{utf8}
argument). 
\begin{example}
Using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data,
\begin{enumerate}
\item Report a 95\% confidence interval for the mean \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
of a tree of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
9.1\,in and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
69\,ft.


The 95\% CI is given by {[}\Sexpr{treesCI[1, 2]}, \Sexpr{treesCI[1, 3]}{]},
so with 95\% confidence the mean \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
lies somewhere between \Sexpr{treesCI[1, 2]} cubic feet and \Sexpr{treesCI[1, 3]}
cubic feet.

\item Report a 95\% prediction interval for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
of a hypothetical tree of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
12.5\,in and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
87\,ft.


The 95\% prediction interval is given by {[}\Sexpr{treesCI[3, 2]},
\Sexpr{treesCI[3, 3]}{]}, so with 95\% confidence we may assert that
the hypothetical \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
of a tree of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
12.5\,in and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
87\,ft would lie somewhere between \Sexpr{treesCI[3, 2]} cubic feet
and \Sexpr{treesCI[3, 3]} feet.

\end{enumerate}
\end{example}

\section{Model Utility and Inference}


\subsection{Multiple Coefficient of Determination}

We saw in Section BLANK that the error sum of squares $SSE$ can be
conveniently written in MLR as \begin{equation}
SSE=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.\end{equation}
It turns out that there are equally convenient formulas for the total
sum of squares $SSTO$ and the regression sum of squares $SSR$. They
are :\begin{alignat}{1}
SSTO= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{I}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}\end{alignat}
and\begin{alignat}{1}
SSR= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{H}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}.\end{alignat}
(The matrix $\mathbf{J}$ is defined in Appendix BLANK.) Immediately
from Equations BLANK, BLANK, and BLANK we get the \emph{Anova Equality}\begin{equation}
SSTO=SSE+SSR.\end{equation}
(See Exercise BLANK.) We define the \emph{multiple coefficient of
determination} by the formula\begin{equation}
R^{2}=1-\frac{SSE}{SSTO}.\end{equation}


We interpret $R^{2}$ as the proportion of total variation that is
explained by the multiple regression model. In MLR we must be careful,
however, because the value of $R^{2}$ can be artificially inflated
by the addition of explanatory variables to the model, regardless
of whether or not the added variables are useful with respect to prediction
of the response variable. In fact, it can be proved that the addition
of a single explanatory variable to a regression model will increase
the value of $R^{2}$, \emph{no matter how worthless} the explanatory
variable is. We could model the height of the ocean tides, then add
a variable for the length of cheetah tongues on the Serengeti plain,
and our $R^{2}$ would inevitably increase.

This is a problem, because as the philosopher, Occam, once said: {}``causes
should not be multiplied beyond necessity''. We address the problem
by penalizing $R^{2}$ when parameters are added to the model. The
result is an \emph{adjusted $R^{2}$} which we denote by $\overline{R}^{2}$.\begin{equation}
\overline{R}^{2}=\left(R^{2}-\frac{p}{n-1}\right)\left(\frac{n-1}{n-p-1}\right).\end{equation}
It is good practice for the statistician to weigh both $R^{2}$ and
$\overline{R}^{2}$ during assesment of model utility. In many cases
their values will be very close to each other. If their values differ
substantially, or if one changes dramatically when an explanatory
variable is added, then (s)he should take a closer look at the explanatory
variables in the model.


\subsection*{How to do it with R}

For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data, we can get $R^{2}$ and $\overline{R}^{2}$ from the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
output or access the values directly by name as shown (recall that
we stored the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
object in \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!treesumry!\inputencoding{utf8}).

<<>>=
treesumry$r.squared
treesumry$adj.r.squared
@

High values of $R^{2}$ and $\overline{R}^{2}$ such as these indicate
that the model fits very well, which agrees with what we saw in Figure
BLANK.


\subsection{Overall \emph{F}-Test}

Another way to assess the model's utility is to to test the hypothesis\[
H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\mbox{ versus }H_{1}:\mbox{ at least one \ensuremath{\beta_{i}\neq0}}.\]
The idea is that if all $\beta_{i}$'s were zero, then the explanatory
variables $X_{1},\ldots,X_{p}$ would be worthless predictors for
the response variable $Y$. We can test the above hypothesis with
the overall $F$ statistic, which in MLR is defined by\begin{equation}
F=\frac{SSR/p}{SSE/(n-p-1)}.\end{equation}
When the regression assumptions hold and under $H_{0}$, it can be
shown that $F\sim\mathsf{f}(\mathtt{df1}=p,\,\mathtt{df2}=n-p-1)$.
We reject $H_{0}$ when $F$ is large, that is, when the explained
variation is large relative to the unexplained variation.$ $


\subsection*{How to do it with R}

The overall $F$ statistic and its associated \emph{p}-value is listed
at the bottom of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
output, or we can access it directly by name; it is stored in the
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!fstatistic!\inputencoding{utf8}
component of the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
object. 

<<>>=
treesumry$fstatistic
@

For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data, we see that $F=$ \Sexpr{treesumry$fstatistic[1]} with a \emph{p}-value
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!< 2.2e-16!\inputencoding{utf8}.
Consequently we reject $H_{0}$, that is, the data provide strong
evidence that not all $\beta_{i}$'s are zero.


\subsection{Student's \emph{t} Tests}

We know that \begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right)\end{equation}
and we have seen how to test the hypothesis $H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0$,
but let us now consider the test\begin{equation}
H_{0}:\beta_{i}=0\mbox{ versus }H_{1}:\beta_{i}\neq0,\end{equation}
where $\beta_{i}$ is the coefficient for the $i$$^{\textrm{th}}$
independent variable. We test the hypothesis by calculating a statistic,
examining it's null distribution, and rejecting $H_{0}$ if the \emph{p}-value
is small. If $H_{0}$ is rejected, then we conclude that there is
a significant relationship between $Y$ and $x_{i}$ \emph{in the
regression model} $Y\sim(x_{1},\ldots,x_{p})$. This last part of
the sentence is very important because the significance of the variable
$x_{i}$ sometimes depends on the presence of other independent variables
in the model%
\footnote{In other words, a variable might be highly significant one moment
but then fail to be significant when another variable is added to
the model. When this happens it often indicates a problem with the
explanatory variables, such as \emph{multicollinearity}. See Section
BLANK.%
}. 

To test the hypothesis we go to find the sampling distribution of
$b_{i},$ the estimator of the corresponding parameter $\beta_{i}$,
when the null hypothesis is true. We saw in Section BLANK that \begin{equation}
T_{i}=\frac{b_{i}-\beta_{i}}{S_{b_{i}}}\end{equation}
has a Student's $t$ distribution with $n-(p+1)$ degrees of freedom.
(Remember, we are estimating $p+1$ parameters.) Consequently, under
the null hypothesis $H_{0}:\beta_{i}=0$ the statistic $t_{i}=b_{i}/S_{b_{i}}$
has a $\mathsf{t}(\mathtt{df}=n-p-1)$ distribution.


\subsection*{How to do it with R}

The Student's $t$ tests for significance of the individual explanatory
variables are shown in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}
output.

<<>>=
treesumry
@

We see from the \emph{p}-values that there is a significant linear
relationship between \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and between \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
in the regression model \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume ~ Girth + Height!\inputencoding{utf8}.
Further, it appears that the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Intercept!\inputencoding{utf8}
is significant in the aforementioned model.


\section{Polynomial Regression}


\subsection{Quadratic Regression Model}

In each of the previous sections we assumed that $\mu$ was a linear
function of the explanatory variables. For example, in SLR we assumed
that $\mu(x)=\beta_{0}+\beta_{1}x$, and in our previous MLR examples
we assumed $\mu(x_{1},x_{2})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$.
In every case the scatterplots indicated that our assumption was reasonable.
Sometimes, however, plots of the data suggest that the linear model
is incomplete and should be modified. 

For example, let us examine a scatterplot of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
versus \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
a little more closely. See Figure BLANK.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(Volume ~ Girth, data = trees)
@
\par\end{centering}

\caption{Scatterplot of Volume versus Girth}

\end{figure}


There might be a slight curvature to the data; the volume curves ever
so slightly upward as the girth increases. After looking at the plot
we might try to capture the curvature with a mean response such as
\begin{equation}
\mu(x_{1})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}.\end{equation}
The model associated with this choice of $\mu$ is\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon.\end{equation}
The regression assumptions are the same. Almost everything indeed
is the same. In fact, it is still called a {}``linear regression
model'', since the mean response $\mu$ is linear \emph{in the parameters}
$\beta_{0}$, $\beta_{1}$, and $\beta_{2}$. 

\textbf{\textsc{However, there is one important difference.}} When
we introduce the squared term in the model, we inadvertently also
introduce strong dependence between the terms which can cause significant
numerical problems when it comes time to calculate the parameter estimates.
Therefore, we should usually rescale the independent variable to have
mean zero (and even variance one if we wish) \textbf{\textsc{before}}
fitting the model. That is, we replace the $x_{i}$'s with $x_{i}-\xbar$
(or $(x_{i}-\xbar)/s$) before fitting the model. 


\subsection*{How to do it with \textsf{R}}

There are at least two ways to fit a quadratic model to the variables
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
using \textsf{R}.
\begin{enumerate}
\item One way would be to square the values for \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and save them in a vector \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girthsq!\inputencoding{utf8}.
Next, fit the linear model \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume ~ Girth + Girthsq!\inputencoding{utf8}.
\item Another way would be to use the \emph{insulate} function in \textsf{R},
denoted by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!I!\inputencoding{utf8}:


\inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily}]
Volume ~ Girth + I(Girth^2)
\end{lstlisting}
\inputencoding{utf8}

\end{enumerate}
The second method is shorter and does not use as much of \textsf{R}'s
memory but the end result is the same. And once we calculate and store
the fitted model (in, say, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!treesquad.lm!\inputencoding{utf8})
all of the previous comments regarding \textsf{R} apply.
\begin{example}
We will fit the quadratic model to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data and display the results with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!summary!\inputencoding{utf8}.
Note that we may rescale the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
variable to have zero mean and unit variance on-the-fly with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!scale!\inputencoding{utf8}
function.
\end{example}
<<>>=
treesquad.lm <- lm(Volume ~ scale(Girth) + I(scale(Girth)^2), data = trees)
summary(treesquad.lm)
@

We see that the $F$ statistic indicates the overall model including
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth^2!\inputencoding{utf8}
is significant. Further, there is strong evidence that both \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth^2!\inputencoding{utf8}
are significantly related to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8}.
We may examine a scatterplot together with the fitted quadratic function
using the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lines!\inputencoding{utf8}
function, which adds a line to the plot tracing the estimated mean
response.

<<eval = FALSE>>=
plot(Volume ~ scale(Girth), data = trees)
lines(fitted(treesquad.lm) ~ scale(Girth), data = trees)
@

The plot is shown in Figure \ref{cap:Fitting-the-Quadratic}. Pay
attention to the scale on the $x$-axis: it is on the scale of the
transformed $ $\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
data and not on the original scale.

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(Volume ~ scale(Girth), data = trees)
lines(fitted(treesquad.lm) ~ scale(Girth), data = trees)
@
\par\end{centering}

\caption{A quadratic model for the trees data\label{cap:Fitting-the-Quadratic}}

\end{figure}

\begin{rem}
When a model includes a quadratic term for an independent variable,
it is customary to also include the linear term in the model. The
principle is called \emph{parsimony}. More generally, if the researcher
decides to include $x^{m}$ as a term in the model, then (s)he should
also include all lower order terms $x$, $x^{2}$, \ldots{},$x^{m-1}$
in the model. 
\end{rem}
We do estimation/prediction the same way that we did in Section BLANK,
except we do not need a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
column in the dataframe \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!new!\inputencoding{utf8}
since the variable is not included in the quadratic model.

<<>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5))
predict(treesquad.lm, newdata = new, interval = "prediction")
@

The predictions and intervals are slightly different from what they
were previously. Notice that it was not necessary to rescale the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
prediction data before input to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!predict!\inputencoding{utf8}
function; the model did the rescaling for us automatically.
\begin{rem}
We have mentioned on several occasions that it is important to rescale
the explanatory variables for polynomial regression. Watch what happens
if we ignore this advice:

<<>>=
summary(lm(Volume ~ Girth + I(Girth^2), data = trees))
@

Now nothing is significant in the model except \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth^2!\inputencoding{utf8}.
We could delete the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Intercept!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
from the model, but the model would no longer be \emph{parsimonious}.
A novice may see the output and be confused about how to proceed,
while the seasoned statistician recognizes immediately that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth^2!\inputencoding{utf8}
are highly correlated (see Section BLANK). The only remedy to this
ailment is to rescale \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8},
which we should have done in the first place.

In Example BLANK of Section BLANK we investigate this issue further.
\end{rem}

\section{Interaction}

In our model for tree volume there have been two independent variables:
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}.
We may suspect that the independent variables are related, that is,
values of one variable may tend to influence values of the other.
It may be desirable to include an additional term in our model to
try and capture the dependence between the variables. Interaction
terms are formed by multiplying one (or more) explanatory variable(s)
by another. 
\begin{example}
Perhaps the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
of the tree interact to influence the its \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume!\inputencoding{utf8};
we would like to investigate whether the model (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
= $x_{1}$ and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
= $x_{2}$) \begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon\end{equation}
would be significantly improved by the model
\end{example}
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{1:2}x_{1}x_{2}+\epsilon,\end{equation}
where the subscript $1:2$ denotes that $\beta_{1:2}$ is a coefficient
of an interaction term between $x{}_{1}$ and $x_{2}$. 


\paragraph*{What does it mean?}

Consider the mean response $\mu(x_{1},x_{2})$ as a function of $x_{2}$:\begin{equation}
\mu(x_{2})=(\beta_{0}+\beta_{1}x_{1})+\beta_{2}x_{2}.\end{equation}
This is a linear function of $x_{2}$ with slope $\beta_{2}$. As
$x_{1}$ changes, the $y$-intercept of the mean response in $x_{2}$
changes, but the slope remains the same. Therefore, the mean response
in $x_{2}$ is represented by a collection of parallel lines all with
common slope $\beta_{2}$.

Now think about what happens when the interaction term $\beta_{1:2}x_{1}x_{2}$
is included. The mean response in $x_{2}$ now looks like\begin{equation}
\mu(x_{2})=(\beta_{0}+\beta_{1}x_{1})+(\beta_{2}+\beta_{1:2}x_{1})x_{2}.\end{equation}
In this case we see that not only the $y$-intercept changes when
$x_{1}$ varies, but the slope also changes in $x_{1}$. Thus, the
interaction term allows the slope of the mean response in $x_{2}$
to increase and decrease as $x_{1}$ varies.


\subsection*{How to do it with \textsf{R}}

There are several ways to introduce an interaction term into the model.
\begin{enumerate}
\item Make a new variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prod <- Girth * Height!\inputencoding{utf8},
then include \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!prod!\inputencoding{utf8}
in the model formula \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume ~ Girth + Height + prod!\inputencoding{utf8}.
This method is perhaps the most transparent, but it also reserves
memory space unnecessarily.
\item Once can construct an interaction term directly in \textsf{R} with
a colon {}``\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!:!\inputencoding{utf8}''.
For this example, the model formula would look like \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Volume ~ Girth + Height + Girth:Height!\inputencoding{utf8}.
\end{enumerate}
For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!trees!\inputencoding{utf8}
data, we fit the model with the interaction using method two and see
if it is significant:

<<>>=
treesint.lm <- lm(Volume ~ Girth + Height + Girth:Height, data = trees)
summary(treesint.lm)
@

We can see from the output that the interaction term is highly significant.
Further, the estimate $b_{1:2}$ is positive. This means that the
slope of $\mu(x_{2})$ is steeper for bigger values of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}.
Keep in mind: the same interpretation holds for $\mu(x_{1})$; that
is, the slope of $\mu(x_{1})$ is steeper for bigger values of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}.

For the sake of completeness we calculate confidence intervals for
the parameters and do prediction as before.

<<>>=
confint(treesint.lm)
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
predict(treesint.lm, newdata = new, interval = "prediction")
@
\begin{rem}
There are two other ways to include interaction terms in model formulas.
For example, we could have written \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth * Height!\inputencoding{utf8}
or even \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!(Girth + Height)^2!\inputencoding{utf8}
and both would be the same as \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth + Height + Girth:Height!\inputencoding{utf8}.
\end{rem}
These examples can be generalized to more than two independent variables,
say three, four, or even more. We may be interested in seeing whether
any pairwise interactions are significant. We do this with a model
formula that looks something like\texttt{ }\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!y ~ (x1 + x2 + x3 + x4)^2!\inputencoding{utf8}. 


\section{Qualitative Explanatory Variables}

We have so far been concerned with numerical independent variables
taking values in a subset of real numbers. In this section, we extend
our treatment to include the case in which one of the explanatory
variables is qualitative, that is, a \emph{factor}. Qualitative variables
take values in a set of \emph{levels}, which may or may not be ordered.
See Section BLANK.
\begin{note*}
The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data do not have any qualitative explanatory variables, so we will
construct one for illustrative purposes. We will leave the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Girth!\inputencoding{utf8}
variable alone, but we will replace the variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Height!\inputencoding{utf8}
by a new variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall!\inputencoding{utf8}
which indicates whether or not the cherry tree is taller than a certain
threshold (which for the sake of argument will be the sample median
height of 76\,ft). That is, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall!\inputencoding{utf8}
will be defined by\begin{equation}
\mathtt{Tall}=\begin{cases}
\mathtt{yes}, & \mbox{if }\mathtt{Height}>76,\\
\mathtt{no}, & \mbox{if }\mathtt{Height}\leq76.\end{cases}\end{equation}
We can construct \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall!\inputencoding{utf8}
very quickly in \textsf{R} with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!cut!\inputencoding{utf8}
function:

<<>>=
trees$Tall <- cut(trees$Height, breaks = c(-Inf, 76, Inf), labels = c("no","yes"))
trees$Tall[1:5]
@

Note that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall!\inputencoding{utf8}
is automatically generated to be a factor with the labels in the correct
order. See \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!?cut!\inputencoding{utf8}
for more. 

\end{note*}
Once we have \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall!\inputencoding{utf8}
we include it in the regression model just like we would any other
variable. It is handled internally in a special way. Define a {}``dummy
variable'' \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tallyes!\inputencoding{utf8}
that takes values\begin{equation}
\mathtt{Tallyes}=\begin{cases}
1, & \mbox{if }\mathtt{Tall}=\mathtt{yes},\\
0, & \mbox{otherwise.}\end{cases}\end{equation}
That is, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tallyes!\inputencoding{utf8}
is an \emph{indicator} \emph{variable} which indicates when a respective
tree is tall. The model may now be written as \begin{equation}
\mathtt{Volume}=\beta_{0}+\beta_{1}\mathtt{Girth}+\beta_{2}\mathtt{Tallyes}+\epsilon.\end{equation}
Let us take a look at what this definition does to the mean response.
Trees with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall = yes!\inputencoding{utf8}
will have the mean response\begin{equation}
\mu(\mathtt{Girth})=(\beta_{0}+\beta_{2})+\beta_{1}\mathtt{Girth},\end{equation}
while trees with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall = no!\inputencoding{utf8}
will have the mean response\begin{equation}
\mu(\mathtt{Girth})=\beta_{0}+\beta_{1}\mathtt{Girth}.\end{equation}
In essence, we are fitting two regression lines: one for tall trees,
and one for short trees. The regression lines have the same slope
but they have differing $y$ intercepts (which are exactly $|\beta_{2}|$
far apart).


\subsection*{How to do it with \textsf{R}}

The important thing is to double check that the qualitative variable
in question is stored as a factor. The way to check is with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!class!\inputencoding{utf8}
command. For example,

<<>>=
class(trees$Tall)
@

If the qualitative variable is not yet stored as a factor then we
may convert it to one with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!factor!\inputencoding{utf8}
command. See Appendix BLANK. Other than this we perform MLR as we
normally would.

<<>>=
treesdummy.lm <- lm(Volume ~ Girth + Tall, data = trees)
summary(treesdummy.lm)
@

From the output we see that all parameter estimates are statistically
significant and we conclude that the mean response differs for trees
with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall = yes!\inputencoding{utf8}
and trees with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall = no!\inputencoding{utf8}. 
\begin{rem}
We were somewhat disingenuous when we defined the dummy variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tallyes!\inputencoding{utf8}
because, in truth, \textsf{R} defines \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tallyes!\inputencoding{utf8}
automatically without input from the user%
\footnote{That is, \textsf{R} by default handles contrasts according to its
internal settings which may be customized by the user for fine control.
Given that we will not investigate contrasts further in this book
it does not serve the discussion to delve into those settings, either.
The interested reader should check ?contrasts for details.%
}. Indeed, the author fit the model beforehand and wrote the discussion
afterward with the knowledge of what \textsf{R} would do so that the
output the reader saw would match what (s)he had previously read.
The way that \textsf{R} handles factors internally is part of a much
larger topic concerning \emph{contrasts}, which falls outside the
scope of this book. The interested reader should see BLANK or BLANK
for more.
\end{rem}

\begin{rem}
In general, if an explanatory variable \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!foo!\inputencoding{utf8}
is qualitative with $n$ levels \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!bar1!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!bar2!\inputencoding{utf8},
\ldots{}, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!barn!\inputencoding{utf8}
then \textsf{R} will by default automatically define $n-1$ indicator
variables in the following way:\begin{eqnarray*}
\mathtt{foobar2} & = & \begin{cases}
1, & \mbox{if }\mathtt{foo}=\mathtt{"bar2"},\\
0, & \mbox{otherwise.}\end{cases},\,\ldots,\,\mathtt{foobarn}=\begin{cases}
1, & \mbox{if }\mathtt{foo}=\mathtt{"barn"},\\
0, & \mbox{otherwise.}\end{cases}\end{eqnarray*}
The level \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!bar1!\inputencoding{utf8}
is represented by $\mathtt{foobar2}=\cdots=\mathtt{foobarn}=0$. We
just need to make sure that \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!foo!\inputencoding{utf8}
is stored as a factor and \textsf{R} will take care of the rest.
\end{rem}

\subsection*{Graphing the Regression Lines}

We can see a plot of the two regression lines with the following mouthful
of code.

<<eval = FALSE>>=
treesTall <- split(trees, trees$Tall)
treesTall[["yes"]]$Fit <- predict(treesdummy.lm, treesTall[["yes"]])
treesTall[["no"]]$Fit <- predict(treesdummy.lm, treesTall[["no"]])
plot(Volume ~ Girth, data = trees, type = "n")
points(Volume ~ Girth, data = treesTall[["yes"]], pch = 1)
points(Volume ~ Girth, data = treesTall[["no"]], pch = 2)
lines(Fit ~ Girth, data = treesTall[["yes"]])
lines(Fit ~ Girth, data = treesTall[["no"]])
@

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
treesTall <- split(trees, trees$Tall)
treesTall[["yes"]]$Fit <- predict(treesdummy.lm, treesTall[["yes"]])
treesTall[["no"]]$Fit <- predict(treesdummy.lm, treesTall[["no"]])
plot(Volume ~ Girth, data = trees, type = "n")
points(Volume ~ Girth, data = treesTall[["yes"]], pch = 1)
points(Volume ~ Girth, data = treesTall[["no"]], pch = 2)
lines(Fit ~ Girth, data = treesTall[["yes"]])
lines(Fit ~ Girth, data = treesTall[["no"]])
@
\par\end{centering}

\caption{A dummy variable model for the trees data}

\end{figure}


It may look intimidating but there is reason to the madness. First
we \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!split!\inputencoding{utf8}
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data into two pieces, with groups determined by the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall!\inputencoding{utf8}
variable. Next we add the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Fit!\inputencoding{utf8}ted
values to each piece via \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!predict!\inputencoding{utf8}.
Then we set up a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!plot!\inputencoding{utf8}
for the variables \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume!\inputencoding{utf8}
versus \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Girth!\inputencoding{utf8},
but we do not plot anything yet (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!type = n!\inputencoding{utf8})
because we want to use different symbols for the two groups. Next
we add \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!points!\inputencoding{utf8}
to the plot for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall = yes!\inputencoding{utf8}
trees and use an open circle for a plot character (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!pch = 1!\inputencoding{utf8}),
followed by \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!points!\inputencoding{utf8}
for the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Tall = no!\inputencoding{utf8}
trees with a triangle character (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!pch = 2!\inputencoding{utf8}).
Finally, we add regression \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!lines!\inputencoding{utf8}
to the plot, one for each group.

There are other -- shorter -- ways to plot regression lines by groups,
namely the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!scatterplot!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!car!\inputencoding{utf8}
package and the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!xyplot!\inputencoding{utf8}
function in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!lattice!\inputencoding{utf8}
package. We elected to introduce the reader to the above approach
since many advanced plots in \textsf{R} are done in a similar, consecutive
fashion.


\section{Partial \emph{F} Statistic}

We saw in Section BLANK how to test $H_{0}:\beta_{0}=\beta_{1}=\cdots=\beta_{p}=0$
with the overall $F$ statistic and we saw in Section BLANK how to
test $H_{0}:\beta_{i}=0$ that a particular coefficient $\beta_{i}$
is zero. Sometimes, however, we would like to test whether a certain
part of the model is significant. Consider the regression model\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\beta_{j+1}x_{j+1}+\cdots+\beta_{p}x_{p}+\epsilon,\end{equation}
 where $j\geq1$ and $p\geq2$. Now we wish to test the hypothesis\begin{equation}
H_{0}:\beta_{j+1}=\beta_{j+2}=\cdots=\beta_{p}=0\end{equation}
versus the alternative \begin{equation}
H_{1}:\mbox{at least one of \ensuremath{\beta_{j+1},\ \beta_{j+2},\ ,\ldots,\beta_{p}\neq0}}.\end{equation}
The interpretation of $H_{0}$ is that none of the variables $x_{j+1}$,
\ldots{},$x_{p}$ is significantly related to $Y$ and the interpretation
of $H_{1}$ is that at least one of $x_{j+1}$, \ldots{},$x_{p}$
is significantly related to $Y$. In essence, for this hypothesis
test there are two competing models under consideration:\begin{align}
\mbox{the full model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{p}x_{p}+\epsilon,\\
\mbox{the reduced model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\epsilon,\end{align}
Of course, the full model will always explain the data \emph{better}
than the reduced model, but does the full model explain the data \emph{significantly
better} than the reduced model? This question is exactly what the
partial $F$ statistic is designed to answer.

We first calculate $SSE_{f}$, the unexplained variation in the full
model, and $SSE_{r}$, the unexplained variation in the reduced model.
We base our test on the difference $SSE_{r}-SSE_{f}$ which measures
the reduction in unexplained variation attributable to the variables
$x_{j+1}$, \ldots{},$x_{p}$. In the full model there are $p+1$
parameters and in the reduced model there are $j+1$ parameters, which
gives a difference of $p-j$ parameters (hence degrees of freedom).
The partial \emph{F} statistic is \begin{equation}
F=\frac{(SSE_{r}-SSE_{f})/(p-j)}{SSE_{f}/(n-p-1)}.\end{equation}
It can be shown when the regression assumptions hold under $H_{0}$
that the partial $F$ statistic has an $\mathsf{f}(\mathtt{df1}=p-j,\,\mathtt{df2}=n-p-1)$
distribution. We calculate the $p$-value of the observed partial
$F$ statistic and reject $H_{0}$ if the $p$-value is small.


\subsection*{How to do it with \textsf{R}}

The key ingredient above is that the two competing models are \emph{nested}
in the sense that the reduced model is entirely contained within the
complete model. The way to test whether the improvement is significant
is to compute \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lm!\inputencoding{utf8}
objects both for the complete model and the reduced model then compare
the answers with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!anova!\inputencoding{utf8}
function.
\begin{example}
For the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data, let us fit a polynomial regression model and for the sake of
argument we will ignore our own good advice and fail to rescale the
explanatory variables. 

<<>>=
treesfull.lm <- lm(Volume ~ Girth + I(Girth^2) + Height + I(Height^2), data = trees)
summary(treesfull.lm)
@

In this ill-formed model nothing is significant except \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth^2!\inputencoding{utf8}.
Let us continue down this path and suppose that we would like to try
a reduced model which contains nothing but \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Girth^2!\inputencoding{utf8}
(not even an \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Intercept!\inputencoding{utf8}).
Our two models are now\begin{align*}
\mbox{the full model:} & \quad Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\beta_{3}x_{2}+\beta_{4}x_{2}^{2}+\epsilon,\\
\mbox{the reduced model:} & \quad Y=\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon,\end{align*}
We fit the reduced model with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!lm!\inputencoding{utf8}
and store the results:

<<>>=
treesreduced.lm <- lm(Volume ~ -1 + Girth + I(Girth^2), data = trees)
@

To delete the intercept from the model we used \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!-1!\inputencoding{utf8}
in the model formula. Next we compare the two models with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!anova!\inputencoding{utf8}
function. The convention is to list the models from smallest to largest.

<<>>=
anova(treesreduced.lm, treesfull.lm)
@

We see from the output that the complete model is highly significant
compared to the model that does not incorporate \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height!\inputencoding{utf8}
or the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Intercept!\inputencoding{utf8}.
We wonder (with our tongue in our cheek) if the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height^2!\inputencoding{utf8}
term in the full model is causing all of the trouble. We will fit
an alternative reduced model that only deletes \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height^2!\inputencoding{utf8}.

<<>>=
treesreduced2.lm <- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees)
anova(treesreduced2.lm, treesfull.lm)
@

In this case, the improvement to the reduced model that is attributable
to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height^2!\inputencoding{utf8}
is not significant, so we can delete \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height^2!\inputencoding{utf8}
from the model with a clear conscience. We notice that the \emph{p}-value
for this latest partial $F$ test is 0.8865, which seems to be remarkably
close to the \emph{p}-value we saw for the univariate \emph{t} test
of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Height^2!\inputencoding{utf8}
at the beginning of this example. In fact, the \emph{p}-values are
\emph{exactly} the same. Perhaps now we gain some insight into the
true meaning of the univariate tests.
\end{example}

\section{Residual Analysis and Diagnostic Tools }

We encountered many, many diagnostic measures for simple linear regression
in Section BLANK. All of these are valid in multiple linear regression,
too, but there are some slight changes that we need to make for the
multivariate case. We list these below, and apply them to the trees
example. 
\begin{description}
\item [{Shapiro-Wilk,~Breusch-Pagan,~Durbin-Watson:}] unchanged from
SLR, but we are now equipped to talk about the Shapiro-Wilk test statistic
for the residuals. It is defined by the formula \begin{equation}
W=\frac{\mathbf{a}^{\mathrm{T}}\mathbf{E}^{\ast}}{\mathbf{E}^{\mathrm{T}}\mathbf{E}},\end{equation}
where $\mathbf{E}^{\ast}$ is the sorted residuals and $\mathbf{a}_{1\times\mathrm{n}}$
is defined by \begin{equation}
\mathbf{a}=\frac{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}}{\sqrt{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}\mathbf{V}^{-1}\mathbf{m}}},\end{equation}
where $\mathbf{m}_{\mathrm{n}\times1}$ and $\mathbf{V}_{\mathrm{n}\times\mathrm{n}}$
are the mean and covariance matrix, respectively, of the order statistics
from an $\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\mathbf{I}\right)$
distribution. 
\item [{Leverages:}] are defined to be the diagonal entries of the hat
matrix $\mathbf{H}$ (which is why we called them $h_{ii}$ in Section
BLANK). The sum of the leverages is $\mbox{tr}(\mathbf{H})=p+1$.
One rule of thumb considers a leverage extreme if it is larger than
double the mean leverage value, which is $2(p+1)/n$, and another
rule of thumb considers leverages bigger than 0.5 to indicate high
leverage, while values between 0.3 and 0.5 indicate moderate leverage.
\item [{Standardized~residuals:}] unchanged. Considered extreme if $|R_{i}|>2$.
\item [{Studentized~residuals:}] compared to a $\mathsf{t}(\mathtt{df}=n-p-2)$
distribution. 
\item [{$DFBETAS$:}] The formula is generalized to\begin{equation}
(DFBETAS)_{j(i)}=\frac{b_{j}-b_{j(i)}}{S_{(i)}\sqrt{c_{jj}}},\quad j=0,\ldots p,\ i=1,\ldots,n,\end{equation}
where $c_{jj}$ is the $j^{\mathrm{th}}$ diagonal entry of $(\mathbf{X}^{\mathrm{T}}\mathbf{X})^{-1}$.
Values larger than one for small data sets or $2/\sqrt{n}$ for large
data sets should be investigated.
\item [{$DFFITS$:}] unchanged. Larger than one in absolute value is considered
extreme.
\item [{Cook's~$D$:}] compared to an $\mathsf{f}(\mathtt{df1}=p+1,\,\mathtt{df2}=n-p-1)$
distribution. Observations falling higher than the 50$^{\textrm{th}}$
percentile are extreme. 
\end{description}
Note that plugging the value $p=1$ into the formulas will recover
all of the ones we saw in Chapter BLANK.


\section{Additional Topics}


\subsection{Nonlinear Regression}

We spent the entire chapter talking about the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data, and all of our models looked like \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Volume ~ Girth + Height!\inputencoding{utf8}
or a variant of this model. But let us think again: we know from elementary
school that the volume of a rectangle is $V=lwh$ and the volume of
a cylinder (which is closer to what a black cherry tree looks like)
is\begin{equation}
V=\pi r^{2}h\quad\mbox{or}\quad V=4\pi dh,\end{equation}
where $r$ and $d$ represent the radius and diameter of the tree,
respectively. With this in mind, it would seem that a more appropriate
model for $\mu$ might be $ $\begin{equation}
\mu(x_{1},x_{2})=\beta_{0}x_{1}^{\beta_{1}}x_{2}^{\beta_{2}},\end{equation}
where $\beta_{1}$ and $\beta_{2}$ are parameters to adjust for the
fact that a black cherry tree is not a perfect cylinder.

How can we fit this model? The model is not linear in the parameters
any more, so our linear regression methods will not work\ldots{}or
will they? In the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
example we may take the logarithm of both sides of Equation BLANK
to get\begin{equation}
\mu^{\ast}(x_{1},x_{2})=\ln\left[\mu(x_{1},x_{2})\right]=\ln\beta_{0}+\beta_{1}\ln x_{1}+\beta_{2}\ln x_{2},\end{equation}
and this new model $\mu^{\ast}$ is linear in the parameters $\beta_{0}^{\ast}=\ln\beta_{0}$,
$\beta_{1}^{\ast}=\beta_{1}$ and $\beta_{2}^{\ast}=\beta_{2}$. We
can use what we have learned to fit a linear model \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!log(Volume) ~ log(Girth) + log(Height)!\inputencoding{utf8},
and everything will proceed as before, with one exception: we will
need to be mindful when it comes time to make predictions because
the model will have been fit on the log scale, and we will need to
transform our predictions back to the original scale (by exponentiating
with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!exp!\inputencoding{utf8})
to make sense.

<<>>=
treesNonlin.lm <- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)
summary(treesNonlin.lm)
@

This is our best model yet (judging by $R^{2}$ and $\overline{R}^{2}$),
all of the parameters are significant, it is simpler than the quadratic
or interaction models, and it even makes theoretical sense. It rarely
gets any better than that.

We may get confidence intervals for the parameters, but remember that
it is usually better to transform back to the original scale for interpretation
purposes :

<<>>=
exp(confint(treesNonlin.lm))
@

(Note that we did not update the row labels of the matrix to show
that we exponentiated and so they are misleading as written.) We do
predictions just as before. Remember to transform the response variable
back to the original scale after prediction.

<<>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
exp(predict(treesNonlin.lm, newdata = new, interval = "confidence"))
@

The predictions and intervals are slightly different from those calculated
earlier, but they are close. Note that we did not need to transform
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Girth!\inputencoding{utf8}
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!Height!\inputencoding{utf8}
arguments in the dataframe \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!new!\inputencoding{utf8}.
All transformations are done for us automatically.


\subsection{Real Nonlinear Regression}

We saw with the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},showstringspaces=false]!trees!\inputencoding{utf8}
data that a nonlinear model might be more appropriate for the data
based on theoretical considerations, and we were lucky because the
functional form of $\mu$ allowed us to take logarithms to transform
the nonlinear model to a linear one. The same trick will not work
in other circumstances, however. We need techniques to fit general
models of the form\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\epsilon,\end{equation}
where $\mu$ is some crazy function that does not lend itself to linear
transformations.

There are a host of methods to address problems like these which are
studied in advanced regression classes. The interested reader should
see BLANK or BLANK. 

It turns out that John Fox has posted an Appendix to his book BLANK
which discusses some of the methods and issues associated with nonlinear
regression; see \url{http://cran.r-project.org/doc/contrib/Fox-Companion/appendix.html}.


\subsection{Multicollinearity}

A multiple regression model exhibits \emph{multicollinearity} when
two or more of the explanatory variables are substantially correlated
with each other. We can measure multicollinearity by having one of
the explanatory play the role of {}``dependent variable'' and regress
it on the remaining explanatory variables. The the $R^{2}$ of the
resulting model is near one, then we say that the model is multicollinear
or shows multicollinearity.

Multicollinearity is a problem because it causes instability in the
regression model. The instability is a consequence of redundancy in
the explanatory variables: a high $R^{2}$ indicates a strong dependence
between the selected independent variable and the others. The redundant
information inflates the variance of the parameter estimates which
can cause them to be statistically insignificant when they would have
been significant otherwise. To wit, multicollinearity is usually measured
by what are called \emph{variance inflation factors}.

Once multicollinearity has been diagnosed there are several approaches
to remediate it. Here are a couple of important ones.
\begin{description}
\item [{Principal~Components~Analysis.}] This approach casts out two
or more of the original explanatory variables and replaces them with
new variables, derived from the original ones, that are by design
uncorrelated with one another. The redundancy is thus eliminated and
we may proceed as usual with the new variables in hand. Principal
Components Analysis is important for other reasons, too, not just
for fixing multicollinearity problems.
\item [{Ridge~Regression.}] The idea of this approach is to replace the
original parameter estimates with a different type of parameter estimate
which is more stable under multicollinearity. The estimators are not
found by ordinary least squares but rather a different optimization
procedure which incorporates the variance inflation factor information.
\end{description}
We decided to omit a thorough discussion of multicollinearity because
we are not equipped to handle the mathematical details. Perhaps the
topic will receive more attention in a later edition.
\begin{itemize}
\item What to do when data are not normal

\begin{itemize}
\item Bootstrap (see Section BLANK).
\end{itemize}
\end{itemize}

\subsection{Akaike's Information Criterion}

aksdjfl\[
AIC=-2\ln L+2(p+1)\]



\section{Chapter Exercises}
\begin{enumerate}
\item Use Equations BLANK, BLANK, and BLANK to prove the Anova Equality:\[
SSTO=SSE+SSR.\]

\end{enumerate}

\chapter{Resampling Methods}

What do I want them to know?
\begin{itemize}
\item basic philosophy of resampling and why it is desired
\item resampling for standard errors and confidence intervals
\end{itemize}

\section{Introduction}

Computers have changed the face of Statistics. Their quick computational
speed and flawless accuracy, coupled with large datasets acquired
by the researcher, make them indispensable for any modern analysis.
In particular, resampling methods (due in large part to Bradley Efron)
have gained prominence in the modern statistician's repertoire. Let
us look at a classical problem to get some insight why.

\textbf{A Classical Question:} Given a population of interest, how
may we effectively learn some of its salient features, \emph{e.g.},
the population's mean? Answer: one way is through representative random
sampling. Given a random sample, how do we summarize the information
contained therein? Answer: by calculating a reasonable statistic,
\emph{e.g.}, the sample mean. Given a value of a statistic, how do
we know whether that value is significantly different from that which
was expected?

\begin{center}
Answer: we don't. 
\par\end{center}

Instead, we look at the \emph{sampling distribution} of the statistic,
and we try to make probabilistic assertions based on a confidence
level or other consideration. For example, we may find ourselves saying
things like, \textquotedbl{}With 95\% confidence, the true population
mean is greater than zero.\textquotedbl{}

\textbf{Problem:} Unfortunately, in most cases the sampling distribution
is \emph{unknown}. Thus in the past, in efforts to say something useful,
statisticians have been obligated to place some restrictive assumptions
on the underlying population. For example, if we suppose that the
population has a normally distribution, then we can say that the distribution
of $\Xbar$ is normal, too, with the same mean (and a smaller standard
deviation). It is then easy to draw conclusions, make inferences,
and go on about our business.

\textbf{An Alternative:} We don't know what the underlying population
distributions is, so let us \emph{estimate} it, just like we would
with any other parameter. The statistic we use is the \emph{empirical
CDF}, that is, the function that places mass $1/n$ at each of the
observed data points $x_{1},\ldots,x_{n}$ (see Section BLANK). As
the sample size increases, we would expect the approximation to get
better and better (with i.i.d.~observations, it does, and there is
a wonderful theorem by Glivenko and Cantelli that proves it). And
now that we have an (estimated) population distribution, it is easy
to find the sampling distribution of any statistic we like: just \textbf{sample}
from the empirical CDF many, many times, calculate the statistic each
time, and make a histogram. Done! Of course, the number of samples
needed to get a representative histogram is prohibitively large\ldots{}human
beings are simply too slow (and clumsy) to do this tedious procedure.

\begin{center}
Enter the computer. 
\par\end{center}

Fortunately, computers are very skilled at doing simple, repetitive
tasks very quickly and accurately. So we employ them to give us a
reasonable idea about the sampling distribution of our statistic,
and we use the generated sampling distribution to guide our inferences
and draw our conclusions. If we would like to have a better approximation
for the sampling distribution, we merely tell the computer to sample
more. In this sense, we are limited only by our current computational
speed and pocket book.
\begin{rem}
Due to the special structure of the empirical CDF, to get an i.i.d.~sample
one needs only to take a random sample of size $n$, with replacement,
from the observed data $x_{1},\ldots,x_{n}$. Repeats are expected
and acceptable. Since we already sampled to get the original data,
the term \emph{resampling} is used to describe the procedure.
\end{rem}
\textbf{A Summary of the Advantages of Resampling Methods:}
\begin{itemize}
\item \textbf{Fewer assumptions.} We are no longer required to assume the
population is normal or the sample size is large. 
\item \textbf{Greater accuracy.} Many classical methods are based on rough
upper bounds or Taylor expansions. The bootstrap procedures can be
iterated long enough to give results accurate to several decimal places,
often beating classical approximations. 
\item \textbf{Generality.} Resampling methods are easy to understand and
apply to a large class of seemingly unrelated procedures. One no longer
needs to memorize long complicated formulas and algorithms. 
\end{itemize}

\section{Bootstrapping Standard Errors}

\textbf{Procedure for Bootstrapping:} To approximate the sampling
distribution of a statistic $S(x)$ based on a $SRS$ of size $n$.
\begin{enumerate}
\item Create many many samples $x_{1}^{\ast},\ldots,x_{M}^{\ast}$, called
\emph{resamples}, by sampling with replacement from the data. 
\item Calculate the statistic of interest $S(x_{1}^{\ast}),\ldots,S(x_{M}^{\ast})$
for each resample. The distribution of the resample statistics is
called a \emph{bootstrap distribution}. 
\item The bootstrap distribution gives information about the sampling distribution
of the statistic. In particular, it gives us some idea about the center,
spread, and shape of the sampling distribution of $S$. \end{enumerate}
\begin{example}
\textbf{Bootstrapping the standard error of the mean.} In this example
we illustrate the bootstrap by trying to estimate the standard error
of the sample mean. We do this in the special case when the underlying
population is $\mathsf{norm}(\mathtt{mean}=2,\,\mathtt{sd}=1)$. Of
course, we don't need a bootstrap distribution; we know from Section
Blank that $\Xbar\sim\mathsf{norm}(\mathtt{mean}=2,\,\mathtt{sd}=1/\sqrt{n})$.
We will use what we already know to see how the bootstrap method performs.
\end{example}
<<>>=
srs <- rnorm(25, mean = 2)
resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
mean(xbarstar)
sd(xbarstar)
@

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
hist(xbarstar, breaks = 40, prob = TRUE)
curve(dnorm(x, 2, 0.2), add = TRUE)
@
\par\end{centering}

\caption{Bootstrapping the standard error of the mean}

\end{figure}


The graph is shown in Figure BLANK.

<<eval = FALSE, keep.source = TRUE>>=
hist(xbarstar, breaks = 40, prob = TRUE)
curve(dnorm(x, 2, 0.2), add = TRUE)  # overlay true normal density
@
\begin{example}
\textbf{Bootstrapping the Standard Error of the Median.} In this example
we extend our study to include more complicated statistics and distributions
such that we do not know the answer ahead of time. This example uses
the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rivers!\inputencoding{utf8}
dataset which gives the lengths (in miles) of 141 \textquotedbl{}major\textquotedbl{}
rivers in North America, as compiled by the US Geological Survey. 

<<>>=
stem(rivers)
@

We see from the stemplot that the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!rivers!\inputencoding{utf8}
data are clearly right-skewed, so a natural estimate of center would
be the sample median. Unfortunately, its sampling distribution falls
out of our reach. We use the bootstrap to help us with this problem,
and the modifications to the last example are scarcely more than trivial.
\end{example}
<<>>=
resamps <- replicate(1000, sample(rivers, 141, TRUE), simplify = FALSE)
medstar <- sapply(resamps, median, simplify = TRUE)
mean(medstar)
sd(medstar)
@

%
\begin{figure}
\begin{centering}
<<echo = FALSE, fig=true, height = 4, width = 6>>=
hist(medstar, breaks = 40, prob = TRUE)
@
\par\end{centering}

\caption{Bootstrapping the standard error of the median}

\end{figure}


The graph is shown in Figure BLANK.

<<eval = FALSE, keep.source = TRUE>>=
hist(medstar, breaks = 40, prob = TRUE)
@
\begin{example}
The boot package in \texttt{R}. It turns out that there are many bootstrap
procedures and commands already built into \texttt{R}, in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot!\inputencoding{utf8}
package. Further, inside the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot!\inputencoding{utf8}
package there is even a function called \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot!\inputencoding{utf8}.
The basic syntax is of the form:

\inputencoding{latin9}
\begin{lstlisting}[basicstyle={\ttfamily}]
boot(data, statistic, R)
\end{lstlisting}
\inputencoding{utf8}
\end{example}
Here, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data!\inputencoding{utf8}
is a vector (or matrix) containing the data to be resampled, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!statistic!\inputencoding{utf8}
is a defined function, \emph{of two arguments}, that tells which statistic
to be computed, and the parameter \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!R!\inputencoding{utf8}
specifies how many resamples should be taken.

For the standard error of the mean (Example BLANK):

<<>>=
library(boot)
mean_fun <- function(x, indices) mean(x[indices])
boot(data = rnorm(25, mean = 2), statistic = mean_fun, R = 1000)
@

For the standard error of the median (Example BLANK):

<<>>=
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
@

We notice that the output from both methods of estimating the standard
errors produced similar results. In fact, the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot!\inputencoding{utf8}
procedure is to be preferred since it invisibly returns much more
information (which we will use later) than our naive script and it
is much quicker in its computations.
\begin{rem}
jldsfj
\begin{itemize}
\item For many statistics, the bootstrap distribution closely resembles
the sampling distribution with respect to spread and shape. However,
the distributions will differ in their centers. While the sampling
distribution is centered at the population mean (plus any bias), the
bootstrap distribution is centered at the original value of the statistic
(plus any bias). We saw that the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot!\inputencoding{utf8}
function gives an empirical estimate of the bias in the statistic. 
\item We tried to estimate the standard error, but we could have (in principle)
tried to estimate something else. Note from the previous remark, however,
that it would be useless to try to estimate the population mean $\mu$
using the bootstrap, since the mean of the bootstrap distribution
is the observed $\xbar$. 
\item You don't get something from nothing. We have seen that we can take
a random sample from a population and use bootstrap methods to get
a very good idea about standard errors, bias, and the like. However,
one must not get lured into believing that by doing some random resampling
somehow one gets more information about the parameters than that which
was contained in the original sample. Indeed, there is some uncertainty
about the parameter due just to the original random sampling, and
there is more uncertainty introduced by resampling. One should think
of the bootstrap as just another method of estimating parameters of
interest. 
\end{itemize}
\end{rem}

\section{Bootstrap Confidence Intervals}


\subsection{Percentile Confidence Intervals}

percentile confidence intervals based on assuming that there is a
(unknown) transformation such that the distribution of 

As a first try, we want to obtain a 95\% confidence interval for a
parameter. Usually the statistic is centered (or at least close by)
the parameter; in such cases a 95\% confidence interval for the parameter
is nothing more than a 95\% confidence interval for the statistic.
And to find a 95\% confidence interval for the statistic we need only
go to its sampling distribution to find an interval that contains
95\% of the area. (The most popular choice is the equal-tailed interval
with 2.5\% in each tail.)

This is incredibly easy to accomplish with the bootstrap. We need
only to take a bunch of bootstrap resamples, order them, and choose
the $\alpha/2$th and $(1-\alpha)$th percentiles. There is a function
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot.ci!\inputencoding{utf8}
in \textsf{R} already created to do just this. Note that in order
to use the function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot.ci!\inputencoding{utf8}
you must first run the program \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot!\inputencoding{utf8}
and save the output in a variable, for example, \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data.boot!\inputencoding{utf8}.
You then plug \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data.boot!\inputencoding{utf8}
into the function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!boot.ci!\inputencoding{utf8}.
\begin{example}
Please see the handout, {}``Bootstrapping Confidence Intervals for
the Median''.
\end{example}
<<>>=
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE), simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
@


\begin{example}
Please see the handout, {}``Bootstrapping Confidence Intervals for
the Median, $2^{\mathrm{nd}}$ try.''
\end{example}
stack.loss because it is small and right skewed

<<>>=
library(boot)
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
@


\subsection{Student's $t$ intervals ({}``normal intervals'')}

The idea is to use confidence intervals that we already know and let
the bootstrap help us when we get into trouble. We know that a $100(1-\alpha)\%$
confidence interval for the mean of a $SRS(n)$ from a normal distribution
is given by \begin{equation}
\Xbar\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}}\end{equation}
where $\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)$ is the appropriate
critical value from Student's $t$ distribution and we remember from
our classes that $\mathrm{SE}(\Xbar)=S/\sqrt{n}$. Of course, the
estimate for the standard error will change when the underlying population
distribution is not normal, or when we use a statistic more complicated
than $\Xbar$. In those situations the bootstrap will give us quite
reasonable estimates for the standard error. And as long as the sampling
distribution of our statistic is approximately bell-shaped with small
bias, the interval \begin{equation}
\mbox{statistic}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)*\mathrm{SE}(\mbox{statistic})\end{equation}
 will have approximately $100(1-\alpha)\%$ confidence of containing
$\E(\mathrm{statistic})$.
\begin{example}
Lawsuit data revisited We will use the t-interval method to find the
bootstrap CI for the Median. We have looked at the bootstrap distribution;
it appears to be symmetric and approximately mound shaped. Further,
we may check that the bias is approximately 40, which on the scale
of these data is practically negligible. Thus, we may consider looking
at the $t$-intervals. Note that, since our sample is so large, instead
of $t$-intervals we will actually be using $z$-intervals.
\end{example}
Please see the handout, {}``Bootstrapping Confidence Intervals for
the Median, $3^{\mathrm{rd}}$ try.''

We see that, considering the scale of the data, the confidence intervals
compare with each other quite well.
\begin{rem}
We have seen two methods for bootstrapping confidence intervals for
a statistic. Which method should we use? If the bias of the bootstrap
distribution is small and if the distribution is close to normal,
then the percentile and $t$-intervals will closely agree. If the
intervals are noticeably different, then it should be considered evidence
that the normality and bias conditions are not met. In this case,
\emph{neither} interval should be used.\end{rem}
\begin{itemize}
\item $BC_{a}$: bias-corrected and accelerated

\begin{itemize}
\item transformation invariant
\item more correct and accurate
\item not monotone in coverage level?
\end{itemize}
\item t - intervals

\begin{itemize}
\item more natural
\item numerically unstable
\end{itemize}
\item Can do things like transform scales, compute confidence intervals,
and then transform back.
\item Studentized bootstrap confidence intervalswhere is the Studentized
version of is the rth order statistic of the simulation
\end{itemize}

\section{Resampling in Hypothesis Tests}

The classical two-sample problem can be stated as follows: given two
groups of interest, we would like to know whether these two groups
are significantly different from one another or whether the groups
are reasonably similar. The standard way to decide is to
\begin{enumerate}
\item Go collect some information from the two groups and calculate an associated
statistic, for example, $\Xbar_{1}-\Xbar_{2}$. 
\item Suppose that there is no difference in the groups, and find the distribution
of the statistic in 1. 
\item Locate the observed value of the statistic with respect to the distribution
found in 2. A value in the main body of the distribution is not spectacular,
it could reasonably have occurred by chance. A value in the tail of
the distribution is unlikely, and hence provides evidence \emph{against}
the null hypothesis. 
\end{enumerate}
PICTURE 

Of course, we usually compute a $p$-value, defined to be the probability
of the observed value of the statistic or more extreme, when the null
hypothesis is true. Small $p$-values are evidence against the null
hypothesis. It is not immediately obvious how to use resampling methods
here, so we discuss an example.
\begin{example}
A study concerned differing dosages of the antiretroviral drug AZT.
The common dosage is 300mg daily. Higher doses cause more side affects,
but are they significantly higher? We examine for a 600mg dose. The
data are as follows: We compare the scores from the two groups by
computing the difference in their sample means. The 300mg data were
entered in x1 and the 600mg data were entered into x2. The observed
difference was

\begin{center}
\begin{tabular}{l|cccccccccc}
300 mg  & 284  & 279  & 289  & 292  & 287  & 295  & 285  & 279  & 306  & 298 \tabularnewline
600 mg  & 298  & 307  & 297  & 279  & 291  & 335  & 299  & 300  & 306  & 291 \tabularnewline
\end{tabular}
\par\end{center}

The average amounts can be found:

\texttt{> mean(x1)}

\texttt{{[}1{]} 289.4}

\texttt{> mean(x2)}

\texttt{{[}1{]} 300.3}

with an observed difference of \texttt{mean(x2) - mean(x1) = 10.9.}
As expected, the 600 mg measurements seem to have a higher average,
and we might be interested in trying to decide if the average amounts
are \emph{significantly} different. The null hypothesis should be
that there is no difference in the amounts, that is, the groups are
more or less the same. If the null hypothesis were true, then the
two groups would indeed be the same, or just one big group. In that
case, the observed difference in the sample means just reflects the
random assignment into the arbitrary \texttt{x1} and \texttt{x2} categories.
It is now clear how we may resample, consistent with the null hypothesis.

\textbf{Procedure:}
\begin{enumerate}
\item Randomly resample 10 scores from the combined scores of \texttt{x1}
and \texttt{x2}, and assign then to the {}``\texttt{x1}'' group.
The rest will then be in the {}``\texttt{x2}'' group. Calculate
the difference in (re)sampled means, and store that value. 
\item Repeat this procedure many, many times and draw a histogram of the
resampled statistics, called the \emph{permutation distribution}.
Locate the observed difference 10.9 on the histogram to get the $p$-value.
If the $p$-value is small, then we consider that evidence against
the hypothesis that the groups are the same. 
\end{enumerate}
\end{example}
Please see the handout, {}``Two Sample Permutation Tests in \texttt{R}.''
\begin{rem}
In calculating the permutation test $p$-value, the formula is essentially
the proportion of resample statistics that are greater than or equal
to the observed value. Of course, this is merely an \emph{estimate}
of the true $p$-value. As it turns out, an adjustment of $+1$ to
both the numerator and denominator of the proportion improves the
performance of the estimated $p$-value, and this adjustment is implemented
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!ts.perm!\inputencoding{utf8}
function.
\end{rem}
<<>>=
library(coin)
oneway_test(len~supp, data = ToothGrowth)
oneway_test(breaks~wool, data = warpbreaks)
oneway_test(conc~state, data = Puromycin)
oneway_test(rate~state, data = Puromycin)
@


\subsection{Comparison with the Two Sample \emph{t} test}

We know from Chapter BLANK that to tell whether there is an improvement
as a result of taking the intervention class the procedure to use
is the two-sample $t$-test. We use the $t$-test because we assume
a normal underlying population, with unknown variance, and we have
a small sample $n=10$. \textbf{Question:} what does the $t$-test
say? It is easy to do in \textsf{R}; below is the output.

<<>>=
t.test(len ~ supp, data = ToothGrowth, alt = "greater", var.equal = TRUE)
@

The $p$-value for the $t$-test was 0.02848, while for the permutation
test it was 0.02948526. These are actually really close! But they
are not necessarily close, in other situations. Note that there is
an underlying normality assumption for the $t$-test, which isn't
present in the permutation test. If the normality assumption may be
questionable, then the permutation test would be more reasonable.
We see what can happen when using a test in a situation where the
assumptions are not met: smaller $p$-values. In situations where
the normality assumptions are not met, for example, small sample scenarios,
the permutation test is to be preferred. In particular, if accuracy
is very important, then one should use the permutation test.
\begin{rem}
jdslkjds
\begin{itemize}
\item When are Permutation tests valid? While the permutation test doesn't
require normality of the populations (as contrasted with the $t$-test),
nevertheless it still requires that the two groups are exchangeable;
see Section BLANK. In particular, this means that they must be identically
distributed under the null hypothesis. They must have not only the
same means, but they must also have the same spread and shape. This
assumption may or may not be true in a given example, but rarely will
that cause the $t$-test to outperform the permutation test. Why?
Because even if the sample standard deviations are markedly different,
that doesn't mean that the population standard deviations are different.
In many situations the permutation test will also carry over to the
$t$-test. 
\item If the distribution of the groups is close to normal, then the $t$-test
$p$-value and the bootstrap $p$-value will be approximately equal.
If they differ markedly, then this should be considered evidence that
the normality assumptions do not hold. 
\item The generality of the permutation test is such that one can use all
kinds of statistics to compare the two groups. One could compare the
difference in variances, or the difference in (just about anything).
Alternatively, one could compare the ratio of sample means, $\Xbar_{1}/\Xbar_{2}$.
Of course, under the null hypothesis this last quantity should be
near 1. 
\item Just as with the bootstrap, the answer we get is subject to variability
due to the inherent randomness of resampling from the data. We can
make the variability as small as we like by taking sufficiently many
resamples. How many? If the conclusion is very important (that is,
if lots of money is at stake), then take thousands. For point estimation
problems typically, $R=1000$ resamples or so is enough. In general,
if the true $p$-value is $p$ then the standard error of the estimated
$p$-value is $\sqrt{p(1-p)/R}$. You can choose $R$ to get whatever
accuracy desired. 
\end{itemize}
\end{rem}
\begin{itemize}
\item Other possible testing designs:

\begin{itemize}
\item Matched Pairs Designs. 
\item Relationship between two variables. 
\end{itemize}
\end{itemize}

\section{Chapter Exercises}


\chapter{Categorical Data Analysis}

In revision. Coming soon.


\chapter{Nonparametric Statistics}

In revision. Coming soon.


\chapter{Time Series}

In revision. Coming soon.

\appendix

\chapter{Data\label{cha:Data}}

In this chapter we introduce the different data structures that a
statistician is likely to encounter. In each subsection we describe
how to display the data of that particular type. 


\section{Data Structures}


\subsection{Vectors}

Simply speaking, a vector is an ordered sequence of numbers, characters,
or both.


\subsection{Matrices}

Basic function is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!matrix!\inputencoding{utf8}.
You can test with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!is.matrix!\inputencoding{utf8}
and you can coerce with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!as.matrix!\inputencoding{utf8}. 


\subsection{Data Frames}

A data frame is a recrangular array of information with a special
status in R. It is used as the fundamental data structure by most
of the modeling functions in R. The biggest difference between

Basic command is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data.frame!\inputencoding{utf8}.
You can test with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!is.data.frame!\inputencoding{utf8}
and you can coerce with \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!as.data.frame!\inputencoding{utf8}.


\subsection{Lists}


\subsection{Tables}

Suppose you have a contingency table and would like to do descriptive
or inferential statistics on it. The default form of the table is
usually inconvenient to use unless we are working with a function
specially tailored for tables. Here is how to transform your data
to a more manageable form, namely, the raw data used to make the table.

First, we coerce the table to a data frame: 

<<>>=
A <- as.data.frame(Titanic)
head(A)
@

Note that there are as many preliminary columns of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!A!\inputencoding{utf8}
as there are dimensions to the table. The rows of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!A!\inputencoding{utf8}
contain every possible combination of levels from each of the dimensions.
There is also a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Freq!\inputencoding{utf8}
column, which shows how many observations there were at that particular
combination of levels.

The form of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!A!\inputencoding{utf8}
is often sufficient for our purposes, but more often we need to do
more work: we would usually like to repeat each row of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!A!\inputencoding{utf8}
exactly the number of times shown in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Freq!\inputencoding{utf8}
column. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!reshape!\inputencoding{utf8}
package has the function \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!untable!\inputencoding{utf8}
designed for that very purpose: 

<<>>=
library(reshape)
B <- with(A, untable(A, Freq))
head(B)
@

Now, this is more like it. Note that we slipped in a call to the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!with!\inputencoding{utf8}
function, which was done to make the call to \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!untable!\inputencoding{utf8}
more pretty; we could just as easily have done \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!untable(A, A$Freq)!\inputencoding{utf8}. 


\subsection{More about Tables}

Suppose you want to make a table that looks like this:

There are at least two ways to do it.
\begin{itemize}
\item Using a matrix:


<<>>=
tab <- matrix(1:6, nrow = 2, ncol = 3)
rownames(tab) <- c('first', 'second')
colnames(tab) <- c('A', 'B', 'C')
tab  # Counts
@
\begin{itemize}
\item note that the columns are filled in consecutively by default. If you
want to fill the data in by rows then do byrow = TRUE in the matrix
command.
\item the object is a matrix
\end{itemize}
\item Using a dataframe


<<>>=
p <- c("milk","tea")
g <- c("milk","tea")
catgs <- expand.grid(poured = p, guessed = g)
cnts <- c(3, 1, 1, 3)
D <- cbind(catgs, count = cnts)
xtabs(count ~ poured + guessed, data = D)
@
\begin{itemize}
\item again, the data are filled in column-wise.
\item the object is a dataframe
\item if you want to store it as a table then do A <- xtabs(count \textasciitilde{}
poured + guessed, data = D)
\end{itemize}
\end{itemize}

\section{Sources of Data}


\subsection{Data in Packages}

If you would like to see the data sets available in the packages that
are currently loaded into memory, you may do so with the simple command
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!data()!\inputencoding{utf8}.
If you would like to see all of the data sets that are available in
all packages that are installed on your computer (but not necessarily
loaded), you may see them with the command \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!data(package = .packages(all.available = TRUE))!\inputencoding{utf8}

If the name of the data set in a particular package is known, it can
be called with the package argument \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!data(RcmdrTestDrive, package = RcmdrPlugin.IPSUR)!\inputencoding{utf8}


\subsection{Text Files}

These are files that are saved in delimited format.


\subsection{Other Software Files}

There are many occasions on which the data for the study are already
stored in a format from third-party software.


\section{Importing A Data Set}


\subsection{Importing a Data Frame}

The basic command is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!read.table!\inputencoding{utf8}.

There are three methods to get data


\section{Creating New Data Sets}

Using \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!c!\inputencoding{utf8}

Using \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!scan!\inputencoding{utf8}

Using R Commander 


\section{Editing Data Sets}


\subsection{Editing Data Values}


\subsection{Inserting Rows and Columns}


\subsection{Deleting Rows and Columns}


\section{Exporting a Data Set}

The basic function is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!write.table!\inputencoding{utf8}
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!base!\inputencoding{utf8}
package. The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!MASS!\inputencoding{utf8}
package also has a \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!write.matrix!\inputencoding{utf8}
function.


\section{Reshaping a Data Set}

Aggregation

Convert Tables to Data Frames and back

rbind, cbind

ab{[}order(ab{[},1{]}),{]}

complete.cases()

aggregate

stack

\# sorting examples using built-in mtcars dataset

\# sort by mpg newdata <- mtcars{[}order(mpg),{]}

\# sort by mpg and cyl newdata <- mtcars{[}order(mpg, cyl),{]}

\#sort by mpg (ascending) and cyl (descending) newdata <- mtcars{[}order(mpg,
-cyl),{]} 


\section{Chapter Exercises}
\begin{enumerate}
\item Make graphs
\item Make table
\end{enumerate}

\chapter{Mathematical Machinery}

This appendix houses many of the standard definitions and theorems
that are used at some point during the narrative. It is targeted for
someone reading the book who forgets the precise definition of something
and would like a quick reminder of an exact statement. No proofs are
given, and the interested reader should consult a good text on Calculus
(say, Stewart or Apostol), Real Analysis (say, Rudin, Folland, or
Carothers), or Measure Theory (Billingsley, Halmos, Dudley) for details. 


\section{Set Algebra\label{sec:The-Algebra-of}}

We denote sets by capital letters, $A$, $B$, $C$, \emph{etc}. The
letter $S$ is reserved for the sample space, also known as the universe
or universal set, the set which contains all possible elements. The
symbol $\emptyset$ represents the empty set, the set with no elements. 


\subsection*{Set Union, Intersection, and Difference}

Given subsets $A$ and $B$, we may manipulate them in an algebraic
fashion. To this end, we have three set operations at our disposal:
union, intersection, and difference. Below is a table summarizing
the pertinent information about these operations.%
\begin{table}


\begin{centering}
\begin{tabular}{|l|c|l|l|}
\hline 
Name  & Denoted  & Defined by elements  & \textsf{R} syntax\tabularnewline
\hline 
Union  & $A\cup B$  & in $A$ or $B$ or both  & \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!union(A, B)!\inputencoding{utf8}\tabularnewline
Intersection  & $A\cap B$  & in both $A$ and $B$  & \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!intersect(A, B)!\inputencoding{utf8} \tabularnewline
Difference & $A\textbackslash B$  & in $A$ but not in $B$  & \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!setdiff(A, B)!\inputencoding{utf8} \tabularnewline
Complement & $A^{c}$ & in $S$ but not in $A$ & \inputencoding{latin9}\lstinline[basicstyle={\ttfamily},breaklines=true,extendedchars=true,showstringspaces=false,tabsize=2]!setdiff(S, A)!\inputencoding{utf8}\tabularnewline
\hline
\end{tabular}
\par\end{centering}

\caption{Set Operations\label{tab:Set-Operations}}

\end{table}



\subsection*{Identities and Properties}
\begin{enumerate}
\item $A\cup\emptyset=A,\quad A\cap\emptyset=\emptyset$
\item $A\cup S=S,\quad A\cap S=A$
\item $A\cup A^{c}=S$, $A\cap A^{c}=\emptyset$
\item $(A{}^{c})^{c}=A$
\item The Commutative Property: \begin{equation}
A\cup B=B\cup A,\quad A\cap B=B\cap A\end{equation}

\item The Associative Property: \begin{equation}
(A\cup B)\cup C=A\cup(B\cup C),\quad(A\cap B)\cap C=A\cap(B\cap C)\end{equation}

\item The Distributive Property: \begin{equation}
A\cup(B\cap C)=(A\cup B)\cap(A\cup B),\quad A\cap(B\cup C)=(A\cap B)\cup(A\cap B)\end{equation}

\item DeMorgan's Laws\begin{equation}
(A\cup B)^{c}=A^{c}\cap B^{c}\quad\mbox{and}\quad(A\cap B)^{c}=A^{c}\cup B^{c},\end{equation}
or more generally,\begin{equation}
\left(\bigcup_{\alpha}A_{\alpha}\right)^{c}=\bigcap_{\alpha}A_{\alpha}^{c},\quad\mbox{and}\quad\left(\bigcap_{\alpha}A_{\alpha}\right)^{c}=\bigcup_{\alpha}A_{\alpha}^{c}\end{equation}

\end{enumerate}

\section{Differential and Integral Calculus\label{sec:Differential-and-Integral}}


\subsection*{Limits and Continuity}
\begin{defn}
Let $f$ be a function defined on some open interval that contains
the number $a$, except possibly at $a$ itself. Then we say the \emph{limit
of} $f(x)$ \emph{as} $x$ \emph{approaches} $a$ \emph{is} $L$,
and we write \begin{equation}
\lim_{x\to a}f(x)=L,\end{equation}
if for every $\epsilon>0$ there exists a number $\delta>0$ such
that $0<|x-a|<\delta$ implies $|f(x)-L|<\epsilon$.
\end{defn}

\begin{defn}
A function $f$ is \emph{continuous at a number} $a$ if \begin{equation}
\lim_{x\to a}f(x)=f(a).\end{equation}
The function $f$ is \emph{right-continuous at the number} $a$ if
$\lim_{x\to a^{+}}f(x)=f(a)$, and \emph{left-continuous} at $a$
if $\lim_{x\to a^{-}}f(x)=f(a)$. Finally, the function $f$ is \emph{continuous
on an interval} $I$ if it is continuous at every number in the interval. 
\end{defn}

\subsection*{Differentiation}
\begin{defn}
The \emph{derivative of a function} $f$ \emph{at a number} $a$,
denoted by $f'(a)$, is\begin{equation}
f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h},\end{equation}
provided this limit exists.

A function is \emph{differentiable at} $a$ if $f'(a)$ exists. It
is \emph{differentiable on an open interval} $(a,b)$ if it is differentiable
at every number in the interval.
\end{defn}

\subsubsection*{Differentiation Rules}

In the table that follows, $f$ and $g$ are differentiable functions
and $c$ is a constant.

%
\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 &  & \tabularnewline
$\frac{\diff}{\diff x}c=0$ & $\frac{\diff}{\diff x}x^{n}=nx^{n-1}$ & $(cf)'=cf'$\tabularnewline
 &  & \tabularnewline
$(f\pm g)'=f'\pm g'$ & $(fg)'=f'g+fg'$ & $\left(\frac{f}{g}\right)'=\frac{f'g-fg'}{g^{2}}$\tabularnewline
 &  & \tabularnewline
\hline
\end{tabular}
\par\end{centering}

\caption{Differentiation Rules\textbf{\label{tab:Differentiation-Rules}}}

\end{table}

\begin{thm}
Chain Rule: If $f$ and $g$ are both differentiable and $F=f\circ g$
is the composite function defined by $F(x)=f[g(x)]$, then $F$ is
differentiable and $F'(x)=f'[g(x)]\cdot g'(x)$. 
\end{thm}

\subsubsection*{Useful Derivatives}

%
\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 &  & \tabularnewline
$\frac{\diff}{\diff x}\me^{x}=\me^{x}$ & $\frac{\diff}{\diff x}\ln x=x^{-1}$ & $\frac{\diff}{\diff x}\sin x=\cos x$\tabularnewline
 &  & \tabularnewline
$\frac{\diff}{\diff x}\cos x=-\sin x$ & $\frac{\diff}{\diff x}\tan x=\sec^{2}x$ & $\frac{\diff}{\diff x}\tan^{-1}x=(1+x^{2})^{-1}$$ $\tabularnewline
 &  & \tabularnewline
\hline
\end{tabular}
\par\end{centering}

\caption{Some Derivatives\textbf{\label{tab:Useful-Derivatives}}}

\end{table}



\subsection*{Optimization}
\begin{defn}
A \emph{critical number} of the function $f$ is a value $x^{\ast}$
for which $f'(x^{\ast})=0$ or for which $f'(x^{\ast})$ does not
exist.\end{defn}
\begin{thm}
First Derivative Test. If $f$ is differentiable and if $x^{\ast}$
is a critical number of $f$ and if $f'(x)\geq0$ for $x\leq x^{\ast}$
and $f'(x)\leq0$ for $x\geq x^{\ast}$ , then $x^{\ast}$ is a local
maximum of $f$. If $f'(x)\leq0$ for $x\leq x^{\ast}$ and $f'(x)\geq0$
for $x\geq x^{\ast}$ , then $x^{\ast}$ is a local minimum of $f$. 
\end{thm}

\begin{thm}
Second Derivative Test. If $f$ is twice differentiable and if $x^{\ast}$
is a critical number of $f$, then $x^{\ast}$ is a local maximum
of $f$ if $f''(x^{\ast})<0$ and $x^{\ast}$ is a local minimum of
$f$ if $f''(x^{\ast})>0$.
\end{thm}

\subsection*{Integration}

As it turns out, there are all sorts of things called {}``integrals'',
each defined in its own idiosyncratic way. There are \emph{Riemann}
integrals, \emph{Lebesgue} integrals, variants of these called \emph{Stieltjes}
integrals, \emph{Daniell} integrals, \emph{Ito} integrals, and the
list continues. Given that this is an introductory book, we will use
the Riemannian integral with the caveat that the Riemann integral
is \emph{not} the integral that will be used in more advanced study.
\begin{defn}
Let $f$ be defined on $[a,b]$, a closed interval of the real line.
For each $n$, divide $[a,b]$ into subintervals $[x_{i},x_{i+1}]$,
$i=0,1,\ldots,n-1$, of length $\Delta x_{i}=(b-a)/n$ where $x_{0}=a$
and $x_{n}=b$, and let $x_{i}^{\ast}$ be any points chosen from
the respective subintervals. Then the \emph{definite integral} of
$f$ from $a$ to $b$ is defined by\begin{equation}
\int_{a}^{b}f(x)\diff x=\lim_{n\to\infty}\sum_{i=0}^{n-1}f(x_{i}^{\ast})\Delta x_{i},\end{equation}
provided the limit exists, and in that case, we say that $f$ is \emph{integrable}
from $a$ to $b$. \end{defn}
\begin{thm}
The Fundamental Theorem of Calculus. Suppose $f$ is continuous on
$[a,b]$. Then
\begin{enumerate}
\item the function $g$ defined by $g(x)=\int_{a}^{x}f(t)\:\diff t$, $a\leq x\leq b$,
is continuous on $[a,b]$ and differentiable on $(a,b)$ with $g'(x)=f(x)$.
\item $\int_{a}^{b}f(x)\diff x=F(b)-F(a)$, where $F$ is any \emph{antiderivative}
of $f$, that is, any function $F$ satisfying $F'=f$.
\end{enumerate}
\end{thm}

\subsubsection*{Change of Variables}
\begin{thm}
If $g$ is a differentiable function whose range is the interval $[a,b]$
and if both $f$ and $g'$ are continuous on the range of $u=g(x)$,
then\begin{equation}
\int_{g(a)}^{g(b)}f(u)\:\diff u=\int_{a}^{b}f[g(x)]\: g'(x)\:\diff x.\end{equation}

\end{thm}

\subsubsection*{Useful Integrals}

%
\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 &  & \tabularnewline
$\int x^{n}\diff x=x^{n+1}/(n+1),\ n\neq-1$ & $\int\me^{x}\diff x=\me^{x}$ & $\int x^{-1}\diff x=\ln|x|$\tabularnewline
 &  & \tabularnewline
$\int\tan x\:\diff x=\ln|\sec x|$ & $\int a^{x}\diff x=a^{x}/\ln a$ & $\int(x^{2}+1)^{-1}\diff x=\tan^{-1}x$\tabularnewline
 &  & \tabularnewline
\hline
\end{tabular}
\par\end{centering}

\caption{Some Integrals (constants of integration omitted)\textbf{\label{tab:Useful-Integrals}}}

\end{table}



\subsubsection*{Integration by Parts}

\begin{equation}
\int u\:\diff v=uv-\int v\:\diff u\end{equation}

\begin{thm}
L'H\^ opital's Rule. Suppose $f$ and $g$ are differentiable and
$g'(x)\neq0$ near $a$, except possibly at $a$. Suppose that the
limit\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}\end{equation}
is an indeterminate form of type $\frac{0}{0}$ or $\infty/\infty$.
Then\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)},\end{equation}
provided the limit on the right-hand side exists or is infinite.
\end{thm}

\subsubsection*{Improper Integrals}

If $\int_{a}^{t}f(x)\diff x$ exists for every number $t\geq a$,
then we define \begin{equation}
\int_{a}^{\infty}f(x)\diff x=\lim_{t\to\infty}\int_{a}^{t}f(x)\diff x,\end{equation}
provided this limit exists as a finite number, and in that case we
say that $\int_{a}^{\infty}f(x)\diff x$ is \emph{convergent}. Otherwise,
we say that the improper integral is \emph{divergent}.

If $\int_{t}^{b}f(x)\diff x$ exists for every number $t\leq b$,
then we define \begin{equation}
\int_{-\infty}^{b}f(x)\diff x=\lim_{t\to-\infty}\int_{t}^{b}f(x)\diff x,\end{equation}
provided this limit exists as a finite number, and in that case we
say that $\int_{-\infty}^{b}f(x)\diff x$ is \emph{convergent}. Otherwise,
we say that the improper integral is \emph{divergent}.

If both $\int_{a}^{\infty}f(x)\diff x$ and $\int_{-\infty}^{a}f(x)\diff x$
are convergent, then we define\begin{equation}
\int_{-\infty}^{\infty}f(x)\diff x=\int_{-\infty}^{a}f(x)\d x+\int_{a}^{\infty}f(x)\diff x,\end{equation}
and we say that $\int_{-\infty}^{\infty}f(x)\diff x$ is \emph{convergent}.
Otherwise, we say that the improper integral is \emph{divergent}.


\section{Sequences and Series\label{sec:Finite-and-Infinite}}

A \emph{sequence} is an ordered list of numbers, $a_{1}$, $a_{2}$,
$a_{3}$, \ldots{}, $a_{n}$ $=\left(a_{k}\right)_{k=1}^{n}$. A
sequence may be finite or infinite. In the latter case we write $a_{1}$,
$a_{2}$, $a_{3}$, \ldots{}$=\left(a_{k}\right)_{k=1}^{\infty}$.
We say that \emph{the infinite sequence} $\left(a_{k}\right)_{k=1}^{\infty}$
\emph{converges to the finite limit} $L$, and we write \begin{equation}
\lim_{k\to\infty}a_{k}=L,\end{equation}
if for every $\epsilon>0$ there exists an integer $N\geq1$ such
that $|a_{k}-L|<\epsilon$ for all $k\geq N$. We say that \emph{the
infinite sequence} $\left(a_{k}\right)_{k=1}^{\infty}$ \emph{diverges
to} $+\infty$ (or -$\infty$) if for every $M\geq0$ there exists
an integer $N\geq1$ such that $a_{k}\geq M$ for all $k\geq N$ (or
$a_{k}\leq-M$ for all $k\geq N$).


\subsection*{Finite Series}

\begin{equation}
\sum_{k=1}^{n}k=1+2+\cdots+n=\frac{n(n+1)}{2}\end{equation}


\begin{equation}
\sum_{k=1}^{n}k^{2}=1^{2}+2^{2}+\cdots+n^{2}=\frac{n(n+1)(2n+3)}{6}\end{equation}



\subsubsection*{The Binomial Series}


\paragraph*{\begin{equation}
\sum_{k=0}^{n}{n \choose k}a^{n-k}b^{k}=(a+b)^{n}\end{equation}
}


\subsection*{Infinite Series}

Given an infinite sequence of numbers $a_{1}$, $a_{2}$, $a_{3}$,
\ldots{}$=\left(a_{k}\right)_{k=1}^{\infty}$, let $s_{n}$ denote
the \emph{partial sum} of the first $n$ terms:\begin{equation}
s_{n}=\sum_{k=1}^{n}a_{k}=a_{1}+a_{2}+\cdots+a_{n}.\end{equation}
If the sequence $\left(s_{n}\right)_{n=1}^{\infty}$ converges to
a finite number $S$ then we say that the infinite series $\sum_{k}a_{k}$
is \emph{convergent} and write\begin{equation}
\sum_{k=1}^{\infty}a_{k}=S.\end{equation}
Otherwise we say the infinite series is \emph{divergent}.


\subsection*{Rules for Series}

Let $\left(a_{k}\right)_{k=1}^{\infty}$ and $\left(b_{k}\right)_{k=1}^{\infty}$
be infinite sequences and let $c$ be a constant.


\paragraph*{\begin{equation}
\sum_{k=1}^{\infty}ca_{k}=c\sum_{k=1}^{\infty}a_{k}\end{equation}
\begin{equation}
\sum_{k=1}^{\infty}(a_{k}\pm b_{k})=\sum_{k=1}^{\infty}a_{k}\pm\sum_{k=1}^{\infty}b_{k}\end{equation}
}

In both of the above the series on the left is convergent if the series
on the right is (are) convergent.


\subsubsection*{The Geometric Series\begin{equation}
\sum_{k=0}^{\infty}x^{k}=\frac{1}{1-x},\quad|x|<1.\end{equation}
}


\subsubsection*{The Exponential Series\begin{equation}
\sum_{k=0}^{\infty}\frac{x^{k}}{k!}=\me^{x},\quad-\infty<x<\infty.\end{equation}
}


\paragraph*{Other Series\begin{equation}
\sum_{k=0}^{\infty}{m+k-1 \choose m-1}x^{k}=\frac{1}{(1-x)^{m}},\quad|x|<1.\end{equation}
}


\paragraph*{\begin{equation}
-\sum_{k=1}^{\infty}\frac{x^{n}}{n}=\ln(1-x),\quad|x|<1.\end{equation}
}

\begin{equation}
\sum_{k=0}^{\infty}{n \choose k}x^{k}=(1+x)^{n},\quad|x|<1.\end{equation}



\subsection*{Taylor Series}

If the function $f$ has a \emph{power series} representation at the
point $a$ with radius of convergence $R>0$, that is, if\begin{equation}
f(x)=\sum_{k=0}^{\infty}c_{k}(x-a)^{k},\quad|x-a|<R,\end{equation}
for some constants $\left(c_{k}\right)_{k=0}^{\infty}$, then $c_{k}$
must be \begin{equation}
c_{k}=\frac{f^{(k)}(a)}{k!},\quad k=0,1,2,\ldots\end{equation}
Furthermore, the function $f$ is differentiable on the open interval
$(a-R,\, a+R)$ with\begin{equation}
f'(x)=\sum_{k=1}^{\infty}kc_{k}(x-a)^{k-1},\quad|x-a|<R,\end{equation}
\begin{equation}
\int f(x)\,\diff x=C+\sum_{k=0}^{\infty}c_{k}\frac{(x-a)^{k+1}}{k+1},\quad|x-a|<R,\end{equation}
in which case both of the above series have radius of convergence
$R$.


\section{The Gamma Function\label{sec:The-Gamma-Function}}

The \emph{Gamma function} $\Gamma$ will be defined in this book according
to the formula

\begin{equation}
\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}\me^{-x}\:\diff x,\quad\mbox{for }\alpha>0.\end{equation}

\begin{fact}
Properties of the Gamma Function:
\begin{itemize}
\item $\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)$ for any $\alpha>1$,
and so $\Gamma(n)=(n-1)!$ for any positive integer $n$.
\item $\Gamma(1/2)=\sqrt{\pi}$.
\end{itemize}
\end{fact}

\section{Linear Algebra}


\subsection*{Matrices}

A \emph{matrix} is an ordered array of numbers or expressions; typically
we write $\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}$ or $\mathbf{A}=\begin{bmatrix}a_{ij}\end{bmatrix}$.
If $\mathbf{A}$ has $m$ rows and $n$ columns then we write\begin{equation}
\mathbf{A}_{\mathrm{m}\times\mathrm{n}}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.\end{equation}
The \emph{identity matrix} $\mathbf{I}_{\mathrm{n}\times\mathrm{n}}$
is an $\mathrm{n}\times\mathrm{n}$ matrix with zeros everywhere except
for 1's along the main diagonal: \begin{equation}
\mathbf{I}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1\end{bmatrix}.\end{equation}
and the matrix with ones everywhere is denoted $\mathbf{J}_{\mathrm{n}\times\mathrm{n}}$:\begin{equation}
\mathbf{J}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 1 & \cdots & 1\\
1 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
1 & 1 & \cdots & 1\end{bmatrix}.\end{equation}


A \emph{vector} is a matrix with one of the dimensions equal to one,
such as $\mathbf{A}_{\mathrm{m}\times1}$ (a column vector) or $\mathbf{A}_{\mathrm{1}\times\mathrm{n}}$
(a row vector). The \emph{zero vector} $\mathbf{0}_{\mathrm{n}\times1}$
is an $\mathrm{n}\times1$ matrix of zeros:\begin{equation}
\mathbf{0}_{\mathrm{n}\times1}=\begin{bmatrix}0 & 0 & \cdots & 0\end{bmatrix}^{\mathrm{T}}.\end{equation}


The \emph{transpose} of a matrix $\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}$
is the matrix $\mathbf{A}^{\mathrm{T}}=\begin{pmatrix}a_{ji}\end{pmatrix}$,
which is just like $\mathbf{A}$ except the rows are columns and the
columns are rows. The matrix $\mathbf{A}$ is said to be \emph{symmetric}
if $\mathbf{A}^{\mathrm{T}}=\mathbf{A}$. Note that $\left(\mathbf{A}\mathbf{B}\right)^{\mathrm{T}}=\mathbf{B}^{\mathrm{T}}\mathbf{A}^{\mathrm{T}}$.

The \emph{trace} of a square matrix $\mathbf{A}$ is the sum of its
diagonal elements: $\mathrm{tr}(\mathbf{A})=\sum_{i}a_{ii}$.

The \emph{inverse} of a square matrix $\mathbf{A}_{\mathrm{n}\times\mathrm{n}}$
(when it exists) is the unique matrix denoted $\mathbf{A}^{-1}$ which
satisfies $\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}_{\mathrm{n}\times\mathrm{n}}$.
If $\mathbf{A}^{-1}$ exists then we say $\mathbf{A}$ is \emph{invertible},
or alternatively \emph{nonsingular}. Note that $\left(\mathbf{A}^{\mathrm{T}}\right)^{-1}=\left(\mathbf{A}^{\mathrm{-1}}\right)^{\mathrm{T}}$.
\begin{fact}
The inverse of the $2\times2$ matrix\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad\mbox{is}\quad\mathbf{A}^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\
-c & a\end{bmatrix},\end{equation}
provided $ad-bc\neq0$.
\end{fact}

\subsection*{Determinants}
\begin{defn}
The \emph{determinant} of a square matrix $\mathbf{A}_{\mathrm{n}\times n}$
is denoted $\mathrm{det}(\mathbf{A})$ or $|\mathbf{A}|$ and is defined
recursively by\begin{equation}
\mathrm{det}(\mathbf{A})=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}\,\mathrm{det}(\mathbf{M}_{ij}),\end{equation}
where $\mathbf{M}_{ij}$ is the submatrix formed by deleting the $i^{\mathrm{th}}$
row and $j^{\mathrm{th}}$ column of $\mathbf{A}$. We may choose
any fixed $1\leq j\leq n$ we wish to compute the determinant; the
final result is independent of the $j$ chosen.\end{defn}
\begin{fact}
The determinant of the $2\times2$ matrix\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad\mbox{is}\quad|\mathbf{A}|=ad-bc.\end{equation}

\end{fact}

\begin{fact}
A square matrix $\mathbf{A}$ is nonsingular if and only if $\mathrm{det}(\mathbf{A})\neq0$.
\end{fact}

\subsection*{Positive (Semi)Definite}

If the matrix $\mathbf{A}$ satisfies $\mathbf{x^{\mathrm{T}}}\mathbf{A}\mathbf{x}\geq0$
for all vectors $\mathbf{x}\neq\mathbf{0}$, then we say that $\mathbf{A}$
is \emph{positive semidefinite}. If strict inequality holds for all
$\mathbf{x}\neq\mathbf{0}$, then $\mathbf{A}$ is \emph{positive
definite}. The connection to statistics is that covariance matrices
(see Chapter BLANK) are always positive semidefinite, and many of
them are even positive definite.


\section{Multivariate Calculus}


\subsection*{Partial Derivatives}

If $f$ is a function of two variables, its \emph{first-order partial
derivatives} are defined by\begin{equation}
\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}f(x,y)=\lim_{h\to0}\frac{f(x+h,\, y)-f(x,y)}{h}\end{equation}
and\begin{equation}
\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}f(x,y)=\lim_{h\to0}\frac{f(x,\, y+h)-f(x,y)}{h},\end{equation}
provided these limits exist. The \emph{second-order partial derivatives}
of $f$ are defined by\begin{equation}
\frac{\partial^{2}f}{\partial x^{2}}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right),\quad\frac{\partial^{2}f}{\partial y^{2}}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right),\quad\frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right),\quad\frac{\partial^{2}f}{\partial y\partial x}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right).\end{equation}
In many cases (and for all cases in this book) it is true that\begin{equation}
\frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial^{2}f}{\partial y\partial x}.\end{equation}



\subsection*{Optimization}

An function $f$ of two variables has a \emph{local maximum} at $(a,b)$
if $f(x,y)\geq f(a,b)$ for all points $(x,y)$ near $(a,b)$, that
is, for all points in an open disk centered at $(a,b)$. The number
$f(a,b)$ is then called a \emph{local maximum value} of $f$. The
function $f$ has a \emph{local minimum} if the same thing happens
with the inequality reversed.

Suppose the point $(a,b)$ is a \emph{critical point} of $f$, that
is, suppose $(a,b)$ satisfies \begin{equation}
\frac{\partial f}{\partial x}(a,b)=\frac{\partial f}{\partial y}(a,b)=0.\end{equation}
Further suppose $\frac{\partial^{2}f}{\partial x^{2}}$ and $\frac{\partial^{2}f}{\partial y^{2}}$
are continuous near $(a,b)$. Let the \emph{Hessian matrix} $H$ (not
to be confused with the \emph{hat matrix} $\mathbf{H}$ of Chapter
BLANK) be defined by\begin{equation}
H=\begin{bmatrix}\frac{\partial^{2}f}{\partial x^{2}} & \frac{\partial^{2}f}{\partial x\partial y}\\
\frac{\partial^{2}f}{\partial y\partial x} & \frac{\partial^{2}f}{\partial y^{2}}\end{bmatrix}.\end{equation}
We use the following rules to decide whether $(a,b)$ is an \emph{extremum}
(that is, a local minimum or local maximum) of $f$.
\begin{itemize}
\item If $\mbox{det}(H)>0$ and $\frac{\partial^{2}f}{\partial x^{2}}(a,b)>0$,
then $(a,b)$ is a local minimum of $f$.
\item If $\mbox{det}(H)>0$ and $\frac{\partial^{2}f}{\partial x^{2}}(a,b)<0$,
then $(a,b)$ is a local maximum of $f$.
\item If $\mbox{det}(H)<0$, then $(a,b)$ is a \emph{saddle point} of $f$
and so is not an extremum of $f$.
\item If $\mbox{det}(H)=0$, then we do not know the status of $(a,b)$;
it might be an extremum or it might not be.
\end{itemize}

\subsection*{Double and Multiple Integrals}

Let $f$ be defined on a rectangle $R=[a,b]\times[c,d]$, and for
each $m$ and $n$ divide $[a,b]$ (respectively $[c,d]$) into subintervals
$[x_{j},x_{j+1}]$, $i=0,1,\ldots,m-1$ (respectively $[y_{i},y_{i+1}]$)
of length $\Delta x_{j}=(b-a)/m$ (respectively $\Delta y_{i}=(d-c)/n$)
where $x_{0}=a$ and $x_{m}=b$ (and $y_{0}=c$ and $y_{n}=d$ ),
and let $x_{j}^{\ast}$ ($y_{i}^{\ast}$) be any points chosen from
their respective subintervals. Then the \emph{double integral} of
$f$ over the rectangle $R$ is\begin{equation}
\iintop_{R}f(x,y)\,\diff A=\intop_{c}^{d}\!\!\!\intop_{a}^{b}f(x,y)\,\diff x\diff y=\lim_{m,n\to\infty}\sum_{i=1}^{n}\sum_{j=1}^{m}f(x_{j}^{\ast},y_{i}^{\ast})\Delta x_{j}\Delta y_{i},\end{equation}
provided this limit exists. Multiple integrals are defined in the
same way just with more letters and sums.


\subsection*{Bivariate and Multivariate Change of Variables}

Suppose we have a transformation%
\footnote{For our purposes $T$ is in fact the \emph{inverse} of a one-to-one
transformation that we are initially given. We usually start with
functions that map $(x,y)\longmapsto(u,v)$, and one of our first
tasks is to solve for the inverse transformation that maps $(u,v)\longmapsto(x,y)$.
It is this inverse transformation which we are calling $T$.%
} $T$ that maps points $(u,v)$ in a set $A$ to points $(x,y)$ in
a set $B$. We typically write $x=x(u,v)$ and $y=y(u,v)$, and we
assume that $x$ and $y$ have continuous first-order partial derivatives.
We say that $T$ is \emph{one-to-one} if no two distinct $(u,v)$
pairs get mapped to the same $(x,y)$ pair; in this book, all of our
multivariate transformations $T$ are one-to-one.

The \emph{Jacobian} (pronounced {}``yah-KOH-bee-uhn'') of $T$ is
denoted by $\partial(x,y)/\partial(u,v)$ and is defined by the determinant
of the following matrix of partial derivatives:\begin{equation}
\frac{\partial(x,y)}{\partial(u,v)}=\left|\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}\end{array}\right|=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}.\end{equation}
If the function $f$ is continuous on $A$ and if the Jacobian of
$T$ is nonzero except perhaps on the boundary of $A$, then \begin{equation}
\iint_{B}f(x,y)\,\diff x\,\diff y=\iint_{A}f\left[x(u,v),\, y(u,v)\right]\ \left|\frac{\partial(x,y)}{\partial(u,v)}\right|\diff u\,\diff v.\end{equation}
A multivariate change of variables is defined in an analogous way:
the one-to-one transformation $T$ maps points $(u_{1},u_{2},\ldots,u_{n})$
to points $(x_{1},x_{2},\ldots,x_{n})$, the Jacobian is the determinant
of the $\mathrm{n}\times\mathrm{n}$ matrix of first-order partial
derivatives of $T$ (lined up in the natural manner), and instead
of a double integral we have a multiple integral over multidimensional
sets $A$ and $B$.


\chapter{Writing Reports with \textsf{R}}

Perhaps the most important part of a statistician's job once the analysis
is complete is to communicate the results to others. This is usually
done with some type of report that is delivered to the client, manager,
or administrator. Other situations that call for reports include term
papers, final projects, thesis work, \emph{etc}. This chapter is designed
to pass along some tips about writing reports once the work is completed
with R.


\section{What to Write}

It is possible to summarize this entire appendix with only one sentence:
\emph{the statistician's goal is to communicate with others}. To this
end, there are some general guidelines that I give to students which
are based on an outline originally written and shared with me by Dr.~G.~Andy
Chang.


\subsection*{Basic Outline for a Statistical Report}
\begin{enumerate}
\item Executive Summary (a one page description of the study and conclusion) 
\item Introduction 

\begin{enumerate}
\item What is the question, and why is it important?
\item Is the study observational or experimental?
\item What are the hypotheses of interest to the researcher?
\item What are the types of analyses employed? (one sample $t$-test, paired-sample
$t$-test, ANOVA, chi-square test, regression, \ldots{}) 
\end{enumerate}
\item Data Collection 

\begin{enumerate}
\item Describe how the data were collected in detail.
\item Identify all variable types: quantitative, qualitative, ordered or
nominal (with levels), discrete, continuous.
\item Discuss any limitations of the data collection procedure. Look carefully
for any sources of bias.
\end{enumerate}
\item Summary Information 

\begin{enumerate}
\item Give numeric summaries of all variables of interest.

\begin{enumerate}
\item Discrete: (relative) frequencies, contingency tables, odds ratios,
\emph{etc}. 
\item Continuous: measures of center, spread, shape.
\end{enumerate}
\item Give visual summaries of all variables of interest.

\begin{enumerate}
\item Side-by-side boxplots, scatterplots, histograms, \emph{etc}.
\end{enumerate}
\item Discuss any unusual features of the data (outliers, clusters, granularity,
\emph{etc}.)
\item Report any missing data and identify any potential problems or bias.
\end{enumerate}
\item Analysis 

\begin{enumerate}
\item State any hypotheses employed, and check the assumptions. 
\item Report test statistics, \emph{p}-values, and confidence intervals. 
\item Interpret the results in the context of the study.
\item Attach (labeled) tables and/or graphs and make reference to them in
the report as needed. 
\end{enumerate}
\item Conclusion

\begin{enumerate}
\item Summarize the results of the study. What did you learn? 
\item Discuss any limitations of the study or inferences.
\item Discuss avenues of future research suggested by the study. 
\end{enumerate}
\end{enumerate}

\section{How to Write It with \textsf{R}}

Once the decision has been made what to write, the next task is to
typeset the information to be shared. To do this the author will need
to select software to use to write the documents. There are many options
available, and choosing one over another is sometimes a matter of
taste. But not all software were created equal, and \textsf{R} plays
better with some applications than it does with others.

In short, \textsf{R} does great with \LaTeX{} and there are many resources
available to make writing a document with \textsf{R} and \LaTeX{}
easier. But \LaTeX{} is not for the beginner, and there are other
word processors which may be acceptable depending on the circumstances.


\subsection{Microsoft$\circledR$ Word}

It is fact of life that Microsoft$\circledR$ Windows is currently
the most prevalent desktop operating system in the world. Those who
own Windows also typically own some version of Microsoft Office, thus
Microsoft Word is the default word processor for many, many people. 

The standard way to write an \textsf{R} report with Microsoft$\circledR$
Word is to generate material with \textsf{R} and then copy-paste the
material at selected places in a Word document. The advantage to this
approach is that Word is nicely designed to make it easy to copy-and-paste
from the \textsf{R} console to the Word document.

A disadvantage to this approach is that is does not work on all operating
systems (not on Linux, in particular). Another disadvantage is that
Microsoft$\circledR$ Word is proprietary; as a result, \textsf{R}
does not communicate with Microsoft$\circledR$ Word as well as it
does with other software.

Nevertheless, if you are going to write a report with Word there are
some steps that you can take to make the report more amenable to the
reader.
\begin{enumerate}
\item Copy and paste graphs into the document. You can do this by right
clicking on the graph and selecting \textsf{Copy as bitmap,} or \textsf{Copy
as metafile}, or one of the other options. Then move the cursor to
the document where you want the picture, right-click, and select \textsf{Paste}.
\item Resize (most) pictures so that they take up no more than 1/2 page.
You may want to put graphs side by side; do this by inserting a table
and placing the graphs inside the cells.
\item Copy selected \textsf{R} input and output to the MS-Word document.
All code should be separated from the rest of the writing, except
when specifically mentioning a function or object in a sentence.
\item The font of \textsf{R} input/output should be Courier New, or some
other monowidth font (not Times New Roman or Calibri); the default
font size of \texttt{12pt} is usually too big and should be reduced
to, for example, \texttt{10pt}.
\end{enumerate}
It is also possible to communicate with \textsf{R} through OpenOffice.org,
which can export to the proprietary (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.doc!\inputencoding{utf8})
format.


\subsection{OpenOffice.org and \texttt{odfWeave}}

OpenOffice.org (OO.o) is an open source desktop productivity suite
which mirrors Microsoft$\circledR$ Office. It is especially nice
because it works on all operating systems. OO.o can read most document
formats, and in particular, it will read \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.doc!\inputencoding{utf8}
files. The standard OO.o file extension for documents is \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.odt!\inputencoding{utf8},
which stands for {}``open document text''.

The \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!odfWeave!\inputencoding{utf8}
package provides a way to generate an \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.odt!\inputencoding{utf8}
file with \textsf{R} input and output code automatically formatted
correctly and inserted in the correct places. In this way, one does
not need to worry about all of the trouble of typesetting \textsf{R}
output. Another advantage of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!odfWeave!\inputencoding{utf8}
is that it allows you to generate the report dynamically; if the data
underlying the report change or are updated, then a few clicks (or
commands) will generate a brand new report.

One disadvantage is that the source \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.odt!\inputencoding{utf8}
file is not easy to read, because it is difficult to visually distinguish
the noweb parts (where the \textsf{R} code is) from the non-noweb
parts. This can be fixed by manually changing the font of the noweb
sections to, for instance, Courier font, size \texttt{10pt}. But it
is extra work. It would be nice if a program would discriminate between
the two different sections and automatically typeset the respective
parts in their correct fonts. This one of the advantages to \LyX{}.

Another advantage of OO.o is that even after you have generated the
outfile, it is fully editable just like any other \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.odt!\inputencoding{utf8}
document. If there are errors or formatting problems, they can be
fixed at any time.

Here are the basic steps to typeset a statistical report with OO.o.
\begin{enumerate}
\item Write your report as an \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.odt!\inputencoding{utf8}
document in OO.o just as you would any other document. Call this document
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!infile.odt!\inputencoding{utf8},
and make sure that it is saved in your working directory (see Section
BLANK).
\item At the places you would like to insert \textsf{R} code in the document,
write the code chunks in the following format:


\texttt{<\textcompwordmark{}<>\textcompwordmark{}>=}~\\
\texttt{x <- rnorm(10)}~\\
\texttt{mean(x)}~\\
\texttt{@}

or write whatever code you want between the symbols \texttt{<\textcompwordmark{}<>\textcompwordmark{}>=}
and \texttt{@}.

\item Open \textsf{R} and type the following:


<<eval = FALSE>>=
library(odfWeave)
odfWeave(file = "infile.odt", dest = "outfile.odt")
@ 

\item The compiled (\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!.odt!\inputencoding{utf8})
file, complete with all of the \textsf{R} output automatically inserted
in the correct places, will now be the file \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!outfile.odt!\inputencoding{utf8}
located in the working directory. Open \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!outfile.odt!\inputencoding{utf8},
examine it, modify it, and repeat if desired.
\end{enumerate}
There are all sorts of extra things that can be done. For example,
the \textsf{R} commands can be suppressed with the tag \texttt{<\textcompwordmark{}<echo
= FALSE>\textcompwordmark{}>=}, and the \textsf{R} output may be hidden
with \texttt{<\textcompwordmark{}<results = hide>\textcompwordmark{}>=}.
See the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!odfWeave!\inputencoding{utf8}
package documentation for details.


\subsection{Sweave and \protect\LaTeX{}}

This approach is nice because it works for all operating systems.

One can quite literally typeset \emph{anything} with \LaTeX{}. All
of this power comes at a price, however. The writer must learn the
\LaTeX{} language which is a nontrivial enterprise. Even given the
language, if there is a single syntax error, or a single delimeter
missing in the entire document, then the whole thing breaks.

\LaTeX{} can do anything, but it is relatively difficult to learn
and very grumpy about syntax errors and delimiter matching. there
are however programs useful for formatting \LaTeX{}.

A disadvantage is that you cannot see the mathematical formulas until
you run the whole file with \LaTeX{}.

A disadvantage is that figures and tables are relatively difficult.

There are programs to make the process easier AUC\TeX{}

dev.copy2eps, also dev.copy2pdf

\url{http://www.stat.uni-muenchen.de/~leisch/Sweave/}


\subsection{Sweave and \protect\LyX{}}

This approach is nice because it works for all operating systems.
It gives you everything from the last section and makes it easier
to use \LaTeX{}. That being said, it is better to know \LaTeX{} already
when migrating to \LyX{}, because you understand all of the machinery
going on under the hood.

Program Listings and the \textsf{R} language

This book was written with \LyX{}.

You can see the 

\url{http://gregor.gorjanc.googlepages.com/lyx-sweave}


\section{Formatting Tables}

prettyR the Hmisc library

<<>>=
library(Hmisc)
summary(cbind(Sepal.Length, Sepal.Width) ~ Species, data = iris)
@

There is a method argument to summary, which is set to method = {}``response''
by default. There are two other methods for summarizing data: reverse
and cross. See ?summary.formula or the following document from Frank
Harrell for more details \url{http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatReport}.


\section{Other Formats}

HTML and prettyR

R2HTML


\section{DO's}


\subsection{Mathematical Typesetting}

Given that you are a student in the Department of Mathematics \& Statistics,
the probability is high that you will want to include mathematical
notation and formulas in your report, and they are entered into \LyX{}
using a special \LaTeX{} math mode. There are three primary ways to
do this. 

The first way is called an {}``inline formula'', which means that
the formula is included in the text with everything else. An example
would be $f(x)$ or $\int\sin x\ dx$. This way is handy when mentioning
variables or short expressions.

The second way is called a {}``displayed formula'', which is separated
from the rest of the text in its own displayed paragraph. An example
would be\[
f(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e^{-x^{2}/2},\quad-\infty<x<\infty},\]
which is useful for longer formulas or equations. 

The last way is a {}``numbered formula'', which displays the formula
labeled with a number, for instance,\begin{equation}
\mathrm{e}^{i\pi}-1=0.\end{equation}
There can be many of these in a the document, and the equation numbers
will be generated automatically by \LyX{}. 

Please note that there are many, many, many things that can be done
with \LaTeX{} and mathematics. To get an idea, take a look at {}``\LyX{}'s
detailed Math Manual'', which can be viewed by clicking \emph{Help}
$\rightarrow$\emph{ Math}. 

In particular, all variables, functions, and expressions in the document
should be written in math mode. It is not acceptable to write X or
Y when discussing variables in your report\ldots{} they should instead
be $X$ and $Y$ so that the reader can easily distinguish between
mathematics and text.


\chapter{Instructions for Instructors}

<<echo = FALSE>>=
set.seed(095259)
@

Probably this \emph{book} could more accurately be described as \emph{software}.
The reason is that the document is one big random variable, one observation
realized out of millions. It is electronically distributed under the
GNU FDL, and {}``free'' in both senses: speech and beer. 

There are four components to \IPSUR: the Document, the Program used
to generate it, the \textsf{R} package that holds the Program, and
the Ancillaries that accompany it.

The majority of the data and exercises have been designed to be randomly
generated. Different realizations of this book will have different
graphs and exercises throughout. The advantage of this approach is
that a teacher, say, can generate a unique version to be used in his/her
class. Students can do the exercises and the teacher will have the
answers to all of the problems in their own, unique solutions manual.
Students may download a different solutions manual online somewhere
else, but none of the answers will match the teacher's copy. 

Then next semester, the teacher can generate a \emph{new} book and
the problems will be more or less identical, except the numbers will
be changed. This means that students from different sections of the
same class will not be able to copy from one another quite so easily.
The same will be true for similar classes at different institutions.
Indeed, as long as the instructor protects his/her \emph{key} used
to generate the book, it will be difficult for students to crack the
code. And if they are industrious enough at this level to find a way
to (a) download and decipher my version's source code, (b) hack the
teacher's password somehow, and (c) generate the teacher's book with
all of the answers, then they probably should be testing out of an
{}``Introduction to Probability and Statistics'' course, anyway. 

The book that you are reading was created with a random seed which
was set at the beginning. The original seed is 42. You can choose
your own seed, and generate a new book with brand new data for the
text and exercises, complete with updated manuals. A method I recommend
for finding a seed is to look down at your watch at this very moment
and record the 6 digit hour, minute, and second (say, 9:52:59am):
choose that for a seed%
\footnote{In fact, this is essentially the method used by \textsf{R} to select
an initial random seed (see ?set.seed). However, the instructor should
set the seed manually so that the book can be regenerated at a later
time, if necessary.%
}. This method already provides for over 43,000 books, without taking
military time into account. An alternative would be to go to \textsf{R}
and type 

<<>>=
options(digits = 16)
runif(1)
@

Now choose \Sexpr{set.seed(095259); options(digits = 16); runif(1)*10^16}
as your secret seed\ldots{} write it down in a safe place and do
not share it with anyone. Next generate the book with your seed using
\LyX{}-Sweave or Sweave-\LaTeX{}. You may wish to also generate Student
and Instructor Solution Manuals. Guidance regarding this is given
below in the How to Use This Document section.


\section{Generating This Document}

You will need three (3) things to generate this document for yourself,
in addition to a current \textsf{R} distribution which at the time
of this writing is \texttt{\Sexpr{version$version.string}:}
\begin{enumerate}
\item a \LaTeX{} distribution,
\item Sweave (which comes with \textsf{R} automatically), and 
\item \LyX{} (optional, but recommended).
\end{enumerate}
We will discuss each of these in turn.
\begin{description}
\item [{\LaTeX{}:}] The distribution used by the present author was \TeX{}
Live (\url{http://www.tug.org/texlive/}). There are plenty of other
perfectly suitable \LaTeX{} distributions depending on your operating
system, one such alternative being Mik\TeX{} (\url{http://miktex.org/})
for Microsoft Windows.
\item [{Sweave:}] If you have \textsf{R} installed, then the required Sweave
files are already on your system\ldots{} somewhere. The only problems
that you may have are likely associated with making sure that your
\LaTeX{} distribution knows where to find the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Sweave.sty!\inputencoding{utf8}
file. See the Sweave Homepage (\url{http://www.statistik.lmu.de/~leisch/Sweave/})
for guidance on how to get it working on your particular operating
system.
\item [{\LyX{}:}] Strictly speaking, \LyX{} is not needed to generate this
document. But this document was written stem to stern with \LyX{},
taking full advantage of all of the bells and whistles that \LyX{}
has to offer over plain \LaTeX{} editors. And it's free. See the \LyX{}
homepage (\url{http://www.lyx.org/}) for additional information.\\
If you decide to give \LyX{} a try, then you will need to complete
some extra steps to coordinate Sweave and \LyX{} with each other.
Luckily, Gregor Gorjanc has a website and an R-News article BLANK
to help you do exactly that. See the \LyX{}-Sweave Homepage (\url{http://gregor.gorjanc.googlepages.com/lyx-sweave})
for details.
\end{description}
An attempt was made to not be extravagant with fonts or packages so
that a person would not need the entire \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CTAN!\inputencoding{utf8}
(or \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8})
installed on their personal computer to generate the book. Nevertheless,
there are a few extra packages required. These packages are listed
in the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!preamble!\inputencoding{utf8}
of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.Rnw!\inputencoding{utf8},
\inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.tex!\inputencoding{utf8},
and \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR.lyx!\inputencoding{utf8}.


\section{How to Use This Document}

The easiest way to use this document is to install the \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!IPSUR!\inputencoding{utf8}
package from \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8}
and be all done. This way would be acceptable if there is another,
primary, text being used for the course and \IPSUR\ is only meant
to play a supplementary role.

If you plan for \IPSUR\ to serve as the primary text for your course,
then it would be wise to generate your own version of the document.
You will need the source code for the Program which can be downloaded
from \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!CRAN!\inputencoding{utf8}
or the \IPSUR\ website. Once the source is obtained there are four
(4) basic steps to generating your own copy.
\begin{enumerate}
\item Randomly select a secret {}``seed'' of integers and replace my seed
of 42 with your own seed.
\item Make sure that the \textsf{\textbf{maintext}} branch is turned \textsf{ON}
and also make sure that both the \textsf{\textbf{solutions}} branch
and the \textsf{\textbf{answers}} branch are turned \textsf{OFF}.
Use \LyX{} or your \LaTeX{} editor with Sweave to generate your unique
PDF copy of the book and distribute this copy to your students. (See
the \LyX{} User's Guide to learn more about branches; the ones referenced
above can be found under \textsf{Document $\triangleright$ Settings
$\triangleright$ Branches}.)
\item Turn the \textsf{\textbf{maintext}} branch\textsf{}%
\footnote{You can leave the \textsf{\textbf{maintext}} branch \textsf{ON} when
generating the solutions manuals, but (1) all of the page numbers
will be different, and (2) the typeset solutions will generate and
take up a lot of space between exercises.%
} \textsf{OFF} and the \textsf{\textbf{solutions}} branch \textsf{ON}.
Generate a {}``Student Solutions Manual'' which has complete solutions
to selected exercises and distribute the PDF to the students.
\item Leave the \textsf{\textbf{solutions}} branch \textsf{ON} and also
turn the \textsf{\textbf{answers}} branch \textsf{ON} and generate
an {}``Instructor Solutions and Answers Manual'' with full solutions
to some of the exercises and just answers to the remaining exercises.
Do NOT distribute this to the students -- unless of course you want
them to have the answers to all of the problems. 
\end{enumerate}
To make it easier for those people who do not want to use \LyX{} (or
for whatever reason cannot get it working), I have included three
(3) Sweave files corresponding to the main text, student solutions,
and instructor answers, that are included in the \IPSUR\ source package
in the \texttt{/tex} subdirectory. In principle it is possible to
change the seed and generate the three parts separately with only
Sweave and \LaTeX{}. This method is not recommended by me, but is
perhaps desirable for some people.


\paragraph*{Generating Quizzes and Exams}
\begin{itemize}
\item you can copy paste selected exercises from the text, put them together,
and you have a quiz. Since the numbers are randomly generated you
do not need to worry about different semesters. And you will have
answer keys already for all of your QUIZZES and EXAMS, too.
\end{itemize}

\section{Ancillary Materials}

In addition to the main text, student manual, and instructor manual,
there are two oth IPSUR.R


\section{Modifying This Document}

Since this document is released under the GNU-FDL, you are free to
modify this document however you wish (in accordance with the license
-- see Appendix BLANK). The immediate benefit of this is that you
can generate the book, with brand new problem sets, and distribute
it to your students simply as a PDF (in an email, for instance). As
long as you distribute less than 100 such \emph{Opaque} copies, you
are not even required by the GNU-FDL to share your \emph{Transparent}
copy (the source code with the secret key) that you used to generate
them. Next semester, choose a new key and generate a new copy to be
distributed to the new class.
\begin{quotation}
But more generally, if you are not keen on the way I explained (or
failed to explain) something, then you are \underbar{free} to rewrite
it. If you would like to cover more (or less) material, then you are
\underbar{free} to add (or delete) whatever Chapters/Sections/Paragraphs
that you wish. And since you have the source code, you do not need
to retype the wheel.
\end{quotation}
Some individuals will argue that the nature of a statistics textbook
like this one, many of the exercises being randomly generated \emph{by
design}, does a disservice to the students because the exercises do
not use real-world data. That is a valid criticism\ldots{} but in
my case the benefits outweighed the detriments and I moved forward
to incorporate static data sets whenever it was feasible and effective.
Frankly, and most humbly, the only response I have for those individuals
is: {}``Please refer to the preceding paragraph.''


\chapter{\texttt{RcmdrTestDrive} Story}

The goal of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrTestDrive!\inputencoding{utf8}
was to have a data set sufficiently rich in the types of data represented
such that a person could load it into the \textsf{R} Commander and
be able to explore all of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}'s
menu options at once. I decided early-on that an efficient way to
do this would be to generate the data set randomly, and later add
to the list of variables as more \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!Rcmdr!\inputencoding{utf8}
menu options became available. Generating the data was easy, but generating
a \emph{story} that related all of the respective variables proved
to be less so.

In the Summer of 2006 I gave a version of the raw data and variable
names to my STAT 3743 Probability and Statistics class and invited
each of them to write a short story linking all of the variables together
in a coherent narrative. No further direction was given.

The most colorful of those I received was written by Jeffery Cornfield,
submitted July 12, 2006, and is included below with his permission.
It was edited slightly by the present author and updated to respond
dynamically to the random generation of \inputencoding{latin9}\lstinline[basicstyle={\ttfamily}]!RcmdrTestDrive!\inputencoding{utf8};
otherwise, the story has been unchanged. 


\section*{CaseFile: ALU-179 {}``Murder Madness in Toon Town}
\begin{quotation}
\noindent \begin{center}
{*}{*}{*}WARNING{*}{*}{*} 
\par\end{center}

\noindent {*}{*}{*}This file is not for the faint of heart, dear reader,
because it is filled with horrible images that will haunt your nightmares.
If you are weak of stomach, have irritable bowel syndrome, or are
simply paranoid, DO NOT READ FURTHER! Otherwise, read at your own
risk.{*}{*}{*}
\end{quotation}
One fine sunny day, Police Chief R.~Runner called up the forensics
department at Acme-Looney University. There had been \Sexpr{dim(RcmdrTestDrive)[1]-2}
murders in the past \Sexpr{round((dim(RcmdrTestDrive)[1]-2)/24)}
days, approximately one murder every hour, of many of the local Human
workers, shop keepers, and residents of Toon Town. These alarming
rates threatened to destroy the fragile balance of Toon and Human
camaraderie that had developed in Toon Town. 

Professor Twee T.~Bird, a world-renowned forensics specialist and
a Czechoslovakian native, received the call. Professor, we need your
expertise in this field to identify the pattern of the killer or killers,
Chief Runner exclaimed. We need to establish a link between these
people to stop this massacre.

Yes, Chief Runner, please give me the details of the case, Professor
Bird declared with a heavy native accent, (though, for the sake of
the case file, reader, I have decided to leave out the accent due
to the fact that it would obviously drive you -- if you will forgive
the pun -- looney!)

All prints are wiped clean and there are no identifiable marks on
the bodies of the victims. All we are able to come up with is the
possibility that perhaps there is some kind of alternative method
of which we are unaware. We have sent a secure e-mail with a listing
of all of the victims \textbf{races}, \textbf{genders}, locations
of the bodies, and the sequential \textbf{order} in which they were
killed. We have also included other information that might be helpful,
said Chief Runner.

Thank you very much. Perhaps I will contact my colleague in the Statistics
Department here, Dr.~Elmer Fudd-Einstein, exclaimed Professor Bird.
He might be able to identify a pattern of attack with mathematics
and statistics. 

Good luck trying to find him, Professor. Last I heard, he had a bottle
of scotch and was in the Hundred Acre Woods hunting rabbits, Chief
Runner declared in a manner that questioned the beloved doctors credibility.

Perhaps I will take a drive to find him. The fresh air will do me
good.
\begin{quotation}
\noindent {*}{*}{*}I will skip ahead, dear reader, for much occurred
during this time. Needless to say, after a fierce battle with a mountain
cat that the Toon-ology Department tagged earlier in the year as Sylvester,
Professor Bird found Dr.~Fudd-Einstein and brought him back, with
much bribery of alcohol and the promise of the future slaying of those
wascally wabbits (it would help to explain that Dr.~Fudd-Einstein
had a speech impediment which was only worsened during the consumption
of alcohol.){*}{*}{*}
\end{quotation}
Once our two heroes returned to the beautiful Acme-Looney University,
and once Dr.~Fudd-Einstein became sober and coherent, they set off
to examine the case and begin solving these mysterious murders. 

First off, Dr.~Fudd-Einstein explained, these people all worked
at the University at some point or another. Also, there also seems
to be a trend in the fact that they all had a \textbf{salary} between
\$\Sexpr{round(min(RcmdrTestDrive$salary))} and \$\Sexpr{round(max(RcmdrTestDrive$salary))}
when they retired. 

Thats not really a lot to live off of, explained Professor Bird. 

Yes, but you forget that the Looney Currency System works differently
than the rest of the American Currency System. One Looney is equivalent
to Ten American Dollars. Also, these faculty members are the ones
who faced a cut in their salary, as denoted by \textbf{reduction}.
Some of them dropped quite substantially when the University had to
fix that little \emph{faux pas} in the Chemistry Department. You remember:
when Dr.~D.~Duck tried to create that Everlasting Elixir? As a
result, these faculty left the university. Speaking of which, when
is his memorial service? inquired Dr.~Fudd-Einstein. 

This coming Monday. But if there were all of these killings, how
in the world could one person do it? It just doesnt seem to be possible;
stay up \Sexpr{round((dim(RcmdrTestDrive)[1]-2)/24)} days straight
and be able to kill all of these people and have the energy to continue
on, Professor Bird exclaimed, doubting the guilt of only one person.

Perhaps then, it was a group of people, perhaps there was more than
one killer placed throughout Toon Town to commit these crimes. If
I feed in these variables, along with any others that might have a
pattern, the Acme Computer will give us an accurate reading of suspects,
with a scant probability of error. As you know, the Acme Computer
was developed entirely in house here at Acme-Looney University, Dr.~Fudd-Einstein
said as he began feeding the numbers into the massive server. 

Hey, look at this, Professor Bird exclaimed, Whats with this \textbf{before}/\textbf{after}
information? 

Scroll down; it shows it as a note from the coroners office. Apparently
Toon Town Coroner Marvin -- that strange fellow from Mars, Pennsylvania
-- feels, in his opinion, that given the fact that the cadavers were
either \textbf{smoke}rs or non-smokers, and given their personal health,
and family medical history, that this was their life expectancy before
contact with cigarettes or second-hand smoke and after, Dr.~Fudd-Einstein
declared matter-of-factly. 

Well, would race or gender have something to do with it, Elmer?
inquired Professor Bird.

Maybe, but I would bet my money on somebody was trying to quiet these
faculty before they made a big ruckus about the secret money-laundering
of Old Man Acme. You know, most people think that is how the University
receives most of its funds, through the mob families out of Chicago.
And I would be willing to bet that these faculty figured out the connection
and were ready to tell the Looney Police. Dr.~Fudd-Einstein spoke
lower, fearing that somebody would overhear their conversation.

Dr.~Fudd-Einstein then pressed \textsf{Enter} on the keyboard and
waited for the results. The massive computer roared to life\ldots{}
and when I say roared, I mean it literally \emph{roared}. All the
hidden bells, whistles, and alarm clocks in its secret compartments
came out and created such a loud racket that classes across the university
had to come to a stand-still until it finished computing. 

Once it was completed, the computer listed 4 names:
\begin{quotation}
{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}SUSPECTS{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*} 
\begin{description}
\item [{Yosemite}] Sam (Looney Insane Asylum) 
\item [{Wile~E.~Coyote}] (deceased) 
\item [{Foghorn~Leghorn}] (whereabouts unknown) 
\item [{Granny}] (1313 Mockingbird Lane, Toon Town USA)
\end{description}
\end{quotation}
Dr.~Fudd-Einstein and Professor Bird looked on in silence. They could
not believe their eyes. The greatest computer on the Gulf of Mexico
seaboard just released the most obscure results imaginable.

There seems to be a mistake. Perhaps something is off, Professor
Bird asked, still unable to believe the results.

Not possible; the Acme Computer takes into account every kind of
connection available. It considers affiliations to groups, and affiliations
those groups have to other groups. It checks the FBI, CIA, British
intelligence, NAACP, AARP, NSA, JAG, TWA, EPA, FDA, USWA, R, MAPLE,
SPSS, SAS, and Ben \& Jerrys files to identify possible links, creating
the most powerful computer in the world\ldots{} with a tweak of Toon
fanaticism, Dr.~Fudd-Einstein proclaimed, being a proud co-founder
of the Acme Computer Technology.

Wait a minute, Ben \& Jerry? What would eating ice cream have to
do with anything? Professor Bird inquired.

It is in the works now, but a few of my fellow statistician colleagues
are trying to find a mathematical model to link the type of ice cream
consumed to the type of person they might become. Assassins always
ate vanilla with chocolate sprinkles, a little known fact they would
tell you about Oswald and Booth, Dr.~Fudd-Einstein declared.

Ive heard about this. My forensics graduate students are trying
to identify car thieves with either rocky road or mint chocolate chip
so far, the pattern is showing a clear trend with chocolate chip,
Professor Bird declared. 

Well, what do we know about these suspects, Twee? Dr.~Fudd-Einstein
asked.

Yosemite Sam was locked up after trying to rob that bank in the West
Borough. Apparently his guns were switched and he was sent the Acme
Kids Joke Gun and they blew up in his face. The containers of peroxide
they contained turned all of his facial hair red. Some little child
is running around Toon Town with a pair of .38s to this day.

Wile E.~Coyote was that psychopath working for the Yahtzee - the
fanatics who believed that Toons were superior to Humans. He strapped
sticks of Acme Dynamite to his chest to be a martyr for the cause,
but before he got to the middle of Toon Town, this defective TNT blew
him up. Not a single other person -- Toon or Human -- was even close.

Foghorn Leghorn is the most infamous Dog Kidnapper of all times.
He goes to the homes of prominent Dog citizens and holds one of their
relatives for ransom. If they refuse to pay, he sends them to the
pound. Either way, theyre sure stuck in the dog house, Professor
Bird laughed. Dr.~Fudd-Einstein didnt seem amused, so Professor
Bird continued. 

Granny is the most beloved alumnus of Acme-Looney University. She
was in the first graduating class and gives graciously each year to
the university. Without her continued financial support, we wouldnt
have the jobs we do. She worked as a parking attendant at the University
lots\ldots{} wait a minute, take a look at this, Professor Bird
said as he scrolled down in the police information. Grannys signature
is on each of these faculty members \textbf{parking} tickets. Kind
of odd, considering the Chief-of-Parking signed each personally. The
deceased had from as few as \Sexpr{min(RcmdrTestDrive$parking)} ticket
to as many as \Sexpr{max(RcmdrTestDrive$parking)}. All tickets were
unpaid.

And look at this, Granny married Old Man Acme after graduation. He
was a resident of Chicago and rumored to be a consigliere to one of
the most prominent crime families in Chicago, the Chuck Jones/Warner
Crime Family, Professor Bird read from the screen as a cold feeling
of terror rose from the pit of his stomach.

Say, dont you live at her house? Wow, youre living under the same
roof as one of the greatest criminals/murderers of all time! Dr.~Fudd-Einstein
said in awe and sarcasm.

I would never have suspected her, but I guess it makes sense. She
is older, so she doesnt need near the amount of sleep as a younger
person. She has access to all of the vehicles so she can copy license
plate numbers and follow them to their houses. She has the finances
to pay for this kind of massive campaign on behalf of the Mob, and
she hates anyone that even remotely smells like smoke, Professor
Bird explained, wishing to have his hit of nicotine at this time.

Well, I guess there is nothing left to do but to call Police Chief
Runner and have him arrest her, Dr.~Fudd-Einstein explained as he
began dialing. What I cant understand is how in the world the Police
Chief sent me all of this information ann acceptabled seemed to screw
it up.

What do you mean? inquired Professor Bird.

Well, look here. The data file from the Chief's email shows \Sexpr{dim(RcmdrTestDrive)[1]}
murders, but there have only been \Sexpr{dim(RcmdrTestDrive)[1]-2}.
This doesnt make any sense. Ill have to straighten it out. Hey,
wait a minute. Look at this, Person \#\Sexpr{dim(RcmdrTestDrive)[1]-1}
and Person \#\Sexpr{dim(RcmdrTestDrive)[1]} seem to match our stats.
But how can that be?

It was at this moment that our two heroes were shot from behind and
fell over the computer, dead. The killer hit \textsf{Delete} on the
computer and walked out slowly (considering they had arthritis) and
cackling loudly in the now quiet computer lab. 

And so, I guess my question to you the reader is, did Granny murder
\Sexpr{dim(RcmdrTestDrive)[1]} people, or did the murderer slip through
the cracks of justice? You be the statistician and come to your own
conclusion. 

Detective Pyork E.~Pig 

{*}{*}{*}End File{*}{*}{*}


\chapter{\textsf{R} Session Information}



<<keep.source = TRUE>>=
options(width = 80)
sessionInfo()
@

\vfill{}



\chapter{GNU Free Documentation License}

\begin{center}
GNU Free Documentation License 
\par\end{center}

\begin{center}
Version 1.3, 3 November 2008\bigskip{}

\par\end{center}

Copyright (C) 2000, 2001, 2002, 2007, 2008 Free Software Foundation,
Inc. <http://fsf.org/> Everyone is permitted to copy and distribute
verbatim copies of this license document, but changing it is not allowed.


\section*{0. PREAMBLE}

The purpose of this License is to make a manual, textbook, or other
functional and useful document \textquotedbl{}free\textquotedbl{}
in the sense of freedom: to assure everyone the effective freedom
to copy and redistribute it, with or without modifying it, either
commercially or noncommercially. Secondarily, this License preserves
for the author and publisher a way to get credit for their work, while
not being considered responsible for modifications made by others.

This License is a kind of \textquotedbl{}copyleft\textquotedbl{},
which means that derivative works of the document must themselves
be free in the same sense. It complements the GNU General Public License,
which is a copyleft license designed for free software.

We have designed this License in order to use it for manuals for free
software, because free software needs free documentation: a free program
should come with manuals providing the same freedoms that the software
does. But this License is not limited to software manuals; it can
be used for any textual work, regardless of subject matter or whether
it is published as a printed book. We recommend this License principally
for works whose purpose is instruction or reference.


\section*{1. APPLICABILITY AND DEFINITIONS}

This License applies to any manual or other work, in any medium, that
contains a notice placed by the copyright holder saying it can be
distributed under the terms of this License. Such a notice grants
a world-wide, royalty-free license, unlimited in duration, to use
that work under the conditions stated herein. The \textquotedbl{}Document\textquotedbl{},
below, refers to any such manual or work. Any member of the public
is a licensee, and is addressed as \textquotedbl{}you\textquotedbl{}.
You accept the license if you copy, modify or distribute the work
in a way requiring permission under copyright law.

A \textquotedbl{}Modified Version\textquotedbl{} of the Document means
any work containing the Document or a portion of it, either copied
verbatim, or with modifications and/or translated into another language.

A \textquotedbl{}Secondary Section\textquotedbl{} is a named appendix
or a front-matter section of the Document that deals exclusively with
the relationship of the publishers or authors of the Document to the
Document's overall subject (or to related matters) and contains nothing
that could fall directly within that overall subject. (Thus, if the
Document is in part a textbook of mathematics, a Secondary Section
may not explain any mathematics.) The relationship could be a matter
of historical connection with the subject or with related matters,
or of legal, commercial, philosophical, ethical or political position
regarding them.

The \textquotedbl{}Invariant Sections\textquotedbl{} are certain Secondary
Sections whose titles are designated, as being those of Invariant
Sections, in the notice that says that the Document is released under
this License. If a section does not fit the above definition of Secondary
then it is not allowed to be designated as Invariant. The Document
may contain zero Invariant Sections. If the Document does not identify
any Invariant Sections then there are none.

The \textquotedbl{}Cover Texts\textquotedbl{} are certain short passages
of text that are listed, as Front-Cover Texts or Back-Cover Texts,
in the notice that says that the Document is released under this License.
A Front-Cover Text may be at most 5 words, and a Back-Cover Text may
be at most 25 words.

A \textquotedbl{}Transparent\textquotedbl{} copy of the Document means
a machine-readable copy, represented in a format whose specification
is available to the general public, that is suitable for revising
the document straightforwardly with generic text editors or (for images
composed of pixels) generic paint programs or (for drawings) some
widely available drawing editor, and that is suitable for input to
text formatters or for automatic translation to a variety of formats
suitable for input to text formatters. A copy made in an otherwise
Transparent file format whose markup, or absence of markup, has been
arranged to thwart or discourage subsequent modification by readers
is not Transparent. An image format is not Transparent if used for
any substantial amount of text. A copy that is not \textquotedbl{}Transparent\textquotedbl{}
is called \textquotedbl{}Opaque\textquotedbl{}.

Examples of suitable formats for Transparent copies include plain
ASCII without markup, Texinfo input format, \LaTeX{} input format,
SGML or XML using a publicly available DTD, and standard-conforming
simple HTML, PostScript or PDF designed for human modification. Examples
of transparent image formats include PNG, XCF and JPG. Opaque formats
include proprietary formats that can be read and edited only by proprietary
word processors, SGML or XML for which the DTD and/or processing tools
are not generally available, and the machine-generated HTML, PostScript
or PDF produced by some word processors for output purposes only.

The \textquotedbl{}Title Page\textquotedbl{} means, for a printed
book, the title page itself, plus such following pages as are needed
to hold, legibly, the material this License requires to appear in
the title page. For works in formats which do not have any title page
as such, \textquotedbl{}Title Page\textquotedbl{} means the text near
the most prominent appearance of the work's title, preceding the beginning
of the body of the text.

The \textquotedbl{}publisher\textquotedbl{} means any person or entity
that distributes copies of the Document to the public.

A section \textquotedbl{}Entitled XYZ\textquotedbl{} means a named
subunit of the Document whose title either is precisely XYZ or contains
XYZ in parentheses following text that translates XYZ in another language.
(Here XYZ stands for a specific section name mentioned below, such
as \textquotedbl{}Acknowledgements\textquotedbl{}, \textquotedbl{}Dedications\textquotedbl{},
\textquotedbl{}Endorsements\textquotedbl{}, or \textquotedbl{}History\textquotedbl{}.)
To \textquotedbl{}Preserve the Title\textquotedbl{} of such a section
when you modify the Document means that it remains a section \textquotedbl{}Entitled
XYZ\textquotedbl{} according to this definition.

The Document may include Warranty Disclaimers next to the notice which
states that this License applies to the Document. These Warranty Disclaimers
are considered to be included by reference in this License, but only
as regards disclaiming warranties: any other implication that these
Warranty Disclaimers may have is void and has no effect on the meaning
of this License.


\section*{2. VERBATIM COPYING}

You may copy and distribute the Document in any medium, either commercially
or noncommercially, provided that this License, the copyright notices,
and the license notice saying this License applies to the Document
are reproduced in all copies, and that you add no other conditions
whatsoever to those of this License. You may not use technical measures
to obstruct or control the reading or further copying of the copies
you make or distribute. However, you may accept compensation in exchange
for copies. If you distribute a large enough number of copies you
must also follow the conditions in section 3.

You may also lend copies, under the same conditions stated above,
and you may publicly display copies.


\section*{3. COPYING IN QUANTITY}

If you publish printed copies (or copies in media that commonly have
printed covers) of the Document, numbering more than 100, and the
Document's license notice requires Cover Texts, you must enclose the
copies in covers that carry, clearly and legibly, all these Cover
Texts: Front-Cover Texts on the front cover, and Back-Cover Texts
on the back cover. Both covers must also clearly and legibly identify
you as the publisher of these copies. The front cover must present
the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition. Copying with
changes limited to the covers, as long as they preserve the title
of the Document and satisfy these conditions, can be treated as verbatim
copying in other respects.

If the required texts for either cover are too voluminous to fit legibly,
you should put the first ones listed (as many as fit reasonably) on
the actual cover, and continue the rest onto adjacent pages.

If you publish or distribute Opaque copies of the Document numbering
more than 100, you must either include a machine-readable Transparent
copy along with each Opaque copy, or state in or with each Opaque
copy a computer-network location from which the general network-using
public has access to download using public-standard network protocols
a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps,
when you begin distribution of Opaque copies in quantity, to ensure
that this Transparent copy will remain thus accessible at the stated
location until at least one year after the last time you distribute
an Opaque copy (directly or through your agents or retailers) of that
edition to the public.

It is requested, but not required, that you contact the authors of
the Document well before redistributing any large number of copies,
to give them a chance to provide you with an updated version of the
Document.


\section*{4. MODIFICATIONS}

You may copy and distribute a Modified Version of the Document under
the conditions of sections 2 and 3 above, provided that you release
the Modified Version under precisely this License, with the Modified
Version filling the role of the Document, thus licensing distribution
and modification of the Modified Version to whoever possesses a copy
of it. In addition, you must do these things in the Modified Version:

A. Use in the Title Page (and on the covers, if any) a title distinct
from that of the Document, and from those of previous versions (which
should, if there were any, be listed in the History section of the
Document). You may use the same title as a previous version if the
original publisher of that version gives permission. 

B. List on the Title Page, as authors, one or more persons or entities
responsible for authorship of the modifications in the Modified Version,
together with at least five of the principal authors of the Document
(all of its principal authors, if it has fewer than five), unless
they release you from this requirement. 

C. State on the Title page the name of the publisher of the Modified
Version, as the publisher. 

D. Preserve all the copyright notices of the Document. 

E. Add an appropriate copyright notice for your modifications adjacent
to the other copyright notices. 

F. Include, immediately after the copyright notices, a license notice
giving the public permission to use the Modified Version under the
terms of this License, in the form shown in the Addendum below. 

G. Preserve in that license notice the full lists of Invariant Sections
and required Cover Texts given in the Document's license notice. 

H. Include an unaltered copy of this License. 

I. Preserve the section Entitled \textquotedbl{}History\textquotedbl{},
Preserve its Title, and add to it an item stating at least the title,
year, new authors, and publisher of the Modified Version as given
on the Title Page. If there is no section Entitled \textquotedbl{}History\textquotedbl{}
in the Document, create one stating the title, year, authors, and
publisher of the Document as given on its Title Page, then add an
item describing the Modified Version as stated in the previous sentence. 

J. Preserve the network location, if any, given in the Document for
public access to a Transparent copy of the Document, and likewise
the network locations given in the Document for previous versions
it was based on. These may be placed in the \textquotedbl{}History\textquotedbl{}
section. You may omit a network location for a work that was published
at least four years before the Document itself, or if the original
publisher of the version it refers to gives permission. 

K. For any section Entitled \textquotedbl{}Acknowledgements\textquotedbl{}
or \textquotedbl{}Dedications\textquotedbl{}, Preserve the Title of
the section, and preserve in the section all the substance and tone
of each of the contributor acknowledgements and/or dedications given
therein. 

L. Preserve all the Invariant Sections of the Document, unaltered
in their text and in their titles. Section numbers or the equivalent
are not considered part of the section titles. 

M. Delete any section Entitled \textquotedbl{}Endorsements\textquotedbl{}.
Such a section may not be included in the Modified Version. 

N. Do not retitle any existing section to be Entitled \textquotedbl{}Endorsements\textquotedbl{}
or to conflict in title with any Invariant Section. 

O. Preserve any Warranty Disclaimers.

If the Modified Version includes new front-matter sections or appendices
that qualify as Secondary Sections and contain no material copied
from the Document, you may at your option designate some or all of
these sections as invariant. To do this, add their titles to the list
of Invariant Sections in the Modified Version's license notice. These
titles must be distinct from any other section titles.

You may add a section Entitled \textquotedbl{}Endorsements\textquotedbl{},
provided it contains nothing but endorsements of your Modified Version
by various parties--for example, statements of peer review or that
the text has been approved by an organization as the authoritative
definition of a standard.

You may add a passage of up to five words as a Front-Cover Text, and
a passage of up to 25 words as a Back-Cover Text, to the end of the
list of Cover Texts in the Modified Version. Only one passage of Front-Cover
Text and one of Back-Cover Text may be added by (or through arrangements
made by) any one entity. If the Document already includes a cover
text for the same cover, previously added by you or by arrangement
made by the same entity you are acting on behalf of, you may not add
another; but you may replace the old one, on explicit permission from
the previous publisher that added the old one.

The author(s) and publisher(s) of the Document do not by this License
give permission to use their names for publicity for or to assert
or imply endorsement of any Modified Version.


\section*{5. COMBINING DOCUMENTS}

You may combine the Document with other documents released under this
License, under the terms defined in section 4 above for modified versions,
provided that you include in the combination all of the Invariant
Sections of all of the original documents, unmodified, and list them
all as Invariant Sections of your combined work in its license notice,
and that you preserve all their Warranty Disclaimers.

The combined work need only contain one copy of this License, and
multiple identical Invariant Sections may be replaced with a single
copy. If there are multiple Invariant Sections with the same name
but different contents, make the title of each such section unique
by adding at the end of it, in parentheses, the name of the original
author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant
Sections in the license notice of the combined work.

In the combination, you must combine any sections Entitled \textquotedbl{}History\textquotedbl{}
in the various original documents, forming one section Entitled \textquotedbl{}History\textquotedbl{};
likewise combine any sections Entitled \textquotedbl{}Acknowledgements\textquotedbl{},
and any sections Entitled \textquotedbl{}Dedications\textquotedbl{}.
You must delete all sections Entitled \textquotedbl{}Endorsements\textquotedbl{}.


\section*{6. COLLECTIONS OF DOCUMENTS}

You may make a collection consisting of the Document and other documents
released under this License, and replace the individual copies of
this License in the various documents with a single copy that is included
in the collection, provided that you follow the rules of this License
for verbatim copying of each of the documents in all other respects.

You may extract a single document from such a collection, and distribute
it individually under this License, provided you insert a copy of
this License into the extracted document, and follow this License
in all other respects regarding verbatim copying of that document.


\section*{7. AGGREGATION WITH INDEPENDENT WORKS}

A compilation of the Document or its derivatives with other separate
and independent documents or works, in or on a volume of a storage
or distribution medium, is called an \textquotedbl{}aggregate\textquotedbl{}
if the copyright resulting from the compilation is not used to limit
the legal rights of the compilation's users beyond what the individual
works permit. When the Document is included in an aggregate, this
License does not apply to the other works in the aggregate which are
not themselves derivative works of the Document.

If the Cover Text requirement of section 3 is applicable to these
copies of the Document, then if the Document is less than one half
of the entire aggregate, the Document's Cover Texts may be placed
on covers that bracket the Document within the aggregate, or the electronic
equivalent of covers if the Document is in electronic form. Otherwise
they must appear on printed covers that bracket the whole aggregate.


\section*{8. TRANSLATION}

Translation is considered a kind of modification, so you may distribute
translations of the Document under the terms of section 4. Replacing
Invariant Sections with translations requires special permission from
their copyright holders, but you may include translations of some
or all Invariant Sections in addition to the original versions of
these Invariant Sections. You may include a translation of this License,
and all the license notices in the Document, and any Warranty Disclaimers,
provided that you also include the original English version of this
License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original
version of this License or a notice or disclaimer, the original version
will prevail.

If a section in the Document is Entitled \textquotedbl{}Acknowledgements\textquotedbl{},
\textquotedbl{}Dedications\textquotedbl{}, or \textquotedbl{}History\textquotedbl{},
the requirement (section 4) to Preserve its Title (section 1) will
typically require changing the actual title.


\section*{9. TERMINATION}

You may not copy, modify, sublicense, or distribute the Document except
as expressly provided under this License. Any attempt otherwise to
copy, modify, sublicense, or distribute it is void, and will automatically
terminate your rights under this License.

However, if you cease all violation of this License, then your license
from a particular copyright holder is reinstated (a) provisionally,
unless and until the copyright holder explicitly and finally terminates
your license, and (b) permanently, if the copyright holder fails to
notify you of the violation by some reasonable means prior to 60 days
after the cessation.

Moreover, your license from a particular copyright holder is reinstated
permanently if the copyright holder notifies you of the violation
by some reasonable means, this is the first time you have received
notice of violation of this License (for any work) from that copyright
holder, and you cure the violation prior to 30 days after your receipt
of the notice.

Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License. If your rights have been terminated and not permanently
reinstated, receipt of a copy of some or all of the same material
does not give you any rights to use it.


\section*{10. FUTURE REVISIONS OF THIS LICENSE}

The Free Software Foundation may publish new, revised versions of
the GNU Free Documentation License from time to time. Such new versions
will be similar in spirit to the present version, but may differ in
detail to address new problems or concerns. See http://www.gnu.org/copyleft/.

Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this
License \textquotedbl{}or any later version\textquotedbl{} applies
to it, you have the option of following the terms and conditions either
of that specified version or of any later version that has been published
(not as a draft) by the Free Software Foundation. If the Document
does not specify a version number of this License, you may choose
any version ever published (not as a draft) by the Free Software Foundation.
If the Document specifies that a proxy can decide which future versions
of this License can be used, that proxy's public statement of acceptance
of a version permanently authorizes you to choose that version for
the Document.


\section*{11. RELICENSING}

\textquotedbl{}Massive Multiauthor Collaboration Site\textquotedbl{}
(or \textquotedbl{}MMC Site\textquotedbl{}) means any World Wide Web
server that publishes copyrightable works and also provides prominent
facilities for anybody to edit those works. A public wiki that anybody
can edit is an example of such a server. A \textquotedbl{}Massive
Multiauthor Collaboration\textquotedbl{} (or \textquotedbl{}MMC\textquotedbl{})
contained in the site means any set of copyrightable works thus published
on the MMC site.

\textquotedbl{}CC-BY-SA\textquotedbl{} means the Creative Commons
Attribution-Share Alike 3.0 license published by Creative Commons
Corporation, a not-for-profit corporation with a principal place of
business in San Francisco, California, as well as future copyleft
versions of that license published by that same organization.

\textquotedbl{}Incorporate\textquotedbl{} means to publish or republish
a Document, in whole or in part, as part of another Document.

An MMC is \textquotedbl{}eligible for relicensing\textquotedbl{} if
it is licensed under this License, and if all works that were first
published under this License somewhere other than this MMC, and subsequently
incorporated in whole or in part into the MMC, (1) had no cover texts
or invariant sections, and (2) were thus incorporated prior to November
1, 2008.

The operator of an MMC Site may republish an MMC contained in the
site under CC-BY-SA on the same site at any time before August 1,
2009, provided the MMC is eligible for relicensing.


\section*{ADDENDUM: How to use this License for your documents}

To use this License in a document you have written, include a copy
of the License in the document and put the following copyright and
license notices just after the title page:
\begin{quotation}
\noindent Copyright (c) YEAR YOUR NAME. Permission is granted to copy,
distribute and/or modify this document under the terms of the GNU
Free Documentation License, Version 1.3 or any later version published
by the Free Software Foundation; with no Invariant Sections, no Front-Cover
Texts, and no Back-Cover Texts. A copy of the license is included
in the section entitled \textquotedbl{}GNU Free Documentation License\textquotedbl{}.
\end{quotation}
If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
replace the \textquotedbl{}with...Texts.\textquotedbl{} line with
this:
\begin{quotation}
\noindent with the Invariant Sections being LIST THEIR TITLES, with
the Front-Cover Texts being LIST, and with the Back-Cover Texts being
LIST.
\end{quotation}
If you have Invariant Sections without Cover Texts, or some other
combination of the three, merge those two alternatives to suit the
situation.

If your document contains nontrivial examples of program code, we
recommend releasing these examples in parallel under your choice of
free software license, such as the GNU General Public License, to
permit their use in free software. 


\chapter{History}

\begin{tabular}{ll}
\textbf{Title:} & Introduction to Probability and Statistics Using \textsf{R}\tabularnewline
\textbf{Year:} & 2009\tabularnewline
\textbf{Authors:} & G.~Jay Kerns\tabularnewline
\textbf{Publisher:} & G.~Jay Kerns\tabularnewline
\end{tabular}


\chapter{Some References}
\begin{itemize}
\item Billingsley, Resnick, or Ash Dooleans-Dade.
\item Michael Friendly (2000), Visualizing Categorical Data, pages 8283,
319322. 
\item \textsf{R} Help Desk: Accessing the Sources. \textsf{R} News 6 (4),
43-45. 
\item Gelman and this other Bayesian book BLANK
\item Calculus (say, Stewart or Apostol), Real Analysis (say, Rudin, Folland,
or Carothers), or Measure Theory (Billingsley, Halmos, Dudley) fo
\item A. Agresti and B.A. Coull, Approximate is better than \textquotedbl{}exact\textquotedbl{}
for interval estimation of binomial proportions, \_American Statistician,\_
{*}52{*}:119-126, 1998. For the score interval.
\item Reference to Tabachnick \& Fidell.
\item Dalgaard, P. (2002). Introductory Statistics with \textsf{R}. Springer.
\item Everitt, B. (2005). An \textsf{R} and \texttt{S-Plus} Companion to
Multivariate Analysis. Springer.
\item Heiberger, R. and Holland, B. (2004). Statistical Analysis and Data
Display. An Intermediate Course with Examples in \texttt{S-Plus},
\textsf{R}, and \texttt{SAS}. Springer.
\item Maindonald, J. and Braun, J. (2003). Data Analysis and Graphics Using
\textsf{R}: an Example Based Approach. Cambridge University Press.
\item Venables, W. and Smith, D. (2005). An Introduction to \textsf{R}.
\url{http://www.r-project.org/Manuals}.
\item Verzani, J. (2005). Using \textsf{R} for Introductory Statistics.
Chapman and Hall.
\item Billingsley,
\item Resnick,
\item Ash Dooleans-Dade
\item odfWeave package
\item distrEx package
\item distrXXX family
\item Rcmdr
\item e1071
\item RcmdrPlugin.IPSUR
\item Gelman Bayesian book, and some more, too.
\item Bootstrap Confidence Intervals, Thomas J. DiCiccio and Bradley Efron,
Statistical Science 1996, Vol. 11, No. 3, 189228
\item Torsten Hothorn, Kurt Hornik, Mark A. van de Wiel \& Achim Zeileis
(2008). Implementing a class of permutation tests: The coin package,
Journal of Statistical Software, 28(8), 123. http://www.jstatsoft.org/v28/i08/
\item \textsf{R} Help Desk: Accessing the Sources. \textsf{R} News 6 (4),
43-45. In short,
\item \url{http://www.rsscse.org.uk/ts/gtb/johnson3.pdf}
\item \url{http://en.wikipedia.org/wiki/Mark_and_recapture}
\end{itemize}
<<>>=
rm(.Random.seed)
save.image(file = "IPSUR.RData")
@


\chapter{\textsf{R} Transcript}

<<echo=FALSE, results=hide, split=FALSE>>= 
Stangle(file="IPSUR.Rnw", output="IPSUR.R", annotate=TRUE)
@

\texttt{\lstinputlisting[emptylines=1]{IPSUR.R}}
\end{document}
